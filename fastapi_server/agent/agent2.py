from __future__ import annotations

import os
from dataclasses import dataclass
from typing import TypedDict, Dict, Sequence, Union, Optional, Any
import asyncio

from asgiref.sync import sync_to_async
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langgraph.graph import END, START, StateGraph
from dotenv import load_dotenv
from openai import OpenAI
from pinecone import Pinecone, ServerlessSpec
from langchain.chat_models import ChatOpenAI
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langgraph.graph.message import add_messages
from fastapi_server.agent.prompt import (
    document_type_system_prompt_agent2,
    proceedings_summary_prompt_agent2,
    internal_policy_summary_prompt_template_agent2,
    product_document_summary_prompt_template_agent2,
    technical_document_summary_prompt_template_agent2,
    unknown_document_type_prompt_agent2,
    rag_answer_generation_prompt_agent2,
    rag_system_message_agent2
)
load_dotenv()

def init_clients():
    # 1-1) OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        raise ValueError("âš ï¸ í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    # OpenAI ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤.
    openai_client = OpenAI(api_key=openai_api_key)
    print("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì™„ë£Œ")

    # 1-2) Pinecone ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    pinecone_api_key = os.getenv("PINECONE_API_KEY")
    pinecone_env     = os.getenv("PINECONE_ENVIRONMENT")   # ì˜ˆ: "us-east1-gcp" ë˜ëŠ” "us-west1-gcp" ë“±
    if not pinecone_api_key or not pinecone_env:
        raise ValueError("âš ï¸ í™˜ê²½ë³€ìˆ˜ PINECONE_API_KEY ë˜ëŠ” PINECONE_ENVIRONMENTê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")

    pc = Pinecone(api_key=pinecone_api_key, environment=pinecone_env)
    print("âœ… Pinecone í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì™„ë£Œ")

    # 1-3) ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    index_name = "dense-index"  # ì‹¤ì œ ì‚¬ìš© ì¤‘ì¸ ì¸ë±ìŠ¤ ì´ë¦„ìœ¼ë¡œ êµì²´í•˜ì„¸ìš”
    existing_indexes = pc.list_indexes().names()
    if index_name not in existing_indexes:
        raise ValueError(f"âš ï¸ ì¸ë±ìŠ¤ '{index_name}'ê°€ Pineconeì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í˜„ì¬ ì¸ë±ìŠ¤ ëª©ë¡: {existing_indexes}")

    # 1-4) í•´ë‹¹ ì¸ë±ìŠ¤ ê°ì²´ ê°€ì ¸ì˜¤ê¸°
    index = pc.Index(index_name)
    print(f"âœ… Pinecone ì¸ë±ìŠ¤ '{index_name}' ì—°ê²° ì™„ë£Œ (Namespaces: {len(index.describe_index_stats().namespaces)})")

    return openai_client, index


# --------------------------------------------------
# 2) ì§ˆë¬¸ ë¬¸ì¥ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
# --------------------------------------------------
def embed_query(openai_client: OpenAI, text: str) -> list:
    """
    ìµœì‹  OpenAI í´ë¼ì´ì–¸íŠ¸ì—ì„œëŠ” resp.data[0].embedding ìœ¼ë¡œ ë²¡í„°ì— ì ‘ê·¼í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    resp = openai_client.embeddings.create(
        model="text-embedding-3-large",
        input=text
    )
    return resp.data[0].embedding


# --------------------------------------------------
# 3) ì—¬ëŸ¬ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì¤‘ â€œê°€ì¥ ë†’ì€ ìœ ì‚¬ë„â€ë¥¼ ì¤€ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì™€ ë§¤ì¹­ ê²°ê³¼ ë°˜í™˜
# --------------------------------------------------
def retrieve_best_namespace(index, query_vector: list, top_k: int = 5):
    """
    1) index.describe_index_stats()ë¥¼ í†µí•´ ëª¨ë“  ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ëª©ë¡ì„ ì–»ëŠ”ë‹¤.
    2) ê° ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ë¡œ query_vectorë¥¼ index.query()ë¡œ ê²€ìƒ‰í•˜ê³ ,
       matches[0].score ë¥¼ ë¹„êµí•´ì„œ â€œìµœê³  ìœ ì‚¬ë„â€ë¥¼ ì°¾ëŠ”ë‹¤.
    3) ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ë¥¼ ì¤€ ë„¤ì„ìŠ¤í˜ì´ìŠ¤(best_ns)ì™€ í•´ë‹¹ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ ì „ì²´ ë§¤ì¹­ ê²°ê³¼(best_matches)ë¥¼ ë°˜í™˜.
    """
    stats = index.describe_index_stats()
    available_namespaces = list(stats.namespaces.keys())
    if not available_namespaces:
        raise ValueError("âš ï¸ ì¸ë±ìŠ¤ì— ë„¤ì„ìŠ¤í˜ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

    best_ns = None
    best_score = -1.0
    best_matches = None

    for ns in available_namespaces:
        count = stats.namespaces[ns]["vector_count"]
        if count == 0:
            # ë¹„ì–´ ìˆëŠ” ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê±´ë„ˆë›°ê¸°
            continue

        res = index.query(
            vector=query_vector,
            namespace=ns,
            top_k=top_k,
            include_metadata=True
        )
        if not res.matches:
            continue

        top_score = res.matches[0].score
        if top_score > best_score:
            best_score = top_score
            best_ns = ns
            best_matches = res.matches

    if best_ns is None:
        raise ValueError("âš ï¸ ì–´ë–¤ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì—ì„œë„ ë§¤ì¹­ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    print(f"ğŸ” ì„ íƒëœ ë„¤ì„ìŠ¤í˜ì´ìŠ¤: '{best_ns}' (ìµœê³  ìœ ì‚¬ë„: {best_score:.4f})")
    return best_ns, best_matches


# --------------------------------------------------
# 4) ê²€ìƒ‰ëœ ë§¤ì¹­ ê²°ê³¼ì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸(ë©”íƒ€ë°ì´í„°)ë¥¼ êº¼ë‚´ Context ë¡œ ê²°í•©
# --------------------------------------------------
def build_context_from_matches(matches):
    """
    res.matches ë¦¬ìŠ¤íŠ¸ ì•ˆì˜ ê° item.metadata ì— ë“¤ì–´ ìˆëŠ” í…ìŠ¤íŠ¸ í•„ë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ì—…ë¡œë“œ ì‹œ metadata í‚¤ê°€ "text"ì˜€ë‹¤ê³  ê°€ì •í–ˆìŠµë‹ˆë‹¤.
    """
    contexts = []
    for m in matches:
        chunk_text = m.metadata.get("text", "")
        if chunk_text:
            contexts.append(chunk_text)

    return "\n---\n".join(contexts)


# --------------------------------------------------
# 5) LLM ChatCompletion í˜¸ì¶œí•˜ì—¬ ë‹µë³€ ìƒì„±
# --------------------------------------------------
def generate_answer_with_context(openai_client: OpenAI, question: str, context: str) -> str:
    """
    ìµœì‹  OpenAI í´ë¼ì´ì–¸íŠ¸ì—ì„œëŠ” client.chat.completions.create(...) í˜•íƒœë¥¼ ì”ë‹ˆë‹¤.
    """
    formatted_prompt = rag_answer_generation_prompt_agent2.format(context=context, question=question)
    resp = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": rag_system_message_agent2},
            {"role": "user", "content": formatted_prompt}
        ],
        temperature=0.0,
        max_tokens=1024
    )
    # resp.choices[0].message.content ìœ¼ë¡œ ë‹µë³€ ì¶”ì¶œ
    return resp.choices[0].message.content.strip()



@dataclass
class State:
    # Compatible with both direct user_input and messages-based interface
    user_input: str = ""
    document_type: str = ""
    result: str = ""
    messages: Sequence[BaseMessage] = None
    
    def __post_init__(self):
        # If initialized from supervisor with messages but no user_input, extract user_input
        if not self.user_input and self.messages:
            # Extract user input from the last human message
            user_messages = [msg for msg in self.messages if isinstance(msg, HumanMessage)]
            if user_messages:
                self.user_input = user_messages[-1].content
    
    def dict(self):
        """Return dict representation with messages if present"""
        result = {
            "result": self.result,
            "document_type": self.document_type,
            "user_input": self.user_input
        }
        # If this was called with messages, return updated messages too
        if self.messages is not None:
            result["messages"] = self.messages + [AIMessage(content=self.result)] if self.result else self.messages
        return result  # ì±—ë´‡ ê²°ê³¼


def choose_document_type(message):
    """
    Langchain LLMì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ íƒ€ì…ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤. ë¦¬í„´ ë°ì´í„° í˜•ì‹ì€ ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€í•©ë‹ˆë‹¤.
    """
    llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo') # ë˜ëŠ” ë‹¤ë¥¸ ëª¨ë¸
    prompt = PromptTemplate.from_template(document_type_system_prompt_agent2)
    chain = prompt | llm
    
    response = chain.ainvoke({"input": message})
    classified_type = response.content.strip()
    print(f"ë¬¸ì„œ íƒ€ì… ë¶„ë¥˜ ê²°ê³¼: {classified_type}")
    return classified_type

def choose_node(state: State):
    # Extract the user input from the state
    user_input = state.user_input

    # Choose document type
    document_type = choose_document_type(user_input)
    
    # Update state with document type
    state.document_type = document_type
    
    # Print document type for debugging
    # print(f"Document Type: {document_type}")
    
    return state.dict()

def choose_one(state: State) -> str:
    choice = state.document_type
    # Use logging instead of print to avoid output being captured in response
    # print(f"(choice_one) Choice: {choice}")
    # This must return the string key for conditional edge routing
    if choice in ["internal_policy", "product_document", "technical_document", "proceedings"]:
        return choice
    else:
        return "product_document"  # Default fallback

def execute_rag(state: State):
    # print(f"\nğŸ“„ RAG ë…¸ë“œ ì‹¤í–‰: ë¬¸ì„œ íƒ€ì… = '{state.document_type}', ì§ˆë¬¸ = '{state.user_input}'")
    openai_client, pinecone_index = init_clients()
    # print("   - í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

    query_vector = embed_query(openai_client, state.user_input)
    # print(f"   - ì§ˆë¬¸ ì„ë² ë”© ì™„ë£Œ (ë²¡í„° í¬ê¸°: {len(query_vector)})")

    namespace_to_search = state.document_type
    if not namespace_to_search or namespace_to_search == "unknown":
        message = f"ë¬¸ì„œ íƒ€ì…ì´ '{namespace_to_search}'(ìœ¼)ë¡œ ë¶„ë¥˜ë˜ì–´ Pinecone ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
        # print(f"   - ì •ë³´: {message}")
        # 'unknown'ì¼ ê²½ìš°, unknown_handler_nodeì—ì„œ ì´ë¯¸ ë©”ì‹œì§€ë¥¼ ì„¤ì •í–ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ë®ì–´ì“°ì§€ ì•Šê±°ë‚˜
        # í˜¹ì€ ì—¬ê¸°ì„œ ë‹¤ë¥¸ ë©”ì‹œì§€ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ê²€ìƒ‰ ë¶ˆê°€ ë©”ì‹œì§€ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
        # ì‹¤ì œë¡œëŠ” 'unknown' íƒ€ì…ì€ ì´ ë…¸ë“œë¡œ ì˜¤ì§€ ì•Šê³  unknown_handler_nodeë¡œ ê°€ì•¼ í•©ë‹ˆë‹¤.
        # ì´ ì½”ë“œëŠ” execute_rag_nodeê°€ 'unknown' íƒ€ì…ìœ¼ë¡œ í˜¸ì¶œë  ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ë°©ì–´ ì½”ë“œì…ë‹ˆë‹¤.
        state.result = "ì ì ˆí•œ ë¬¸ì„œ ì €ì¥ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return state.dict()

    # print(f"   - Pinecone ë„¤ì„ìŠ¤í˜ì´ìŠ¤ '{namespace_to_search}'ì—ì„œ ê²€ìƒ‰ ì‹œì‘...")
    index_stats = pinecone_index.describe_index_stats()
    if namespace_to_search not in index_stats.namespaces or \
        index_stats.namespaces[namespace_to_search].vector_count == 0:
        message = f"'{namespace_to_search}' ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ Pineconeì—ì„œ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜, í•´ë‹¹ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. Pinecone ëŒ€ì‹œë³´ë“œì—ì„œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì´ë¦„ê³¼ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        # print(f"   - ê²½ê³ : {message}")
        state.result = message
        return state.dict()

    res = pinecone_index.query(
        vector=query_vector,
        namespace=namespace_to_search,
        top_k=5, # ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
        include_metadata=True
    )
    matches = res.matches
    # print(f"   - Pinecone ê²€ìƒ‰ ì™„ë£Œ: {len(matches)}ê°œ ê²°ê³¼ ìˆ˜ì‹ ")

    if not matches:
        message = f"'{namespace_to_search}' ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì—ì„œ '{state.user_input}' ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        # print(f"   - ì •ë³´ ì—†ìŒ: {message}")
        state.result = message
        return state.dict()
    
    context = build_context_from_matches(matches)
    if not context:
        message = "ê²€ìƒ‰ëœ ì •ë³´ì—ì„œ ë‹µë³€ì„ ìƒì„±í•  ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        print(f"   - ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶• ì‹¤íŒ¨: {message}")
        state.result = message
        return state.dict()
    print(f"   - ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶• ì™„ë£Œ (ê¸¸ì´: {len(context)})")

    state.result = context
    return state.dict()
    

def summarize_node(state: State):
    text = state.result
    document_type = state.document_type
    user_input = state.user_input

    if state.document_type == "proceedings":
        system_message = proceedings_summary_prompt_agent2
    elif state.document_type == "internal_policy":
        system_message = internal_policy_summary_prompt_template_agent2.format(user_input=user_input)
    elif state.document_type == "product_document":
        system_message = product_document_summary_prompt_template_agent2.format(user_input=user_input)
    elif state.document_type == "technical_document":
        system_message = technical_document_summary_prompt_template_agent2.format(user_input=user_input)
    else: # unknown or fallback
        system_message = unknown_document_type_prompt_agent2
    
    system_message = SystemMessagePromptTemplate.from_template(system_message)
    human_message = HumanMessagePromptTemplate.from_template("{text}")

    chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])

    # 2. LLM ìƒì„±
    llm = ChatOpenAI(model="gpt-4o")

    # 3. Promptì™€ LLM ê²°í•©
    chatbot = chat_prompt | llm

    # 4. ì‹¤í–‰
    response = chatbot.invoke({"text": text})
    result = response.content
    # print(result)  # ë””ë²„ê¹…ìš© ì¶œë ¥ ì œê±°
    state.result = result
    
    # ë¬¸ì„œ íƒ€ì…ì´ ìµœì¢… ê²°ê³¼ì— í¬í•¨ë˜ì§€ ì•Šë„ë¡, document_typeì„ ì œì™¸í•œ ìƒíƒœë§Œ ë°˜í™˜
    result_state = state.dict()
    if "document_type" in result_state:
        # document_type ê°’ì´ ìµœì¢… ì¶œë ¥ì— í¬í•¨ë˜ì§€ ì•Šë„ë¡ ì œê±°
        del result_state["document_type"]
    
    return result_state

# ë¹„ë™ê¸° ë…¸ë“œ ë˜í¼ í•¨ìˆ˜ë“¤ ì •ì˜
async def async_choose_node(state: State):
    return await sync_to_async(choose_node)(state)

async def async_execute_rag(state: State):
    return await sync_to_async(execute_rag)(state)

async def async_summarize_node(state: State):
    return await sync_to_async(summarize_node)(state)

async def async_choose_one(state: State):
    return await sync_to_async(choose_one)(state)

# Define the graph with async nodes
graph = (
    StateGraph(State)
    # (1) choose_node ë¶„ê¸° ë…¸ë“œ ë“±ë¡ (outputsì— ë¦¬í„´ í‚¤ ëª…ì‹œ)
    .add_node("choose_node", async_choose_node)
    # (2) RAG ì‹¤í–‰ ë…¸ë“œë“¤ ë“±ë¡
    .add_node("product_node", async_execute_rag)
    .add_node("proceedings_node", async_execute_rag)
    .add_node("hr_policy_node", async_execute_rag)
    .add_node("technical_document_node", async_execute_rag)
    # (3) summarize_node ë“±ë¡ (ìµœì¢… ë…¸ë“œ)
    .add_node("summarize_node", async_summarize_node)
    # (4) START â†’ ë¶„ê¸° ë…¸ë“œ(choose_node) â†’ (next_node ê°’ì— ë”°ë¼) ë¶„ê¸°
    .add_edge(START,"choose_node")
    .add_conditional_edges(
        "choose_node",
        async_choose_one,
        {
            "product_document": "product_node",
            "proceedings": "proceedings_node",
            "internal_policy": "hr_policy_node",
            "technical_document": "technical_document_node"
        }
    )
    # (5) ê° RAG ë…¸ë“œ â†’ summarize_node ì—°ê²°
    .add_edge("product_node", "summarize_node")
    .add_edge("proceedings_node", "summarize_node")
    .add_edge("hr_policy_node", "summarize_node")
    .add_edge("technical_document_node", "summarize_node")
    .add_edge("summarize_node", END)
    # (6) ìµœì¢… ì»´íŒŒì¼
    .compile(name="New Graph")
)