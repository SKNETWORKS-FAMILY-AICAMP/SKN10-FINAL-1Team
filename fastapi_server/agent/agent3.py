"""LangGraph graph for analysis_agent with supervisor logic.

Handles DB queries and general questions based on node/edge routing.
"""

from __future__ import annotations

import os
from typing import Optional, Dict, Any, List, Annotated, Literal, TypedDict
import asyncio
import io
from dotenv import load_dotenv
import os
import operator # For adding to message history
import psycopg2
import pandas as pd
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage # Added SystemMessage
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder # Added ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableConfig # Added missing import
from langchain_core.output_parsers import JsonOutputParser
from langchain_community.chat_models import ChatOpenAI
from langgraph.graph import StateGraph, END
from pydantic import BaseModel, Field
import pandas as pd
import numpy as np
import joblib
from sklearn.preprocessing import LabelEncoder
import logging # Added logging
from datetime import datetime
import re # Added import for regex
import json # Added for direct OpenAI call
from openai import OpenAI # Added for direct OpenAI call
from fastapi_server.agent.prompt import (
    supervisor_chat_prompt_agent3,
    sql_generation_chat_prompt_agent3,
    sql_result_summary_chat_prompt_agent3,
    general_chat_prompt_agent3
)

# Setup logger for agent3
logger = logging.getLogger(__name__)

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
import json
# Basic logging configuration if not configured elsewhere (e.g., in a main app setup)
if not logger.hasHandlers():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# --- Configuration (Optional - can be used to pass API keys, model names, etc.) ---
class Configuration(TypedDict, total=False):
    openai_api_key: Optional[str]
    db_env_path: Optional[str] # Path to .env file for DB credentials

# --- State Definition ---
class AgentState(BaseModel):
    messages: Annotated[List[BaseMessage], operator.add] = Field(default_factory=list)
    user_query: Optional[str] = None
    csv_file_content: Optional[str] = None
    query_type: Optional[Literal["db_query", "category_predict_query", "general_query"]] = None
    sql_query: Optional[str] = None
    sql_result: Optional[Any] = None
    final_answer: Optional[str] = None
    error_message: Optional[str] = None
    visualization_output: Optional[str] = None
    sql_output_choice: Optional[Literal["summarize", "visualize"]] = None # Decision for SQL output processing

    class Config:
        arbitrary_types_allowed = True # For Annotated and operator.add with BaseMessage
        
    def __post_init__(self):
        # Extract user query from messages if coming from supervisor
        if not self.user_query and self.messages:
            # Extract user input from the last human message
            user_messages = [msg for msg in self.messages if isinstance(msg, HumanMessage)]
            if user_messages:
                self.user_query = user_messages[-1].content
    
    def dict(self):
        """Return dict representation to ensure compatibility with supervisor state"""
        result = super().dict()
        # When returning to supervisor, ensure final answer is properly formatted as a new message
        if self.final_answer and self.messages is not None:
            result["messages"] = self.messages + [AIMessage(content=self.final_answer)]
        return result

# --- LLM and Prompts Setup ---
# Ensure OPENAI_API_KEY is set in your environment or passed via config
llm = ChatOpenAI(temperature=0, model="gpt-4o") # Or your preferred model

# --- Helper function for OpenAI API message format ---
def _lc_messages_to_openai_format(lc_messages: List[BaseMessage]) -> List[Dict[str, str]]:
    openai_messages = []
    for msg in lc_messages:
        if isinstance(msg, HumanMessage):
            openai_messages.append({"role": "user", "content": msg.content})
        elif isinstance(msg, AIMessage):
            openai_messages.append({"role": "assistant", "content": msg.content})
        # SystemMessages from state.messages are less common here as the main system prompt is usually separate
        elif isinstance(msg, SystemMessage):
             openai_messages.append({"role": "system", "content": msg.content})
    return openai_messages

# --- Pydantic Models for Structured Output ---
class SupervisorDecision(BaseModel):
    query_type: str = Field(description="The type of the user's question (db_query, category_predict_query, or general_query)")

class SQLGenerationOutput(BaseModel):
    sql_query: str = Field(description="The generated SQL query. This field MUST contain ONLY the SQL query string, without any surrounding text, explanations, or markdown formatting like ```sql.")
    sql_output_choice: Literal["summarize", "visualize"] = Field(description="The type of output processing required for the SQL result: 'summarize' or 'visualize'.")

# --- Node Functions ---
async def supervisor_node(state: AgentState, config: Optional[RunnableConfig] = None):
    """Determines the type of query (db, category_predict, or general)."""
    logger.info("--- Entering supervisor_node ---")
    if not state.messages:
        logger.warning("Supervisor_node: No messages in state. Cannot determine query type.")
        # Potentially set a default or error state
        state.query_type = "general_query" # Fallback, or handle error appropriately
        state.error_message = "No input message found."
        return state
        
    logger.debug(f"Supervisor_node: Current messages: {state.messages}")
    
    # The user_query is still useful for logging or if other parts need it, 
    # but the prompt now relies on the full message history.
    if not state.user_query:
        user_messages = [msg for msg in state.messages if isinstance(msg, HumanMessage)]
        if user_messages:
            state.user_query = user_messages[-1].content
        else:
            logger.warning("Supervisor_node: No HumanMessage found to extract user_query.")
            # Fallback if no human message, though MessagesPlaceholder handles history
            state.query_type = "general_query"
            state.error_message = "No human message found in history."
            return state

    logger.info(f"Supervisor_node: User query for routing: '{state.user_query}'")
    logger.info(f"Invoking supervisor_chat_prompt_agent3 with messages: {state.messages}")
    chain = supervisor_chat_prompt_agent3 | llm | JsonOutputParser(pydantic_object=SupervisorDecision) # Replaced with direct OpenAI call
    client = OpenAI()
    
    try:
        # Construct System Prompt for OpenAI API
        # supervisor_chat_prompt.messages[0] is the SystemMessage
        system_prompt_content = supervisor_chat_prompt_agent3.messages[0].content

        openai_api_messages = [{"role": "system", "content": system_prompt_content}]
        openai_api_messages.extend(_lc_messages_to_openai_format(state.messages))

        logger.debug(f"Supervisor_node: Sending to OpenAI API: {openai_api_messages}")

        completion = await asyncio.to_thread(
            client.chat.completions.create,
            model="gpt-4o", # Ensure this matches the intended model
            messages=openai_api_messages,
            temperature=0.0,
            response_format={"type": "json_object"}
        )
        raw_response_content = completion.choices[0].message.content
        logger.debug(f"Supervisor_node: Raw OpenAI response: {raw_response_content}")
        response_data = json.loads(raw_response_content)
        response = SupervisorDecision(**response_data)
        logger.info(f"Supervisor_node: LLM decision: {response.query_type}")
        state.query_type = response.query_type
        state.error_message = None # Clear previous errors
    except Exception as e:
        logger.error(f"Supervisor_node: Error invoking LLM or parsing output: {e}", exc_info=True)
        state.query_type = "general_query"  # Fallback on error
        state.final_answer = f"ì£„ì†¡í•©ë‹ˆë‹¤, ìš”ì²­ì„ ì´í•´í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
        state.error_message = str(e)
    
    logger.info(f"--- Exiting supervisor_node with query_type: {state.query_type} ---")
    return state

async def generate_sql_node(state: AgentState, config: Optional[RunnableConfig] = None):
    logger.info("--- Entering generate_sql_node ---")
    if not state.messages:
        logger.error("generate_sql_node: No messages in state. Cannot generate SQL.")
        state.error_message = "No input message found for SQL generation."
        state.final_answer = "SQL ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ ì…ë ¥ ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤."
        return state
        
    logger.debug(f"generate_sql_node: User query from state: {state.user_query}") # user_query might be stale, messages is source of truth
    logger.debug(f"generate_sql_node: Full messages for prompt: {state.messages}")

    logger.info(f"Invoking sql_generation_chat_prompt_agent3 with messages: {state.messages}")
    chain = sql_generation_chat_prompt_agent3 | llm | JsonOutputParser(pydantic_object=SQLGenerationOutput) # Replaced with direct OpenAI call
    client = OpenAI()

    try:
        current_date_str = datetime.now().strftime("%Y-%m-%d")
        # sql_generation_chat_prompt.messages[0] is the SystemMessage
        system_prompt_template = sql_generation_chat_prompt_agent3.messages[0].content
        system_prompt_content_sql = system_prompt_template.replace("{{current_date}}", current_date_str)

        openai_api_messages_sql = [{"role": "system", "content": system_prompt_content_sql}]
        openai_api_messages_sql.extend(_lc_messages_to_openai_format(state.messages))
        
        logger.debug(f"generate_sql_node: Sending to OpenAI API: {openai_api_messages_sql}")

        completion_sql = await asyncio.to_thread(
            client.chat.completions.create,
            model="gpt-4o", # Ensure this matches the intended model
            messages=openai_api_messages_sql,
            temperature=0.0,
            response_format={"type": "json_object"}
        )
        raw_response_content_sql = completion_sql.choices[0].message.content
        logger.debug(f"generate_sql_node: Raw OpenAI response: {raw_response_content_sql}")
        response_data_sql = json.loads(raw_response_content_sql)
        response = SQLGenerationOutput(**response_data_sql)
        
        # Clean the SQL query to remove any prepended JSON-like structures
        raw_sql_query = response.sql_query
        logger.debug(f"Raw SQL query from LLM: {raw_sql_query}")
        
        # Regex to find the actual SQL query, robustly handling optional prepended JSON objects.
        # It looks for common SQL keywords after any number of {...} blocks.
        # This regex assumes SQL queries start with standard keywords like SELECT, INSERT, UPDATE, DELETE, WITH, CREATE, ALTER, DROP.
        # It captures from the SQL keyword to the end of the string.
        match = re.search(r'^(?:\{.*?\})*?(SELECT\s.*|INSERT\s.*|UPDATE\s.*|DELETE\s.*|WITH\s.*|CREATE\s.*|ALTER\s.*|DROP\s.*)$', raw_sql_query.strip(), re.IGNORECASE | re.DOTALL)
        
        if match:
            cleaned_sql_query = match.group(1).strip() # Get the captured SQL part
            if not cleaned_sql_query.endswith(';'):
                cleaned_sql_query += ';'
            logger.info(f"Cleaned SQL query: {cleaned_sql_query}")
            state.sql_query = cleaned_sql_query
        else:
            # If regex doesn't match, log a warning and use the raw query, 
            # or handle as an error if it's critical that it's clean.
            logger.warning(f"Could not extract clean SQL from: {raw_sql_query}. Using raw query.")
            state.sql_query = raw_sql_query # Fallback to raw query
            # Ensure it ends with a semicolon if it looks like SQL
            if state.sql_query and isinstance(state.sql_query, str) and not state.sql_query.strip().endswith(';') and any(keyword in state.sql_query.upper() for keyword in ["SELECT", "INSERT", "UPDATE", "DELETE"]):
                 state.sql_query = state.sql_query.strip() + ';'

        state.sql_output_choice = response.sql_output_choice
        logger.info(f"Final state.sql_query: {state.sql_query}, Output choice: {state.sql_output_choice}")
        state.error_message = None # Clear previous errors
    except Exception as e:
        logger.error(f"Error generating SQL: {e}", exc_info=True)
        state.error_message = f"Error generating SQL: {e}"
        state.final_answer = f"ì£„ì†¡í•©ë‹ˆë‹¤, SQL ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
        state.sql_query = None
    logger.info("--- Exiting generate_sql_node ---")
    return state

async def execute_sql_node(state: AgentState, config: Optional[RunnableConfig] = None):
    logger.info("--- Entering execute_sql_node ---")
    if not state.sql_query:
        logger.error("No SQL query to execute.")
        state.error_message = "No SQL query to execute."
        state.final_answer = "ì‹¤í–‰í•  SQL ì¿¼ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤."
        # If there's no SQL query, we can't proceed to summarize or visualize.
        # We should indicate an error and perhaps end or route to a fallback.
        # For now, setting final_answer and error_message. The graph might need a dedicated error handling path.
        return state

    logger.info(f"Executing SQL: {state.sql_query}")
    
    # Determine the base directory for .env files
    # Assuming this script is in 'fastapi_server/agent/agent3.py'
    # Adjust if your structure is different
    current_script_dir = os.path.dirname(os.path.abspath(__file__))
    base_dir_analysis_env = os.path.join(current_script_dir, '..', '..', 'analysis_env') # Path to analysis_env folder
    base_dir_my_state_env = os.path.join(current_script_dir, '..', '..', 'my_state_env') # Path to my_state_env folder

    try:
        # Run the synchronous DB operation in a separate thread
        result_df = await asyncio.to_thread(
            _execute_sql_sync, 
            state.sql_query,
            base_dir_analysis_env,
            base_dir_my_state_env
        )
        
        if isinstance(result_df, pd.DataFrame):
            state.sql_result = result_df.to_dict(orient='records') # Convert DataFrame to list of dicts
            logger.info(f"SQL Result:\n{state.sql_result}")
        elif isinstance(result_df, dict) and 'sql_result' in result_df:
            # ê²°ê³¼ê°€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜ëœ ê²½ìš° (sql_result í‚¤ê°€ ìˆìœ¼ë©´ ì •ìƒì ì¸ ê²°ê³¼ë¡œ ê°„ì£¼)
            state.sql_result = result_df['sql_result']
            state.error_message = result_df.get('error_message', None)
            logger.info(f"SQL Result from dict: {state.sql_result}")
        else: # Error string from _execute_sql_sync
            state.sql_result = str(result_df) if result_df is not None else None
            state.error_message = f"Error executing SQL: {result_df}"
            state.final_answer = f"SQL ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {result_df}"
            logger.error(f"Error executing SQL (returned as string): {result_df}")
        
        # SQL ê²°ê³¼ê°€ ì¡´ì¬í•˜ë©´ ì˜¤ë¥˜ ë©”ì‹œì§€ëŠ” Noneìœ¼ë¡œ ì„¤ì •
        if state.sql_result and not state.error_message:
            state.error_message = None

    except Exception as e:
        logger.error(f"Exception executing SQL: {e}", exc_info=True)
        state.error_message = f"Exception executing SQL: {e}"
        state.sql_result = None
        state.final_answer = f"ì£„ì†¡í•©ë‹ˆë‹¤, SQL ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•˜ëŠ” ì¤‘ ì˜ˆì™¸ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    logger.info("--- Exiting execute_sql_node ---")
    return state

async def summarize_sql_result_node(state: AgentState, config: Optional[RunnableConfig] = None):
    logger.info("--- Entering summarize_sql_result_node ---")
    
    # ë””ë²„ê¹…ì„ ìœ„í•´ ìƒíƒœ ì¶œë ¥
    logger.info(f"State before summarize_sql_result_node: error_message={state.error_message}, sql_result type={type(state.sql_result)}, sql_query={state.sql_query}")
    
    # ê²°ê³¼ê°€ Noneì¸ ê²½ìš°
    if not state.sql_result:
        logger.warning("SQL result is None or empty.")
        if state.error_message:
            state.final_answer = f"SQL ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ê²°ê³¼ë¥¼ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {state.error_message}"
        else:
            state.final_answer = "ìš”ì•½í•  SQL ì‹¤í–‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        return state
    
    # SQL ê²°ê³¼ê°€ ë¬¸ìì—´ì¸ì§€ í™•ì¸í•˜ê³ , ë¬¸ìì—´ì´ ì•„ë‹ˆë©´ ë¬¸ìì—´ë¡œ ë³€í™˜
    if not isinstance(state.sql_result, str):
        logger.info(f"Converting SQL result from {type(state.sql_result)} to string")
        state.sql_result = str(state.sql_result)
    
    # SQL ê²°ê³¼ê°€ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ì€ì§€ í™•ì¸
    if len(state.sql_result.strip()) < 5:
        logger.warning(f"SQL result is suspiciously short: '{state.sql_result}'")
        state.final_answer = "SQL ì¿¼ë¦¬ ê²°ê³¼ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì²˜ë¦¬í•  ìˆ˜ ì—†ëŠ” í˜•ì‹ì…ë‹ˆë‹¤."
        return state
        
    # ë°ì´í„°í”„ë ˆì„ ì¶œë ¥ í˜•ì‹ì´ ì œëŒ€ë¡œ ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if "count" in state.sql_result and any(c.isdigit() for c in state.sql_result):
        logger.info("SQL result contains 'count' and numbers, which looks like a valid result")
    
    logger.info(f"SQL Result for summary (processed): {state.sql_result[:200]}...")
    logger.debug(f"Summarizing SQL result for user query: {state.user_query}")
    logger.debug(f"SQL Query for summary: {state.sql_query}")

    try:
        # ìƒì„¸ ë¡œê¹… ì¶”ê°€
        logger.info(f"Preparing to call LLM with SQL Query: {state.sql_query}")
        logger.info(f"SQL Result first 100 chars: {state.sql_result[:100]}")
        
        # ì‚¬ìš©ì ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì°¾ê¸°
        user_messages = [msg for msg in state.messages if isinstance(msg, HumanMessage)]
        last_user_message = user_messages[-1].content if user_messages else "SQL ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”."
        logger.info(f"Last user message: {last_user_message[:100]}")
        
        # ì²´ì¸ êµ¬ì„± ë° í˜¸ì¶œ
        # SQL ì¿¼ë¦¬ì™€ ê²°ê³¼ë¥¼ ëª…ì‹œì ìœ¼ë¡œ í¬í•¨í•˜ëŠ” ì‚¬ìš©ì ë©”ì‹œì§€ ìƒì„±
        explicit_sql_message = HumanMessage(
            content=f"SQL ì¿¼ë¦¬: {state.sql_query}\n\nSQL ê²°ê³¼:\n{state.sql_result}\n\nì´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."
        )
        
        # ê¸°ì¡´ ë©”ì‹œì§€ ë³µì‚¬ ë° SQL ë©”ì‹œì§€ ì¶”ê°€
        messages_with_sql = state.messages.copy()
        messages_with_sql.append(explicit_sql_message)
        
        # ì²´ì¸ êµ¬ì„± ë° í˜¸ì¶œ
        chain = sql_result_summary_chat_prompt_agent3 | llm
        response = await chain.ainvoke(
            {
                "messages": messages_with_sql, 
                "sql_query": str(state.sql_query), 
                "sql_result": state.sql_result
            },
            config=config
        )
        
        # ì‘ë‹µ í™•ì¸ ë° ì²˜ë¦¬
        state.final_answer = response.content
        if not state.final_answer or len(state.final_answer.strip()) < 10:
            logger.warning(f"LLM returned empty or very short response: '{state.final_answer}'")
            state.final_answer = f"SQL ì¿¼ë¦¬ '{state.sql_query}'ì˜ ê²°ê³¼ëŠ” {state.sql_result}ì…ë‹ˆë‹¤."
            
        logger.info(f"Generated summary: {state.final_answer}")
        state.error_message = None # Clear previous errors
    except Exception as e:
        logger.error(f"Error summarizing SQL result: {e}", exc_info=True)
        state.error_message = f"Error summarizing SQL result: {e}"
        state.final_answer = f"ì£„ì†¡í•©ë‹ˆë‹¤, SQL ê²°ê³¼ë¥¼ ìš”ì•½í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    logger.info("--- Exiting summarize_sql_result_node ---")
    return state

async def general_question_node(state: AgentState, config: Optional[RunnableConfig] = None):
    logger.info("--- Entering general_question_node ---")
    if not state.messages:
        logger.error("general_question_node: No messages in state. Cannot generate answer.")
        state.error_message = "No input message found for general question."
        state.final_answer = "ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•œ ì…ë ¥ ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤."
        return state

    logger.debug(f"Answering general question from user query (from state): {state.user_query}")
    logger.info(f"Invoking general_chat_prompt_agent3 with messages: {state.messages}")
    chain = general_chat_prompt_agent3 | llm
    try:
        # Pass the full message history to the chain
        response = await chain.ainvoke({"messages": state.messages}, config=config)
        state.final_answer = response.content
        logger.info(f"Generated general answer: {state.final_answer}")
        state.error_message = None # Clear previous errors
    except Exception as e:
        logger.error(f"Error answering general question: {e}", exc_info=True)
        state.error_message = f"Error answering general question: {e}"
        state.final_answer = f"ì£„ì†¡í•©ë‹ˆë‹¤, ì¼ë°˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    logger.info("--- Exiting general_question_node ---")
    return state

async def category_predict_node(state: AgentState, config: Optional[RunnableConfig] = None) -> Dict[str, Any]:
    print("--- CATEGORY PREDICT NODE (Telecom Churn Prediction with Csv File Content) ---")

    # --- ê²½ë¡œ ì„¤ì • ---
    base_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
    MODEL_PATH = os.path.join(base_path, 'churn_predictor_pipeline.pkl')
    CATEGORICAL_COLS_PATH = os.path.join(base_path, 'categorical_cols.pkl')
    LABEL_ENCODERS_PATH = os.path.join(base_path, 'label_encoders.pkl')

    EXPECTED_FEATURE_ORDER = [
        'seniorcitizen', 'partner', 'dependents', 'tenure', 'phoneservice',
        'multiplelines', 'onlinesecurity', 'onlinebackup', 'techsupport',
        'contract', 'paperlessbilling', 'paymentmethod', 'monthlycharges', 'totalcharges',
        'new_totalservices', 'new_avg_charges', 'new_increase', 'new_avg_service_fee',
        'charge_increased', 'charge_growth_rate', 'is_auto_payment',
        'expected_contract_months', 'contract_gap'
    ]
    CUSTOMER_ID_COL = 'customerid'
    PREDICTION_THRESHOLD = 0.312

    csv_data_str: Optional[str] = None

    # 1. state.csv_file_content (LangGraph Studioì˜ 'Csv File Content' í•„ë“œ) í™•ì¸
    if state.csv_file_content:
        print("INFO: Using CSV data from state.csv_file_content.")
        csv_data_str = state.csv_file_content
    # 2. state.user_query (Chat ë˜ëŠ” Messages ì…ë ¥) í™•ì¸
    elif hasattr(state, 'user_query') and state.user_query:
        print(f"INFO: Attempting to use state.user_query for CSV data. Content (first 100 chars): '{state.user_query[:100]}...'")
        # 2a. state.user_queryë¥¼ íŒŒì¼ ê²½ë¡œë¡œ ì‹œë„
        if os.path.exists(state.user_query):
            try:
                print(f"INFO: state.user_query '{state.user_query}' is an existing path. Reading file.")
                def read_file_sync(path):
                    with open(path, 'r', encoding='utf-8') as f_sync:
                        return f_sync.read()
                csv_data_str = await asyncio.to_thread(read_file_sync, state.user_query)
                if not csv_data_str:
                    print(f"WARNING: File at '{state.user_query}' was empty.")
            except Exception as e:
                print(f"WARNING: Error reading file from state.user_query path '{state.user_query}': {e}. Will attempt to treat as raw content.")
        
        # 2b. state.user_queryë¥¼ íŒŒì¼ ê²½ë¡œë¡œ ì½ì§€ ëª»í–ˆê±°ë‚˜, ê²½ë¡œê°€ ì•„ë‹ˆì—ˆë‹¤ë©´ ì›ë³¸ CSV ë‚´ìš©ìœ¼ë¡œ ê°„ì£¼
        if csv_data_str is None: # íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ë˜ëŠ” ê²½ë¡œê°€ ì•„ë‹ˆì—ˆìŒ
            print("INFO: Treating state.user_query as raw CSV content.")
            csv_data_str = state.user_query # pd.read_csvê°€ ì´í›„ì— íŒŒì‹± ì‹œë„

    # CSV ë°ì´í„°ë¥¼ ì–´ë””ì—ì„œë„ ì°¾ì§€ ëª»í•œ ê²½ìš° ì˜¤ë¥˜ ë°˜í™˜
    if csv_data_str is None:
        error_message_parts = ["âŒ ì˜¤ë¥˜: CSV ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]
        checked_sources = ["'Csv File Content' í•„ë“œ"]
        if hasattr(state, 'user_query'):
            checked_sources.append("'User Query' / ì±„íŒ… ë©”ì‹œì§€ (íŒŒì¼ ê²½ë¡œ ë˜ëŠ” CSV ë‚´ìš© ì§ì ‘ ì…ë ¥)")
        error_message_parts.append(f"í™•ì¸í•œ ì…ë ¥ ì†ŒìŠ¤: {', '.join(checked_sources)}.")
        error_message_parts.append("Csv File Content í•„ë“œì— ì§ì ‘ CSV ë‚´ìš©ì„ ë¶™ì—¬ë„£ê±°ë‚˜, ì±„íŒ…ìœ¼ë¡œ CSV íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œ ë˜ëŠ” CSV ë‚´ìš© ìì²´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        final_answer = "\n".join(error_message_parts)
        current_messages = state.messages # Get current messages
        updated_messages = current_messages + [AIMessage(content=final_answer)] if current_messages else [AIMessage(content=final_answer)]
        return {"messages": updated_messages, "final_answer": final_answer, "error_message": final_answer}

    print(f"INFO: CSV data obtained. Length: {len(csv_data_str)}. Preview (first 200 chars): {csv_data_str[:200]}...")

    try:
        # --- ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ ê°ì²´ ë¹„ë™ê¸° ë¡œë“œ ---
        pipeline_final = await asyncio.to_thread(joblib.load, MODEL_PATH)
        CATEGORICAL_COLS = await asyncio.to_thread(joblib.load, CATEGORICAL_COLS_PATH)
        label_encoders = await asyncio.to_thread(joblib.load, LABEL_ENCODERS_PATH)

        # --- CSV ë¬¸ìì—´ â†’ DataFrame ë³€í™˜ ---
        if not csv_data_str: # ì´ì¤‘ í™•ì¸, csv_data_strì´ Noneì´ë‚˜ ë¹ˆ ë¬¸ìì—´ì´ë©´ ì—ëŸ¬ ë°œìƒ ë°©ì§€
            final_answer = "âŒ ì˜¤ë¥˜: ë‚´ë¶€ ë¡œì§ ì˜¤ë¥˜ - CSV ë°ì´í„° ë¬¸ìì—´ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
            current_messages = state.messages # Get current messages
            updated_messages = current_messages + [AIMessage(content=final_answer)] if current_messages else [AIMessage(content=final_answer)]
            return {"messages": updated_messages, "final_answer": final_answer, "error_message": final_answer}
        # --- BEGIN REVISED CSV DATA CLEANING LOGIC ---
        raw_lines = csv_data_str.strip().splitlines()
        cleaned_lines = []

        if not raw_lines:
            final_answer = "âŒ ì˜¤ë¥˜: CSV ë°ì´í„° ë¬¸ìì—´ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
            current_messages = state.messages
            updated_messages = current_messages + [AIMessage(content=final_answer)] if current_messages else [AIMessage(content=final_answer)]
            return {"messages": updated_messages, "final_answer": final_answer, "error_message": final_answer}

        MIN_COMMAS_THRESHOLD = 1  # ì‰¼í‘œê°€ ì´ ê°œìˆ˜ ì´ìƒì´ë©´ 'ê°•ë ¥í•œ' CSV ë¼ì¸ìœ¼ë¡œ ê°„ì£¼

        # 'ê°•ë ¥í•œ' CSV ë¼ì¸ë“¤ì˜ ì¸ë±ìŠ¤ë¥¼ ì°¾ìŒ
        strong_csv_indices = [i for i, line in enumerate(raw_lines) if line.count(',') >= MIN_COMMAS_THRESHOLD]

        if strong_csv_indices:
            # 'ê°•ë ¥í•œ' ë¼ì¸ë“¤ì´ ì¡´ì¬í•˜ë©´, ì´ë“¤ì˜ ë²”ìœ„ë¥¼ í•µì‹¬ CSV ë¸”ë¡ìœ¼ë¡œ ê°„ì£¼
            core_block_start_idx = strong_csv_indices[0]
            core_block_end_idx = strong_csv_indices[-1]

            if core_block_start_idx > 0:
                print(f"INFO: CSV ì‹œì‘ ì „ {core_block_start_idx}ê°œì˜ ë¼ì¸ì„ ì§ˆë¬¸ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì œê±°í•©ë‹ˆë‹¤. ì²«ë²ˆì§¸ ì œê±°ëœ ë¼ì¸: '{raw_lines[0][:100]}...'", flush=True)
            if core_block_end_idx < len(raw_lines) - 1:
                print(f"INFO: CSV ì¢…ë£Œ í›„ {len(raw_lines) - 1 - core_block_end_idx}ê°œì˜ ë¼ì¸ì„ ì§ˆë¬¸ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì œê±°í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ ì œê±°ëœ ë¼ì¸: '{raw_lines[-1][:100]}...'", flush=True)

            # í•µì‹¬ CSV ë¸”ë¡ ì¶”ì¶œ
            core_block_lines = raw_lines[core_block_start_idx : core_block_end_idx + 1]

            if not core_block_lines: # Should not happen if strong_csv_indices is not empty
                final_answer = "âŒ ì˜¤ë¥˜: CSV í•µì‹¬ ë¸”ë¡ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                current_messages = state.messages
                updated_messages = current_messages + [AIMessage(content=final_answer)] if current_messages else [AIMessage(content=final_answer)]
                return {"messages": updated_messages, "final_answer": final_answer, "error_message": final_answer}

            # í•µì‹¬ ë¸”ë¡ì˜ ì²« ì¤„ì„ í—¤ë”ë¡œ ì‚¬ìš©
            header_line = core_block_lines[0]
            cleaned_lines.append(header_line)
            commas_in_header = header_line.count(',') # í—¤ë”ëŠ” MIN_COMMAS_THRESHOLD ì´ìƒì¼ ê²ƒì„

            # í•µì‹¬ ë¸”ë¡ ë‚´ì˜ ë°ì´í„° ë¼ì¸ ì •ì œ
            for i in range(1, len(core_block_lines)):
                line = core_block_lines[i]
                if line.count(',') >= MIN_COMMAS_THRESHOLD: # 'ê°•ë ¥í•œ' ë°ì´í„° ë¼ì¸
                    cleaned_lines.append(line)
                elif commas_in_header >= MIN_COMMAS_THRESHOLD: # í—¤ë”ëŠ” 'ê°•í–ˆìœ¼ë‚˜', í˜„ì¬ ë¼ì¸ì€ 'ì•½í•¨' (0ê°œì˜ ì‰¼í‘œ)
                    if not line.strip(): # ì˜ë„ì ìœ¼ë¡œ ë¹„ì–´ìˆëŠ” ë¼ì¸ì´ë©´ ìœ ì§€
                        cleaned_lines.append(line)
                    else: # ë‚´ìš©ì´ ìˆëŠ” 0ì‰¼í‘œ ë¼ì¸ì€ ë¸”ë¡ ë‚´ ì§ˆë¬¸ìœ¼ë¡œ ì˜ì‹¬í•˜ì—¬ í•„í„°ë§
                        print(f"INFO: CSV ë¸”ë¡ ë‚´ì—ì„œ 0ê°œì˜ ì‰¼í‘œë¥¼ ê°€ì§„ ë¹„ì–´ìˆì§€ ì•Šì€ ë¼ì¸ì„ í•„í„°ë§í•©ë‹ˆë‹¤: '{line[:100]}...'", flush=True)
                else: # í—¤ë”ë„ 'ì•½í–ˆê³ ' (ì´ ê²½ìš°ëŠ” strong_csv_indices ë¡œì§ìƒ ê±°ì˜ ì—†ìŒ) í˜„ì¬ ë¼ì¸ë„ 'ì•½í•˜ë©´' ì¼ë‹¨ í¬í•¨
                    cleaned_lines.append(line)
        else:
            # 'ê°•ë ¥í•œ' CSV ë¼ì¸ì´ í•˜ë‚˜ë„ ì—†ìŒ (ëª¨ë“  ë¼ì¸ì˜ ì‰¼í‘œ < MIN_COMMAS_THRESHOLD, ì˜ˆ: ëª¨ë‘ 0ê°œ)
            # ì´ ê²½ìš° ë‹¨ì¼ ì—´ CSVì´ê±°ë‚˜ ì „ì²´ê°€ ì§ˆë¬¸ì¼ ìˆ˜ ìˆìŒ. ì¼ë‹¨ ëª¨ë“  ë¼ì¸ì„ ì‚¬ìš©.
            print(f"INFO: ëª¨ë“  ë¼ì¸ì˜ ì‰¼í‘œ ê°œìˆ˜ê°€ {MIN_COMMAS_THRESHOLD}ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤. ë‹¨ì¼ ì—´ CSVë¡œ ê°„ì£¼í•˜ê±°ë‚˜ ì „ì²´ê°€ ì§ˆë¬¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë“  ë¼ì¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.", flush=True)
            cleaned_lines = raw_lines

        if not cleaned_lines:
            final_answer = "âŒ ì˜¤ë¥˜: CSV ë°ì´í„° ì •ì œ í›„ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
            current_messages = state.messages
            updated_messages = current_messages + [AIMessage(content=final_answer)] if current_messages else [AIMessage(content=final_answer)]
            return {"messages": updated_messages, "final_answer": final_answer, "error_message": final_answer}
        
        cleaned_csv_data_str = "\n".join(cleaned_lines)
        
        print(f"INFO: ìµœì¢… ì •ì œëœ CSV ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²« 200ì): {cleaned_csv_data_str[:200]}...", flush=True)
        input_df = await asyncio.to_thread(pd.read_csv, io.StringIO(cleaned_csv_data_str))
        # --- END REVISED CSV DATA CLEANING LOGIC ---

        if CUSTOMER_ID_COL not in input_df.columns:
            final_answer = f"âŒ ì˜¤ë¥˜: '{CUSTOMER_ID_COL}' ì»¬ëŸ¼ì´ CSVì— ì—†ìŠµë‹ˆë‹¤."
            current_messages = state.messages # Get current messages
            updated_messages = current_messages + [AIMessage(content=final_answer)] if current_messages else [AIMessage(content=final_answer)]
            return {"messages": updated_messages, "final_answer": final_answer, "error_message": final_answer}

        customer_ids = input_df[CUSTOMER_ID_COL]
        X_predict = input_df.drop(columns=[CUSTOMER_ID_COL], errors='ignore')

        # --- ë²”ì£¼í˜• ì»¬ëŸ¼ ì¸ì½”ë”© ---
        for col in CATEGORICAL_COLS:
            if col in X_predict.columns:
                if col in label_encoders:
                    le = label_encoders[col]
                    X_predict[col] = X_predict[col].apply(
                        lambda x: le.transform([x])[0] if x in le.classes_ else -1
                    )
                else:
                    print(f"WARNING: Label encoder for column '{col}' not found. Skipping encoding.")
            else:
                print(f"WARNING: Categorical column '{col}' not found in input CSV. Skipping.")

        # --- ëˆ„ë½ëœ ì»¬ëŸ¼ ì²˜ë¦¬ (ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ëª¨ë“  ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸) ---
        missing_cols = set(EXPECTED_FEATURE_ORDER) - set(X_predict.columns)
        for col in missing_cols:
            print(f"INFO: Adding missing column '{col}' with default value 0.")
            X_predict[col] = 0 # ë˜ëŠ” np.nan ë“± ì ì ˆí•œ ê¸°ë³¸ê°’

        # --- ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬ ---
        X_predict = X_predict[EXPECTED_FEATURE_ORDER]

        # --- ì˜ˆì¸¡ ìˆ˜í–‰ ---
        predictions_proba = await asyncio.to_thread(pipeline_final.predict_proba, X_predict)
        predictions = (predictions_proba[:, 1] >= PREDICTION_THRESHOLD).astype(int)

        # --- ê²°ê³¼ ìƒì„± ---
        results_df = pd.DataFrame({
            CUSTOMER_ID_COL: customer_ids,
            'Churn Probability': predictions_proba[:, 1],
            'Churn Prediction (Threshold 0.312)': predictions
        })
        results_df['Churn Prediction (Threshold 0.312)'] = results_df['Churn Prediction (Threshold 0.312)'].map({1: 'Yes', 0: 'No'})

        final_answer = "ğŸ“Š ê³ ê° ì´íƒˆ ì˜ˆì¸¡ ê²°ê³¼:\n" + results_df.to_string(index=False)
        print(f"Prediction successful. Result preview: {final_answer[:200]}...")
        current_messages = state.messages # Get current messages
        updated_messages = current_messages + [AIMessage(content=final_answer)] if current_messages else [AIMessage(content=final_answer)]
        return {"messages": updated_messages, "final_answer": final_answer, "error_message": None}

    except pd.errors.EmptyDataError:
        error_msg = "âŒ ì˜¤ë¥˜: ì…ë ¥ëœ CSV ë°ì´í„°ê°€ ë¹„ì–´ ìˆê±°ë‚˜ ì˜ëª»ëœ í˜•ì‹ì…ë‹ˆë‹¤. CSV ë‚´ìš©ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”."
        print(f"ERROR: {error_msg}")
        current_messages = state.messages # Get current messages
        updated_messages = current_messages + [AIMessage(content=error_msg)] if current_messages else [AIMessage(content=error_msg)]
        return {"messages": updated_messages, "final_answer": error_msg, "error_message": error_msg}
    except FileNotFoundError as e:
        error_msg = f"âŒ ì˜¤ë¥˜: ëª¨ë¸ ë˜ëŠ” ì „ì²˜ë¦¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. ({e})"
        print(f"ERROR: {error_msg}")
        current_messages = state.messages # Get current messages
        updated_messages = current_messages + [AIMessage(content=error_msg)] if current_messages else [AIMessage(content=error_msg)]
        return {"messages": updated_messages, "final_answer": error_msg, "error_message": error_msg}
    except KeyError as e:
        error_msg = f"âŒ ì˜¤ë¥˜: CSV ë°ì´í„°ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆê±°ë‚˜, ëª¨ë¸ í•™ìŠµ ì‹œ ì‚¬ìš©ëœ ì»¬ëŸ¼ê³¼ ë‹¤ë¦…ë‹ˆë‹¤. (ì˜¤ë¥˜ ì»¬ëŸ¼: {e}) CSV íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
        print(f"ERROR: {error_msg}")
        current_messages = state.messages # Get current messages
        updated_messages = current_messages + [AIMessage(content=error_msg)] if current_messages else [AIMessage(content=error_msg)]
        return {"messages": updated_messages, "final_answer": error_msg, "error_message": error_msg}
    except ValueError as e: # Often from pandas if query is malformed for read_sql_query
        error_msg = f"âŒ ì˜¤ë¥˜: ë°ì´í„° ë³€í™˜ ì¤‘ ê°’ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. CSV ë°ì´í„° íƒ€ì…ì„ í™•ì¸í•´ì£¼ì„¸ìš”. (ì˜¤ë¥˜: {e})"
        print(f"ERROR: {error_msg}")
        current_messages = state.messages # Get current messages
        updated_messages = current_messages + [AIMessage(content=error_msg)] if current_messages else [AIMessage(content=error_msg)]
        return {"messages": updated_messages, "final_answer": error_msg, "error_message": error_msg}
    except Exception as e:
        error_msg = f"âŒ ì˜ˆì¸¡ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}"
        print(f"ERROR: {error_msg}")
        current_messages = state.messages # Get current messages
        updated_messages = current_messages + [AIMessage(content=error_msg)] if current_messages else [AIMessage(content=error_msg)]
        return {"messages": updated_messages, "final_answer": error_msg, "error_message": error_msg}

async def create_visualization_node(state: AgentState, config: Optional[RunnableConfig] = None):
    logger.info("--- Entered create_visualization_node ---")
    
    # Validate sql_result upfront
    if (
        not state.sql_result or 
        not isinstance(state.sql_result, list) or
        (state.sql_result and not all(isinstance(row, dict) for row in state.sql_result))
    ):
        logger.warning(f"SQL result is None, not a list of dicts, or empty. Type: {type(state.sql_result)}")
        state.error_message = "SQL result is None, not a list of dicts, or empty."
        state.final_answer = "ì‹œê°í™”í•  SQL ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ì˜¬ë°”ë¥´ì§€ ì•Šì€ í˜•ì‹ì…ë‹ˆë‹¤."
        state.visualization_output = "```mermaid\ngraph TD\n  A[ë°ì´í„° ì—†ìŒ ë˜ëŠ” í˜•ì‹ ì˜¤ë¥˜]\n```"
        return state

    try:
        llm = ChatOpenAI(model="gpt-4.1-2025-04-14", temperature=0)
        # OPENAI_API_KEYëŠ” í™˜ê²½ ë³€ìˆ˜ì—ì„œ ìë™ìœ¼ë¡œ ë¡œë“œë©ë‹ˆë‹¤.

        # SQL ê²°ê³¼ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜ (ensure_ascii=Falseë¡œ í•œê¸€ ì²˜ë¦¬)
        # í”„ë¡¬í”„íŠ¸ì— ë„ˆë¬´ ê¸´ ë‚´ìš©ì´ ë“¤ì–´ê°€ì§€ ì•Šë„ë¡ ê¸¸ì´ ì œí•œ
        sql_result_str = json.dumps(state.sql_result, indent=2, ensure_ascii=False)
        MAX_PROMPT_SQL_RESULT_LEN = 8000 # LLM í”„ë¡¬í”„íŠ¸ í† í° ì œí•œ ê³ ë ¤
        
        if len(sql_result_str) > MAX_PROMPT_SQL_RESULT_LEN:
            truncation_note = f"... (ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ì–´ ì¼ë¶€ê°€ ì˜ë ¸ìŠµë‹ˆë‹¤. ì›ë³¸ í–‰ ìˆ˜: {len(state.sql_result)})"
            sql_result_str = sql_result_str[:MAX_PROMPT_SQL_RESULT_LEN - len(truncation_note) - 10] + "\n" + truncation_note
            logger.info(f"Mermaid ìƒì„±ì„ ìœ„í•œ SQL ê²°ê³¼ê°€ í”„ë¡¬í”„íŠ¸ ì œí•œì„ ì´ˆê³¼í•˜ì—¬ ì¼ë¶€ ì˜ë¼ëƒˆìŠµë‹ˆë‹¤. ì›ë³¸ ê¸¸ì´: {len(sql_result_str)}")

        prompt_text = f"""
        [ì‚¬ìš©ì ì§ˆë¬¸]
    {state.user_query}
ë‹¤ìŒì€ SQL ì¿¼ë¦¬ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ ë°ì´í„°ë¥¼ ì‹œê°ì ìœ¼ë¡œ íš¨ê³¼ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆëŠ” Mermaid ë‹¤ì´ì–´ê·¸ë¨(markdown ì½”ë“œë¸”ë¡, ì‹œì‘ê³¼ ëì„ ë°˜ë“œì‹œ ```mermaid ... ```ë¡œ ê°ìŒ€ ê²ƒ)ê³¼, í•´ë‹¹ ì‹œê°í™”ê°€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ ê°„ê²°í•œ í•œêµ­ì–´ ì„¤ëª…(ë‹µë³€)ì„ í•¨ê»˜ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

        - ë°ì´í„°ì˜ íŠ¹ì„±(ì§‘ê³„, ë¶„í¬, ê´€ê³„, ìˆœì„œ ë“±)ì— ë”°ë¼ ë‹¤ìŒ Mermaid ë‹¤ì´ì–´ê·¸ë¨ íƒ€ì… ì¤‘ì—ì„œ ê°€ì¥ ì í•©í•œ ê²ƒì„ ìë™ìœ¼ë¡œ ì„ íƒí•˜ì—¬ ìƒì„±í•´ì£¼ì„¸ìš”. ê° íƒ€ì…ì˜ ì˜ˆì‹œì™€ ê°„ë‹¨í•œ ì„¤ëª…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. (ì‹¤ì œ ë°ì´í„°ì— ë§ê²Œ ë‚´ìš©ì„ ì±„ìš°ê³ , ëª¨ë“  í…ìŠ¤íŠ¸ëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤):

          - **Pie Chart (pie):** ë°ì´í„° í•­ëª©ë“¤ì˜ ì „ì²´ì— ëŒ€í•œ ë¹„ìœ¨ì„ ì›í˜•ìœ¼ë¡œ í‘œí˜„í•©ë‹ˆë‹¤. ê° ì¡°ê°ì˜ í¬ê¸°ê°€ í•´ë‹¹ í•­ëª©ì˜ ë¹„ìœ¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
            ì˜ˆì‹œ:
            ```mermaid
            pie title ì›”ë³„ ì§€ì¶œ ë¹„ìœ¨
                "ì‹ë¹„" : 40
                "êµí†µë¹„" : 20
                "ê³µê³¼ê¸ˆ" : 15
                "ì—¬ê°€" : 25
            ```

          - **Line Chart (xyChart ì‚¬ìš©):** ì‹œê°„ ê²½ê³¼ë‚˜ ìˆœì„œì— ë”°ë¥¸ ë°ì´í„° ê°’ì˜ ë³€í™” ì¶”ì„¸ë¥¼ ì„ ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤. (Mermaid ìµœì‹  ë²„ì „ì—ì„œëŠ” xyChart ì‚¬ìš© ê¶Œì¥)
            ì˜ˆì‹œ:
            ```mermaid
            xychart-beta
                title "ì›”ë³„ ì›¹ì‚¬ì´íŠ¸ ë°©ë¬¸ì ìˆ˜"
                x-axis "ì›”" ["1ì›”", "2ì›”", "3ì›”", "4ì›”", "5ì›”"]
                y-axis "ë°©ë¬¸ì ìˆ˜" 0 --> 1000
                line [300, 450, 600, 500, 750]
            ```

          - **Bar Chart (xyChart ì‚¬ìš©):** ë°ì´í„° í•­ëª©ë“¤ì˜ ê°’ì„ ë§‰ëŒ€ë¡œ í‘œí˜„í•˜ì—¬ ë¹„êµí•©ë‹ˆë‹¤. `xyChart`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°” ì°¨íŠ¸ë„ ê·¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            ì˜ˆì‹œ:
            ```mermaid
            xychart-beta
                title "ë¶„ê¸°ë³„ ì œí’ˆ íŒë§¤ëŸ‰"
                x-axis "ë¶„ê¸°" ["1ë¶„ê¸°", "2ë¶„ê¸°", "3ë¶„ê¸°", "4ë¶„ê¸°"]
                y-axis "íŒë§¤ëŸ‰ (ë‹¨ìœ„: ê°œ)" 0 --> 200
                bar [80, 120, 150, 100]
            ```

          - **Graph TD (Flowchart):** ì‘ì—…, í”„ë¡œì„¸ìŠ¤, ì‹œìŠ¤í…œ êµ¬ì„± ìš”ì†Œ ê°„ì˜ ìˆœì„œ, íë¦„, ê´€ê³„ë¥¼ ë…¸ë“œì™€ í™”ì‚´í‘œë¡œ í‘œí˜„í•©ë‹ˆë‹¤.
            ì˜ˆì‹œ:
            ```mermaid
            graph TD
                A[ë°ì´í„° ì…ë ¥] --> B(ë°ì´í„° ì²˜ë¦¬);
                B --> C{{ì¡°ê±´ ë¶„ê¸°}};
                C -- ì¡°ê±´1 --> D[ê²°ê³¼ A];
                C -- ì¡°ê±´2 --> E[ê²°ê³¼ B];
            ```

          - **C4Context Diagram:** ì†Œí”„íŠ¸ì›¨ì–´ ì‹œìŠ¤í…œê³¼ ê·¸ ì£¼ë³€ í™˜ê²½(ì‚¬ìš©ì, ì™¸ë¶€ ì‹œìŠ¤í…œ) ê°„ì˜ ìƒí˜¸ì‘ìš©ì„ ë†’ì€ ìˆ˜ì¤€ì—ì„œ ê°œëµì ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ì…ë‹ˆë‹¤.
            ì˜ˆì‹œ:
            ```mermaid
            C4Context
              title ì˜¨ë¼ì¸ ì‡¼í•‘ëª° ì‹œìŠ¤í…œ ì»¨í…ìŠ¤íŠ¸
              Enterprise_Boundary(b0, "íšŒì‚¬ ì‹œìŠ¤í…œ ê²½ê³„") {{
                Person(customer, "ê³ ê°", "ìƒí’ˆì„ êµ¬ë§¤í•˜ëŠ” ì‚¬ìš©ì")
                System(shoppingMall, "ì˜¨ë¼ì¸ ì‡¼í•‘ëª°", "ìƒí’ˆ ê²€ìƒ‰, ì£¼ë¬¸, ê²°ì œ ê¸°ëŠ¥ ì œê³µ")

                System_Ext(paymentGateway, "ê²°ì œ ì‹œìŠ¤í…œ", "ì™¸ë¶€ PGì‚¬ ì—°ë™")
                System_Ext(deliverySystem, "ë°°ì†¡ ì‹œìŠ¤í…œ", "ì™¸ë¶€ ë°°ì†¡ ì—…ì²´ ì—°ë™")
              }}

              Rel(customer, shoppingMall, "ìƒí’ˆ ì£¼ë¬¸")
              Rel(shoppingMall, paymentGateway, "ê²°ì œ ìš”ì²­")
              Rel(shoppingMall, deliverySystem, "ë°°ì†¡ ìš”ì²­")
            ```

          - **Class Diagram:** ì‹œìŠ¤í…œì„ êµ¬ì„±í•˜ëŠ” í´ë˜ìŠ¤ë“¤ì˜ ì†ì„±, ì˜¤í¼ë ˆì´ì…˜(ë©”ì„œë“œ) ë° í´ë˜ìŠ¤ ê°„ì˜ ê´€ê³„(ìƒì†, ì—°ê´€, ì˜ì¡´ ë“±)ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
            ì˜ˆì‹œ:
            ```mermaid
            classDiagram
              ì£¼ë¬¸ <|-- ì¼ë°˜ì£¼ë¬¸
              ì£¼ë¬¸ <|-- íŠ¹ë³„ì£¼ë¬¸
              ì£¼ë¬¸ : +String ì£¼ë¬¸ë²ˆí˜¸
              ì£¼ë¬¸ : +Date ì£¼ë¬¸ì¼ì
              ì£¼ë¬¸ : +calculateTotalPrice()
              class ì¼ë°˜ì£¼ë¬¸{{
                  +String ë°°ì†¡ì§€
                  +calculateShippingCost()
              }}
              class ê³ ê°{{
                  +String ì´ë¦„
                  +List<ì£¼ë¬¸> ì£¼ë¬¸ë‚´ì—­
                  +placeOrder(ì£¼ë¬¸)
              }}
              ê³ ê° "1" -- "*" ì£¼ë¬¸ : ì£¼ë¬¸í•œë‹¤
            ```

          - **Gantt Chart:** í”„ë¡œì íŠ¸ì˜ ì‘ì—…(íƒœìŠ¤í¬) ëª©ë¡, ê° ì‘ì—…ì˜ ì‹œì‘ì¼, ì¢…ë£Œì¼, ê¸°ê°„ ë“±ì„ ì‹œê°„ ì¶•ì— ë§‰ëŒ€ë¡œ í‘œì‹œí•˜ì—¬ ì¼ì • ê´€ë¦¬ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
            ì˜ˆì‹œ:
            ```mermaid
            gantt
                dateFormat  YYYY-MM-DD
                title       í”„ë¡œì íŠ¸ A ì§„í–‰ í˜„í™©
                excludes    weekends

                section ê¸°íš ë‹¨ê³„
                ìš”êµ¬ì‚¬í•­ ë¶„ì„    :task1, 2024-01-01, 7d
                í™”ë©´ ì„¤ê³„       :task2, after task1, 5d

                section ê°œë°œ ë‹¨ê³„
                ê¸°ëŠ¥ ê°œë°œ      :task3, after task2, 20d
                í…ŒìŠ¤íŠ¸         :task4, after task3, 10d
            ```
        - ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì™¸ì˜ ë¶ˆí•„ìš”í•œ ì„¤ëª…ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        - ë…¸ë“œÂ·ë¼ë²¨ ì´ë¦„ì— íŠ¹ìˆ˜ë¬¸ì(", \\n ë“±)ëŠ” Mermaid ë¬¸ë²•ì— ë§ê²Œ ì ì ˆíˆ ì´ìŠ¤ì¼€ì´í”„í•˜ê±°ë‚˜ ì œê±°í•˜ì„¸ìš”.
        - ëª¨ë“  ë‹¤ì´ì–´ê·¸ë¨ ë‚´ í…ìŠ¤íŠ¸(ì„¤ëª…, ë…¸ë“œ, ë¼ë²¨ ë“±)ëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        - ë§ˆì§€ë§‰ì— "ì„¤ëª…:" ì´ë¼ëŠ” ì œëª© ì•„ë˜, ì‹œê°í™”ê°€ ë³´ì—¬ì£¼ëŠ” í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ í•œë‘ ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ì„¤ëª…í•˜ì„¸ìš”.

SQL ê²°ê³¼:
```json
{sql_result_str}
```

Mermaid ë‹¤ì´ì–´ê·¸ë¨ê³¼ ì„¤ëª…:
"""
        messages = [HumanMessage(content=prompt_text)]
        
        logger.info(f"LLMì—ê²Œ Mermaid ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ìš”ì²­ ì¤‘... SQL ê²°ê³¼ (ì¼ë¶€): {sql_result_str[:200]}...")
        
        response = await llm.ainvoke(messages)
        llm_output = response.content.strip()

        logger.info(f"LLMì´ ìƒì„±í•œ Mermaid ë‚´ìš© (ì¼ë¶€): {llm_output[:300]}...")

        # LLM ì¶œë ¥ì´ ì˜¬ë°”ë¥¸ Mermaid ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ì¸ì§€ í™•ì¸
        if llm_output.startswith("```mermaid") and llm_output.endswith("```"):
            state.visualization_output = llm_output
            state.final_answer = llm_output
            state.error_message = None
        else:
            logger.error(f"LLMì´ ìœ íš¨í•œ Mermaid ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ì„ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¶œë ¥: {llm_output}")
            state.error_message = "LLMìœ¼ë¡œë¶€í„° ìœ íš¨í•œ Mermaid ë‹¤ì´ì–´ê·¸ë¨ì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            state.final_answer = "Mermaid ë‹¤ì´ì–´ê·¸ë¨ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. LLMì´ ì˜¬ë°”ë¥¸ í˜•ì‹ì„ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            state.visualization_output = "```mermaid\ngraph TD\n  A[LLM ìƒì„± ì˜¤ë¥˜: ì˜ëª»ëœ í˜•ì‹]\n```"
            
    except Exception as e:
        logger.error(f"LLMì„ í†µí•´ Mermaid ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        state.error_message = f"LLM Mermaid ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)[:100]}"
        state.final_answer = f"LLM Mermaid ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)[:100]}"
        state.visualization_output = "```mermaid\ngraph TD\n  A[ì˜¤ë¥˜ ë°œìƒ: ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ì‹¤íŒ¨]\n```"
        return state # ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ state ë°˜í™˜ ë³´ì¥

        
    logger.info(f"create_visualization_node state after processing: visualization_output length: {len(state.visualization_output or '')}, final_answer length: {len(state.final_answer or '')}")
    return state

def route_sql_output(state: AgentState) -> Literal["create_visualization_node", "summarize_sql_result_node"]:
    choice = state.sql_output_choice
    # If execute_sql_node itself resulted in an error (indicated by error_message and no sql_result)
    # both visualization and summarization nodes have internal logic to handle this.
    # The choice made by the supervisor should still be respected if possible.
    if state.error_message and not state.sql_result:
        print(f"Error detected before routing SQL output: {state.error_message}. Proceeding with choice: {choice}")

    if choice == "visualize":
        print("Routing to create_visualization_node based on sql_output_choice.")
        return "create_visualization_node"
    elif choice == "summarize":
        print("Routing to summarize_sql_result_node based on sql_output_choice.")
        return "summarize_sql_result_node"
    else:
        # Fallback if sql_output_choice is somehow not set for a db_query path
        print(f"Warning: sql_output_choice is '{choice}'. Defaulting to summarize_sql_result_node.")
        return "summarize_sql_result_node"

def route_query(state: AgentState) -> Literal["generate_sql_node", "category_predict_node", "general_question_node"]:
    query_type = state.query_type
    print(f"Routing based on query_type: {query_type}")
    if query_type == "db_query":
        return "generate_sql_node"
    elif query_type == "category_predict_query":
        return "category_predict_node"
    elif query_type == "general_query":
        return "general_question_node"
    else:
        # This case should ideally not be reached if supervisor is strict
        print(f"Warning: Unknown query_type '{query_type}', defaulting to general_question_node.")
        return "general_question_node"

# --- Graph Definition ---
workflow = StateGraph(AgentState)

# Add nodes
workflow.add_node("supervisor", supervisor_node)
workflow.add_node("generate_sql_node", generate_sql_node)
workflow.add_node("execute_sql_node", execute_sql_node)
workflow.add_node("create_visualization_node", create_visualization_node)
workflow.add_node("summarize_sql_result_node", summarize_sql_result_node)
workflow.add_node("general_question_node", general_question_node)
workflow.add_node("category_predict_node", category_predict_node)

# Set entry point
workflow.set_entry_point("supervisor")

# Conditional edges from supervisor
workflow.add_conditional_edges(
    "supervisor",
    route_query,
    {
        "generate_sql_node": "generate_sql_node",
        "category_predict_node": "category_predict_node",
        "general_question_node": "general_question_node"
    }
)

# Edges for DB query flow (now common for 3 branches)
workflow.add_edge("generate_sql_node", "execute_sql_node")
# After execute_sql_node, route to either visualization or summarization
workflow.add_conditional_edges(
    "execute_sql_node",
    route_sql_output,
    {
        "create_visualization_node": "create_visualization_node",
        "summarize_sql_result_node": "summarize_sql_result_node"
    }
)
workflow.add_edge("create_visualization_node", END)
workflow.add_edge("summarize_sql_result_node", END)

# Edges from placeholder nodes to the SQL generation flow
workflow.add_edge("category_predict_node", END)

# Edge for general question
workflow.add_edge("general_question_node", END)

# Compile the graph
app = workflow.compile()
graph = app # For langgraph dev compatibility

# To make it runnable with langgraph dev, ensure it's assigned to 'graph'
# Example of how to run (for testing locally):
# async def main():
#     inputs = {"user_query": "ì§€ë‚œ ë‹¬ ì‚¬ìš©ì ìˆ˜ëŠ” ëª‡ ëª…ì¸ê°€ìš”?"}
#     # For testing, you can invoke the graph like this:
#     # inputs = {"messages": [HumanMessage(content="ìš°ë¦¬ íšŒì‚¬ ì§ì›ë“¤ ì¤‘ ê°€ì¥ ì—°ë´‰ì´ ë†’ì€ ìƒìœ„ 3ëª…ì€ ëˆ„êµ¬ì¸ê°€ìš”?")]}
#     # async for event in app.astream_events(inputs, version="v1"):
#     #     kind = event["event"]
#     #     if kind == "on_chat_model_stream":
#     #         content = event["data"]["chunk"].content
#     #         if content:
#     #             print(content, end="")
#     #     elif kind == "on_tool_end":
#     #         print(f"\nTool Output: {event['data']['output']}")
#     #     # print(f"\n--- Event: {kind} ---")
#     #     # print(event["data"])
# if __name__ == "__main__":
#     import asyncio
#     async def main_test():
#         app_test = await main()
#         inputs = {"input": "ìš°ë¦¬ íšŒì‚¬ í…Œì´ë¸” ëª©ë¡ ì¢€ ë³´ì—¬ì¤˜"}
#     # inputs = {"input": "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?"}
#     async for event in app_test.astream_events(inputs, version="v1"):
#             kind = event["event"]
#             if kind == "on_chat_model_stream":
#                 content = event["data"]["chunk"].content
#                 if content:
#                     print(content, end="")
#                 print(f"\nTool Output: {event['data']['output']}")
#             elif kind == "on_chain_end" and event["name"] == "AgentGraph": # Check for the graph's end
#                 print("\n--- Final State ---")
#                 print(event["data"].get("output")) # Access final output from the event

#     asyncio.run(main_test())

def _execute_sql_sync(sql_query: str, base_dir_analysis_env: str, base_dir_my_state_env: str) -> Dict[str, Any]:
    # Determine .env path and load environment variables
    dotenv_path_analysis = os.path.join(base_dir_analysis_env, '.env')
    dotenv_path_my_state = os.path.join(base_dir_my_state_env, '.env')
    specific_env_path = None

    if os.path.exists(dotenv_path_analysis):
        specific_env_path = dotenv_path_analysis
    elif os.path.exists(dotenv_path_my_state):
        specific_env_path = dotenv_path_my_state

    if specific_env_path:
        print(f"_execute_sql_sync: Loading .env from: {specific_env_path}")
        load_dotenv(dotenv_path=specific_env_path, override=True)
    else:
        print("_execute_sql_sync: No specific .env file found. Relying on system environment variables or a global .env.")
        load_dotenv(override=True) # Load global .env or system vars

    db_host = os.getenv("DB_HOST")
    db_port = os.getenv("DB_PORT")
    db_name = os.getenv("DB_NAME")
    db_user = os.getenv("DB_USER")
    db_password = os.getenv("DB_PASSWORD")

    if not all([db_host, db_port, db_name, db_user, db_password]):
        error_msg = "_execute_sql_sync: Database connection details missing in environment variables."
        print(error_msg)
        return {"error_message": error_msg, "sql_result": ""}

    conn_string = f"host='{db_host}' port='{db_port}' dbname='{db_name}' user='{db_user}' password='{db_password}'"
    conn = None
    try:
        print(f"_execute_sql_sync: Connecting to DB with: {conn_string.replace(db_password, '****') if db_password else conn_string}")
        conn = psycopg2.connect(conn_string)
        print(f"_execute_sql_sync: Executing SQL: {sql_query}")
        df = pd.read_sql_query(sql_query, conn)
        # Instead of converting to string, we return the DataFrame directly.
        # execute_sql_node will handle converting this DataFrame to a list of dicts.
        print(f"_execute_sql_sync: SQL query executed successfully. DataFrame shape: {df.shape}, Columns: {df.columns.tolist()}")
        return df  # Return the DataFrame object directly
    except (psycopg2.Error, pd.io.sql.DatabaseError) as e:
        error_msg = f"_execute_sql_sync: Database error: {e}"
        print(error_msg)
        return {"error_message": error_msg, "sql_result": ""}
    except ValueError as e: # Often from pandas if query is malformed for read_sql_query
        error_msg = f"_execute_sql_sync: SQL query validation error for pandas: {e}"
        print(error_msg)
        return {"error_message": error_msg, "sql_result": ""}
    except Exception as e:
        error_msg = f"_execute_sql_sync: An unexpected error occurred during SQL execution: {e}"
        print(error_msg)
        return {"error_message": error_msg, "sql_result": ""}
    finally:
        if conn:
            conn.close()
            print("_execute_sql_sync: DB ì—°ê²° ì¢…ë£Œ.")
