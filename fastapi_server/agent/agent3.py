"""LangGraph graph for analysis_agent with supervisor logic.

Handles DB queries and general questions based on node/edge routing.
"""

from __future__ import annotations

import os
from typing import Optional, Dict, Any, List, Annotated, Literal, TypedDict
import asyncio
import io
from dotenv import load_dotenv
import os
import operator # For adding to message history
import psycopg2
import pandas as pd
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage # Added SystemMessage
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder # Added ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableConfig # Added missing import
from langchain_core.output_parsers import JsonOutputParser
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from pydantic import BaseModel, Field
import pandas as pd
import numpy as np
import joblib
from sklearn.preprocessing import LabelEncoder
import logging # Added logging
from datetime import datetime
import re # Added import for regex
import json # Added for direct OpenAI call
from openai import OpenAI # Added for direct OpenAI call

# Setup logger for agent3
logger = logging.getLogger(__name__)
# Basic logging configuration if not configured elsewhere (e.g., in a main app setup)
if not logger.hasHandlers():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# --- Configuration (Optional - can be used to pass API keys, model names, etc.) ---
class Configuration(TypedDict, total=False):
    openai_api_key: Optional[str]
    db_env_path: Optional[str] # Path to .env file for DB credentials

# --- State Definition ---
class AgentState(BaseModel):
    messages: Annotated[List[BaseMessage], operator.add] = Field(default_factory=list)
    user_query: Optional[str] = None
    csv_file_content: Optional[str] = None
    query_type: Optional[Literal["db_query", "category_predict_query", "general_query"]] = None
    sql_query: Optional[str] = None
    sql_result: Optional[Any] = None
    final_answer: Optional[str] = None
    error_message: Optional[str] = None
    visualization_output: Optional[str] = None
    sql_output_choice: Optional[Literal["summarize", "visualize"]] = None # Decision for SQL output processing

    class Config:
        arbitrary_types_allowed = True # For Annotated and operator.add with BaseMessage
        
    def __post_init__(self):
        # Extract user query from messages if coming from supervisor
        if not self.user_query and self.messages:
            # Extract user input from the last human message
            user_messages = [msg for msg in self.messages if isinstance(msg, HumanMessage)]
            if user_messages:
                self.user_query = user_messages[-1].content
    
    def dict(self):
        """Return dict representation to ensure compatibility with supervisor state"""
        result = super().dict()
        # When returning to supervisor, ensure final answer is properly formatted as a new message
        if self.final_answer and self.messages is not None:
            result["messages"] = self.messages + [AIMessage(content=self.final_answer)]
        return result

# --- LLM and Prompts Setup ---
# Ensure OPENAI_API_KEY is set in your environment or passed via config
llm = ChatOpenAI(temperature=0, model="gpt-4o") # Or your preferred model

# MODIFIED: Changed supervisor_prompt to a ChatPromptTemplate for history
supervisor_chat_prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content="""You are an expert routing assistant. Based on the entire conversation history,
analyze the LATEST user's question to determine the query type.
Respond with a JSON object. The JSON object MUST contain a 'query_type' field
set to one of 'db_query', 'category_predict_query', or 'general_query'.
Focus on the most recent user message for the specific question, but use the provided history for context if needed.
Example: If the user asks 'ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?', respond with {"query_type": "general_query"}.
Example: If the user asks 'ì§€ë‚œ ë‹¬ ì‚¬ìš©ì ë¶„ì„í•´ì¤˜', respond with {"query_type": "db_query"}.
Example: If the user asks 'ì´ ê³ ê°ì€ ì–´ë–¤ ìƒí’ˆì„ ì‚´ ê²ƒ ê°™ì•„?', respond with {"query_type": "category_predict_query"}."""),
    MessagesPlaceholder(variable_name="messages")
])

# MODIFIED: Changed sql_generation_prompt to ChatPromptTemplate
sql_generation_chat_prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content="""You are an expert SQL generation assistant. Based on the user's question from the conversation history and the database schema provided, 
generate an accurate SQL query. \n\n
Database Schema Information:\n
You have access to the following tables and columns. Use this information to construct your queries.\n
Ensure all column and table names match exactly as provided in the schema.\n
If a user asks for information that requires joining tables, please construct the join correctly.\n
If a user's question is ambiguous or lacks detail for a precise query, ask for clarification rather than guessing.\n
Always prioritize accuracy and correctness of the SQL query.\n
If the question implies a date range (e.g., 'last month', 'this year'), calculate the specific dates and use them in the WHERE clause.\n
Today's date is {{current_date}}.\n\n
Table Name: chat_sessions\n
Columns:\n
  - id (uuid)\n
  - user_id (uuid)\n
  - created_at (timestamp with time zone)\n
  - updated_at (timestamp with time zone)\n
  - title (text)\n
  - system_prompt (text)\n
  - agent_profile_id (uuid)\n
  - org_id (uuid)\n\n
Table Name: chat_messages\n
Columns:\n
  - id (uuid)\n
  - session_id (uuid)\n
  - content (text)\n
  - message_type (character varying) -- enum: USER, AI, SYSTEM\n
  - created_at (timestamp with time zone)\n
  - metadata (jsonb)\n
  - tokens (integer)\n
  - model_name (character varying)\n\n
Table Name: documents\n
Columns:\n
  - id (uuid)\n
  - title (character varying)\n
  - content (text)\n
  - s3_url (character varying)\n
  - created_at (timestamp with time zone)\n
  - updated_at (timestamp with time zone)\n
  - user_id (uuid)\n
  - org_id (uuid)\n
  - metadata (jsonb)\n\n
Table Name: embed_chunks\n
Columns:\n
  - id (uuid)\n
  - document_id (uuid)\n
  - text (text)\n
  - vector_id (character varying)\n
  - metadata (jsonb)\n
  - created_at (timestamp with time zone)\n
  - user_id (uuid)\n
  - session_id (uuid)\n\n
Table Name: model_artifacts\n
Columns:\n
  - id (uuid)\n
  - artifact_type (character varying)\n
  - s3_key (text)\n
  - meta (jsonb)\n
  - created_at (timestamp with time zone)\n
  - user_id (uuid)\n\n
Table Name: organizations\n
Columns:\n
  - id (uuid)\n
  - name (character varying)\n
  - created_at (timestamp with time zone)\n\n
Table Name: summary_news_keywords\n
Columns:\n
  - id (uuid)\n
  - date (date)\n
  - keyword (text)\n
  - title (text)\n
  - summary (text)\n
  - category (character varying)\n
  - source (character varying)\n
  - score (double precision)\n
  - created_at (timestamp with time zone)\n
  - org_id (uuid)\n\n
Table Name: users\n
Columns:\n
  - id (uuid)\n
  - email (character varying)\n
  - password (character varying) -- Hashed password, do not query directly for login\n
  - full_name (character varying)\n
  - is_superuser (boolean)\n
  - created_at (timestamp with time zone)\n
  - last_login (timestamp with time zone)\n
  - is_active (boolean)\n
  - is_staff (boolean)\n
  - org_id (uuid)\n\n
Respond with a JSON object that strictly adheres to the Pydantic model `SQLGenerationOutput` shown below.\n
The `sql_query` field MUST contain ONLY the SQL query string, without any surrounding text, explanations, or markdown formatting like ```sql.\n
The `sql_output_choice` field must be one of 'summarize' or 'visualize'. Choose 'visualize' if the user asks for a chart, graph, or any visual representation, or if the query result is likely to be complex and better understood visually (e.g., time series data, comparisons across multiple categories). Otherwise, choose 'summarize'."""),
    MessagesPlaceholder(variable_name="messages")
])

# MODIFIED: Changed sql_result_summary_prompt to ChatPromptTemplate with explicit SQL details
sql_result_summary_chat_prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content="""You are an AI assistant that summarizes SQL query results in Korean. 
Provide a concise and clear natural language answer based on the user's question (from the end of the conversation history) and the SQL query result.
If the result is empty or indicates no data, state that clearly in Korean.
Always respond in Korean regardless of how the question is asked.

You MUST use the SQL result provided to answer the question. Focus on providing a direct, helpful answer that explains what the data shows.

For example, if the SQL returns a count of 22 chat sessions, say "ì´ 22ê°œì˜ ì±„íŒ… ì„¸ì…˜ì´ ìˆìŠµë‹ˆë‹¤." Don't simply acknowledge receipt of the SQL - actually interpret the result and answer the question."""),
    MessagesPlaceholder(variable_name="messages"),
    HumanMessage(content="ë‹¤ìŒì€ SQL ì¿¼ë¦¬ì™€ ê·¸ ê²°ê³¼ì…ë‹ˆë‹¤:\n\nSQL ì¿¼ë¦¬: {sql_query}\n\nSQL ê²°ê³¼:\n{sql_result}\n\nìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.")
])

# MODIFIED: Changed general_answer_prompt to ChatPromptTemplate
general_chat_prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content="Please answer the user's question based on our conversation history. Provide the answer in Korean if the user is speaking Korean or requests it."),
    MessagesPlaceholder(variable_name="messages")
])

# --- Helper function for OpenAI API message format ---
def _lc_messages_to_openai_format(lc_messages: List[BaseMessage]) -> List[Dict[str, str]]:
    openai_messages = []
    for msg in lc_messages:
        if isinstance(msg, HumanMessage):
            openai_messages.append({"role": "user", "content": msg.content})
        elif isinstance(msg, AIMessage):
            openai_messages.append({"role": "assistant", "content": msg.content})
        # SystemMessages from state.messages are less common here as the main system prompt is usually separate
        elif isinstance(msg, SystemMessage):
             openai_messages.append({"role": "system", "content": msg.content})
    return openai_messages

# --- Pydantic Models for Structured Output ---
class SupervisorDecision(BaseModel):
    query_type: str = Field(description="The type of the user's question (db_query, category_predict_query, or general_query)")

class SQLGenerationOutput(BaseModel):
    sql_query: str = Field(description="The generated SQL query. This field MUST contain ONLY the SQL query string, without any surrounding text, explanations, or markdown formatting like ```sql.")
    sql_output_choice: Literal["summarize", "visualize"] = Field(description="The type of output processing required for the SQL result: 'summarize' or 'visualize'.")

# --- Node Functions ---
async def supervisor_node(state: AgentState, config: Optional[RunnableConfig] = None):
    """Determines the type of query (db, category_predict, or general)."""
    logger.info("--- Entering supervisor_node ---")
    if not state.messages:
        logger.warning("Supervisor_node: No messages in state. Cannot determine query type.")
        # Potentially set a default or error state
        state.query_type = "general_query" # Fallback, or handle error appropriately
        state.error_message = "No input message found."
        return state

    logger.debug(f"Supervisor_node: Current messages: {state.messages}")
    
    # The user_query is still useful for logging or if other parts need it, 
    # but the prompt now relies on the full message history.
    if not state.user_query:
        user_messages = [msg for msg in state.messages if isinstance(msg, HumanMessage)]
        if user_messages:
            state.user_query = user_messages[-1].content
        else:
            logger.warning("Supervisor_node: No HumanMessage found to extract user_query.")
            # Fallback if no human message, though MessagesPlaceholder handles history
            state.query_type = "general_query"
            state.error_message = "No human message found in history."
            return state

    logger.info(f"Supervisor_node: User query for routing: '{state.user_query}'")
    logger.debug(f"Supervisor_node: Full messages for prompt: {state.messages}")

    # chain = supervisor_chat_prompt | llm.with_structured_output(SupervisorDecision) # Replaced with direct OpenAI call
    client = OpenAI()
    
    try:
        # Construct System Prompt for OpenAI API
        # supervisor_chat_prompt.messages[0] is the SystemMessage
        system_prompt_content = supervisor_chat_prompt.messages[0].content

        openai_api_messages = [{"role": "system", "content": system_prompt_content}]
        openai_api_messages.extend(_lc_messages_to_openai_format(state.messages))

        logger.debug(f"Supervisor_node: Sending to OpenAI API: {openai_api_messages}")

        completion = await asyncio.to_thread(
            client.chat.completions.create,
            model="gpt-4o", # Ensure this matches the intended model
            messages=openai_api_messages,
            temperature=0.0,
            response_format={"type": "json_object"}
        )
        raw_response_content = completion.choices[0].message.content
        logger.debug(f"Supervisor_node: Raw OpenAI response: {raw_response_content}")
        response_data = json.loads(raw_response_content)
        response = SupervisorDecision(**response_data)
        logger.info(f"Supervisor_node: LLM decision: {response.query_type}")
        state.query_type = response.query_type
        state.error_message = None # Clear previous errors
    except Exception as e:
        logger.error(f"Supervisor_node: Error invoking LLM or parsing output: {e}", exc_info=True)
        state.query_type = "general_query"  # Fallback on error
        state.final_answer = f"ì£„ì†¡í•©ë‹ˆë‹¤, ìš”ì²­ì„ ì´í•´í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
        state.error_message = str(e)
    
    logger.info(f"--- Exiting supervisor_node with query_type: {state.query_type} ---")
    return state

async def generate_sql_node(state: AgentState, config: Optional[RunnableConfig] = None):
    logger.info("--- Entering generate_sql_node ---")
    if not state.messages:
        logger.error("generate_sql_node: No messages in state. Cannot generate SQL.")
        state.error_message = "No input message found for SQL generation."
        state.final_answer = "SQL ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ ì…ë ¥ ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤."
        return state
        
    logger.debug(f"generate_sql_node: User query from state: {state.user_query}") # user_query might be stale, messages is source of truth
    logger.debug(f"generate_sql_node: Full messages for prompt: {state.messages}")

    # Prepare the chain with the new chat prompt
    # chain = sql_generation_chat_prompt | llm.with_structured_output(SQLGenerationOutput) # Replaced with direct OpenAI call
    client = OpenAI()

    try:
        current_date_str = datetime.now().strftime("%Y-%m-%d")
        # sql_generation_chat_prompt.messages[0] is the SystemMessage
        system_prompt_template = sql_generation_chat_prompt.messages[0].content
        system_prompt_content_sql = system_prompt_template.replace("{{current_date}}", current_date_str)

        openai_api_messages_sql = [{"role": "system", "content": system_prompt_content_sql}]
        openai_api_messages_sql.extend(_lc_messages_to_openai_format(state.messages))
        
        logger.debug(f"generate_sql_node: Sending to OpenAI API: {openai_api_messages_sql}")

        completion_sql = await asyncio.to_thread(
            client.chat.completions.create,
            model="gpt-4o", # Ensure this matches the intended model
            messages=openai_api_messages_sql,
            temperature=0.0,
            response_format={"type": "json_object"}
        )
        raw_response_content_sql = completion_sql.choices[0].message.content
        logger.debug(f"generate_sql_node: Raw OpenAI response: {raw_response_content_sql}")
        response_data_sql = json.loads(raw_response_content_sql)
        response = SQLGenerationOutput(**response_data_sql)
        
        # Clean the SQL query to remove any prepended JSON-like structures
        raw_sql_query = response.sql_query
        logger.debug(f"Raw SQL query from LLM: {raw_sql_query}")
        
        # Regex to find the actual SQL query, robustly handling optional prepended JSON objects.
        # It looks for common SQL keywords after any number of {...} blocks.
        # This regex assumes SQL queries start with standard keywords like SELECT, INSERT, UPDATE, DELETE, WITH, CREATE, ALTER, DROP.
        # It captures from the SQL keyword to the end of the string.
        match = re.search(r'^(?:\{.*?\})*?(SELECT\s.*|INSERT\s.*|UPDATE\s.*|DELETE\s.*|WITH\s.*|CREATE\s.*|ALTER\s.*|DROP\s.*)$', raw_sql_query.strip(), re.IGNORECASE | re.DOTALL)
        
        if match:
            cleaned_sql_query = match.group(1).strip() # Get the captured SQL part
            if not cleaned_sql_query.endswith(';'):
                cleaned_sql_query += ';'
            logger.info(f"Cleaned SQL query: {cleaned_sql_query}")
            state.sql_query = cleaned_sql_query
        else:
            # If regex doesn't match, log a warning and use the raw query, 
            # or handle as an error if it's critical that it's clean.
            logger.warning(f"Could not extract clean SQL from: {raw_sql_query}. Using raw query.")
            state.sql_query = raw_sql_query # Fallback to raw query
            # Ensure it ends with a semicolon if it looks like SQL
            if state.sql_query and isinstance(state.sql_query, str) and not state.sql_query.strip().endswith(';') and any(keyword in state.sql_query.upper() for keyword in ["SELECT", "INSERT", "UPDATE", "DELETE"]):
                 state.sql_query = state.sql_query.strip() + ';'

        state.sql_output_choice = response.sql_output_choice
        logger.info(f"Final state.sql_query: {state.sql_query}, Output choice: {state.sql_output_choice}")
        state.error_message = None # Clear previous errors
    except Exception as e:
        logger.error(f"Error generating SQL: {e}", exc_info=True)
        state.error_message = f"Error generating SQL: {e}"
        state.final_answer = f"ì£„ì†¡í•©ë‹ˆë‹¤, SQL ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
        state.sql_query = None
    logger.info("--- Exiting generate_sql_node ---")
    return state

async def execute_sql_node(state: AgentState, config: Optional[RunnableConfig] = None):
    logger.info("--- Entering execute_sql_node ---")
    if not state.sql_query:
        logger.error("No SQL query to execute.")
        state.error_message = "No SQL query to execute."
        state.final_answer = "ì‹¤í–‰í•  SQL ì¿¼ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤."
        # If there's no SQL query, we can't proceed to summarize or visualize.
        # We should indicate an error and perhaps end or route to a fallback.
        # For now, setting final_answer and error_message. The graph might need a dedicated error handling path.
        return state

    logger.info(f"Executing SQL: {state.sql_query}")
    
    # Determine the base directory for .env files
    # Assuming this script is in 'fastapi_server/agent/agent3.py'
    # Adjust if your structure is different
    current_script_dir = os.path.dirname(os.path.abspath(__file__))
    base_dir_analysis_env = os.path.join(current_script_dir, '..', '..', 'analysis_env') # Path to analysis_env folder
    base_dir_my_state_env = os.path.join(current_script_dir, '..', '..', 'my_state_env') # Path to my_state_env folder

    try:
        # Run the synchronous DB operation in a separate thread
        result_df = await asyncio.to_thread(
            _execute_sql_sync, 
            state.sql_query,
            base_dir_analysis_env,
            base_dir_my_state_env
        )
        
        if isinstance(result_df, pd.DataFrame):
            state.sql_result = result_df.to_string() # Or to_json, or keep as DataFrame if downstream can handle
            logger.info(f"SQL Result:\n{state.sql_result}")
        elif isinstance(result_df, dict) and 'sql_result' in result_df:
            # ê²°ê³¼ê°€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜ëœ ê²½ìš° (sql_result í‚¤ê°€ ìˆìœ¼ë©´ ì •ìƒì ì¸ ê²°ê³¼ë¡œ ê°„ì£¼)
            state.sql_result = result_df['sql_result']
            state.error_message = result_df.get('error_message', None)
            logger.info(f"SQL Result from dict: {state.sql_result}")
        else: # Error string from _execute_sql_sync
            state.sql_result = str(result_df) if result_df is not None else None
            state.error_message = f"Error executing SQL: {result_df}"
            state.final_answer = f"SQL ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {result_df}"
            logger.error(f"Error executing SQL (returned as string): {result_df}")
        
        # SQL ê²°ê³¼ê°€ ì¡´ì¬í•˜ë©´ ì˜¤ë¥˜ ë©”ì‹œì§€ëŠ” Noneìœ¼ë¡œ ì„¤ì •
        if state.sql_result and not state.error_message:
            state.error_message = None

    except Exception as e:
        logger.error(f"Exception executing SQL: {e}", exc_info=True)
        state.error_message = f"Exception executing SQL: {e}"
        state.sql_result = None
        state.final_answer = f"ì£„ì†¡í•©ë‹ˆë‹¤, SQL ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•˜ëŠ” ì¤‘ ì˜ˆì™¸ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    logger.info("--- Exiting execute_sql_node ---")
    return state

async def summarize_sql_result_node(state: AgentState, config: Optional[RunnableConfig] = None):
    logger.info("--- Entering summarize_sql_result_node ---")
    
    # ë””ë²„ê¹…ì„ ìœ„í•´ ìƒíƒœ ì¶œë ¥
    logger.info(f"State before summarize_sql_result_node: error_message={state.error_message}, sql_result type={type(state.sql_result)}, sql_query={state.sql_query}")
    
    # ê²°ê³¼ê°€ Noneì¸ ê²½ìš°
    if not state.sql_result:
        logger.warning("SQL result is None or empty.")
        if state.error_message:
            state.final_answer = f"SQL ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ê²°ê³¼ë¥¼ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {state.error_message}"
        else:
            state.final_answer = "ìš”ì•½í•  SQL ì‹¤í–‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        return state
    
    # SQL ê²°ê³¼ê°€ ë¬¸ìì—´ì¸ì§€ í™•ì¸í•˜ê³ , ë¬¸ìì—´ì´ ì•„ë‹ˆë©´ ë¬¸ìì—´ë¡œ ë³€í™˜
    if not isinstance(state.sql_result, str):
        logger.info(f"Converting SQL result from {type(state.sql_result)} to string")
        state.sql_result = str(state.sql_result)
    
    # SQL ê²°ê³¼ê°€ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ì€ì§€ í™•ì¸
    if len(state.sql_result.strip()) < 5:
        logger.warning(f"SQL result is suspiciously short: '{state.sql_result}'")
        state.final_answer = "SQL ì¿¼ë¦¬ ê²°ê³¼ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì²˜ë¦¬í•  ìˆ˜ ì—†ëŠ” í˜•ì‹ì…ë‹ˆë‹¤."
        return state
        
    # ë°ì´í„°í”„ë ˆì„ ì¶œë ¥ í˜•ì‹ì´ ì œëŒ€ë¡œ ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if "count" in state.sql_result and any(c.isdigit() for c in state.sql_result):
        logger.info("SQL result contains 'count' and numbers, which looks like a valid result")
    
    logger.info(f"SQL Result for summary (processed): {state.sql_result[:200]}...")
    logger.debug(f"Summarizing SQL result for user query: {state.user_query}")
    logger.debug(f"SQL Query for summary: {state.sql_query}")

    try:
        # ìƒì„¸ ë¡œê¹… ì¶”ê°€
        logger.info(f"Preparing to call LLM with SQL Query: {state.sql_query}")
        logger.info(f"SQL Result first 100 chars: {state.sql_result[:100]}")
        
        # ì‚¬ìš©ì ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì°¾ê¸°
        user_messages = [msg for msg in state.messages if isinstance(msg, HumanMessage)]
        last_user_message = user_messages[-1].content if user_messages else "SQL ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”."
        logger.info(f"Last user message: {last_user_message[:100]}")
        
        # ì²´ì¸ êµ¬ì„± ë° í˜¸ì¶œ
        # SQL ì¿¼ë¦¬ì™€ ê²°ê³¼ë¥¼ ëª…ì‹œì ìœ¼ë¡œ í¬í•¨í•˜ëŠ” ì‚¬ìš©ì ë©”ì‹œì§€ ìƒì„±
        explicit_sql_message = HumanMessage(
            content=f"SQL ì¿¼ë¦¬: {state.sql_query}\n\nSQL ê²°ê³¼:\n{state.sql_result}\n\nì´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."
        )
        
        # ê¸°ì¡´ ë©”ì‹œì§€ ë³µì‚¬ ë° SQL ë©”ì‹œì§€ ì¶”ê°€
        messages_with_sql = state.messages.copy()
        messages_with_sql.append(explicit_sql_message)
        
        # ì²´ì¸ êµ¬ì„± ë° í˜¸ì¶œ
        chain = sql_result_summary_chat_prompt | llm
        response = await chain.ainvoke(
            {
                "messages": messages_with_sql, 
                "sql_query": str(state.sql_query), 
                "sql_result": state.sql_result
            },
            config=config
        )
        
        # ì‘ë‹µ í™•ì¸ ë° ì²˜ë¦¬
        state.final_answer = response.content
        if not state.final_answer or len(state.final_answer.strip()) < 10:
            logger.warning(f"LLM returned empty or very short response: '{state.final_answer}'")
            state.final_answer = f"SQL ì¿¼ë¦¬ '{state.sql_query}'ì˜ ê²°ê³¼ëŠ” {state.sql_result}ì…ë‹ˆë‹¤."
            
        logger.info(f"Generated summary: {state.final_answer}")
        state.error_message = None # Clear previous errors
    except Exception as e:
        logger.error(f"Error summarizing SQL result: {e}", exc_info=True)
        state.error_message = f"Error summarizing SQL result: {e}"
        state.final_answer = f"ì£„ì†¡í•©ë‹ˆë‹¤, SQL ê²°ê³¼ë¥¼ ìš”ì•½í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    logger.info("--- Exiting summarize_sql_result_node ---")
    return state

async def general_question_node(state: AgentState, config: Optional[RunnableConfig] = None):
    logger.info("--- Entering general_question_node ---")
    if not state.messages:
        logger.error("general_question_node: No messages in state. Cannot generate answer.")
        state.error_message = "No input message found for general question."
        state.final_answer = "ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•œ ì…ë ¥ ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤."
        return state

    logger.debug(f"Answering general question from user query (from state): {state.user_query}")
    logger.debug(f"Full messages for prompt: {state.messages}")

    chain = general_chat_prompt | llm
    try:
        # Pass the full message history to the chain
        response = await chain.ainvoke({"messages": state.messages}, config=config)
        state.final_answer = response.content
        logger.info(f"Generated general answer: {state.final_answer}")
        state.error_message = None # Clear previous errors
    except Exception as e:
        logger.error(f"Error answering general question: {e}", exc_info=True)
        state.error_message = f"Error answering general question: {e}"
        state.final_answer = f"ì£„ì†¡í•©ë‹ˆë‹¤, ì¼ë°˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    logger.info("--- Exiting general_question_node ---")
    return state

async def category_predict_node(state: AgentState, config: Optional[RunnableConfig] = None) -> Dict[str, Any]:
    print("--- CATEGORY PREDICT NODE (Telecom Churn Prediction with Csv File Content) ---")

    # --- ê²½ë¡œ ì„¤ì • ---
    base_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
    MODEL_PATH = os.path.join(base_path, 'churn_predictor_pipeline.pkl')
    CATEGORICAL_COLS_PATH = os.path.join(base_path, 'categorical_cols.pkl')
    LABEL_ENCODERS_PATH = os.path.join(base_path, 'label_encoders.pkl')

    EXPECTED_FEATURE_ORDER = [
        'seniorcitizen', 'partner', 'dependents', 'tenure', 'phoneservice',
        'multiplelines', 'onlinesecurity', 'onlinebackup', 'techsupport',
        'contract', 'paperlessbilling', 'paymentmethod', 'monthlycharges', 'totalcharges',
        'new_totalservices', 'new_avg_charges', 'new_increase', 'new_avg_service_fee',
        'charge_increased', 'charge_growth_rate', 'is_auto_payment',
        'expected_contract_months', 'contract_gap'
    ]
    CUSTOMER_ID_COL = 'customerid'
    PREDICTION_THRESHOLD = 0.312

    csv_data_str: Optional[str] = None

    # 1. state.csv_file_content (LangGraph Studioì˜ 'Csv File Content' í•„ë“œ) í™•ì¸
    if state.csv_file_content:
        print("INFO: Using CSV data from state.csv_file_content.")
        csv_data_str = state.csv_file_content
    # 2. state.user_query (Chat ë˜ëŠ” Messages ì…ë ¥) í™•ì¸
    elif hasattr(state, 'user_query') and state.user_query:
        print(f"INFO: Attempting to use state.user_query for CSV data. Content (first 100 chars): '{state.user_query[:100]}...'")
        # 2a. state.user_queryë¥¼ íŒŒì¼ ê²½ë¡œë¡œ ì‹œë„
        if os.path.exists(state.user_query):
            try:
                print(f"INFO: state.user_query '{state.user_query}' is an existing path. Reading file.")
                def read_file_sync(path):
                    with open(path, 'r', encoding='utf-8') as f_sync:
                        return f_sync.read()
                csv_data_str = await asyncio.to_thread(read_file_sync, state.user_query)
                if not csv_data_str:
                    print(f"WARNING: File at '{state.user_query}' was empty.")
            except Exception as e:
                print(f"WARNING: Error reading file from state.user_query path '{state.user_query}': {e}. Will attempt to treat as raw content.")
        
        # 2b. state.user_queryë¥¼ íŒŒì¼ ê²½ë¡œë¡œ ì½ì§€ ëª»í–ˆê±°ë‚˜, ê²½ë¡œê°€ ì•„ë‹ˆì—ˆë‹¤ë©´ ì›ë³¸ CSV ë‚´ìš©ìœ¼ë¡œ ê°„ì£¼
        if csv_data_str is None: # íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ë˜ëŠ” ê²½ë¡œê°€ ì•„ë‹ˆì—ˆìŒ
            print("INFO: Treating state.user_query as raw CSV content.")
            csv_data_str = state.user_query # pd.read_csvê°€ ì´í›„ì— íŒŒì‹± ì‹œë„

    # CSV ë°ì´í„°ë¥¼ ì–´ë””ì—ì„œë„ ì°¾ì§€ ëª»í•œ ê²½ìš° ì˜¤ë¥˜ ë°˜í™˜
    if csv_data_str is None:
        error_message_parts = ["âŒ ì˜¤ë¥˜: CSV ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]
        checked_sources = ["'Csv File Content' í•„ë“œ"]
        if hasattr(state, 'user_query'):
            checked_sources.append("'User Query' / ì±„íŒ… ë©”ì‹œì§€ (íŒŒì¼ ê²½ë¡œ ë˜ëŠ” CSV ë‚´ìš© ì§ì ‘ ì…ë ¥)")
        error_message_parts.append(f"í™•ì¸í•œ ì…ë ¥ ì†ŒìŠ¤: {', '.join(checked_sources)}.")
        error_message_parts.append("Csv File Content í•„ë“œì— ì§ì ‘ CSV ë‚´ìš©ì„ ë¶™ì—¬ë„£ê±°ë‚˜, ì±„íŒ…ìœ¼ë¡œ CSV íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œ ë˜ëŠ” CSV ë‚´ìš© ìì²´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        final_answer = "\n".join(error_message_parts)
        current_messages = state.messages # Get current messages
        updated_messages = current_messages + [AIMessage(content=final_answer)] if current_messages else [AIMessage(content=final_answer)]
        return {"messages": updated_messages, "final_answer": final_answer, "error_message": final_answer}

    print(f"INFO: CSV data obtained. Length: {len(csv_data_str)}. Preview (first 200 chars): {csv_data_str[:200]}...")

    try:
        # --- ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ ê°ì²´ ë¹„ë™ê¸° ë¡œë“œ ---
        pipeline_final = await asyncio.to_thread(joblib.load, MODEL_PATH)
        CATEGORICAL_COLS = await asyncio.to_thread(joblib.load, CATEGORICAL_COLS_PATH)
        label_encoders = await asyncio.to_thread(joblib.load, LABEL_ENCODERS_PATH)

        # --- CSV ë¬¸ìì—´ â†’ DataFrame ë³€í™˜ ---
        if not csv_data_str: # ì´ì¤‘ í™•ì¸, csv_data_strì´ Noneì´ë‚˜ ë¹ˆ ë¬¸ìì—´ì´ë©´ ì—ëŸ¬ ë°œìƒ ë°©ì§€
            final_answer = "âŒ ì˜¤ë¥˜: ë‚´ë¶€ ë¡œì§ ì˜¤ë¥˜ - CSV ë°ì´í„° ë¬¸ìì—´ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
            current_messages = state.messages # Get current messages
            updated_messages = current_messages + [AIMessage(content=final_answer)] if current_messages else [AIMessage(content=final_answer)]
            return {"messages": updated_messages, "final_answer": final_answer, "error_message": final_answer}
        # --- BEGIN REVISED CSV DATA CLEANING LOGIC ---
        raw_lines = csv_data_str.strip().splitlines()
        cleaned_lines = []

        if not raw_lines:
            final_answer = "âŒ ì˜¤ë¥˜: CSV ë°ì´í„° ë¬¸ìì—´ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
            current_messages = state.messages
            updated_messages = current_messages + [AIMessage(content=final_answer)] if current_messages else [AIMessage(content=final_answer)]
            return {"messages": updated_messages, "final_answer": final_answer, "error_message": final_answer}

        MIN_COMMAS_THRESHOLD = 1  # ì‰¼í‘œê°€ ì´ ê°œìˆ˜ ì´ìƒì´ë©´ 'ê°•ë ¥í•œ' CSV ë¼ì¸ìœ¼ë¡œ ê°„ì£¼

        # 'ê°•ë ¥í•œ' CSV ë¼ì¸ë“¤ì˜ ì¸ë±ìŠ¤ë¥¼ ì°¾ìŒ
        strong_csv_indices = [i for i, line in enumerate(raw_lines) if line.count(',') >= MIN_COMMAS_THRESHOLD]

        if strong_csv_indices:
            # 'ê°•ë ¥í•œ' ë¼ì¸ë“¤ì´ ì¡´ì¬í•˜ë©´, ì´ë“¤ì˜ ë²”ìœ„ë¥¼ í•µì‹¬ CSV ë¸”ë¡ìœ¼ë¡œ ê°„ì£¼
            core_block_start_idx = strong_csv_indices[0]
            core_block_end_idx = strong_csv_indices[-1]

            if core_block_start_idx > 0:
                print(f"INFO: CSV ì‹œì‘ ì „ {core_block_start_idx}ê°œì˜ ë¼ì¸ì„ ì§ˆë¬¸ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì œê±°í•©ë‹ˆë‹¤. ì²«ë²ˆì§¸ ì œê±°ëœ ë¼ì¸: '{raw_lines[0][:100]}...'", flush=True)
            if core_block_end_idx < len(raw_lines) - 1:
                print(f"INFO: CSV ì¢…ë£Œ í›„ {len(raw_lines) - 1 - core_block_end_idx}ê°œì˜ ë¼ì¸ì„ ì§ˆë¬¸ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì œê±°í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ ì œê±°ëœ ë¼ì¸: '{raw_lines[-1][:100]}...'", flush=True)

            # í•µì‹¬ CSV ë¸”ë¡ ì¶”ì¶œ
            core_block_lines = raw_lines[core_block_start_idx : core_block_end_idx + 1]

            if not core_block_lines: # Should not happen if strong_csv_indices is not empty
                final_answer = "âŒ ì˜¤ë¥˜: CSV í•µì‹¬ ë¸”ë¡ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                current_messages = state.messages
                updated_messages = current_messages + [AIMessage(content=final_answer)] if current_messages else [AIMessage(content=final_answer)]
                return {"messages": updated_messages, "final_answer": final_answer, "error_message": final_answer}

            # í•µì‹¬ ë¸”ë¡ì˜ ì²« ì¤„ì„ í—¤ë”ë¡œ ì‚¬ìš©
            header_line = core_block_lines[0]
            cleaned_lines.append(header_line)
            commas_in_header = header_line.count(',') # í—¤ë”ëŠ” MIN_COMMAS_THRESHOLD ì´ìƒì¼ ê²ƒì„

            # í•µì‹¬ ë¸”ë¡ ë‚´ì˜ ë°ì´í„° ë¼ì¸ ì •ì œ
            for i in range(1, len(core_block_lines)):
                line = core_block_lines[i]
                if line.count(',') >= MIN_COMMAS_THRESHOLD: # 'ê°•ë ¥í•œ' ë°ì´í„° ë¼ì¸
                    cleaned_lines.append(line)
                elif commas_in_header >= MIN_COMMAS_THRESHOLD: # í—¤ë”ëŠ” 'ê°•í–ˆìœ¼ë‚˜', í˜„ì¬ ë¼ì¸ì€ 'ì•½í•¨' (0ê°œì˜ ì‰¼í‘œ)
                    if not line.strip(): # ì˜ë„ì ìœ¼ë¡œ ë¹„ì–´ìˆëŠ” ë¼ì¸ì´ë©´ ìœ ì§€
                        cleaned_lines.append(line)
                    else: # ë‚´ìš©ì´ ìˆëŠ” 0ì‰¼í‘œ ë¼ì¸ì€ ë¸”ë¡ ë‚´ ì§ˆë¬¸ìœ¼ë¡œ ì˜ì‹¬í•˜ì—¬ í•„í„°ë§
                        print(f"INFO: CSV ë¸”ë¡ ë‚´ì—ì„œ 0ê°œì˜ ì‰¼í‘œë¥¼ ê°€ì§„ ë¹„ì–´ìˆì§€ ì•Šì€ ë¼ì¸ì„ í•„í„°ë§í•©ë‹ˆë‹¤: '{line[:100]}...'", flush=True)
                else: # í—¤ë”ë„ 'ì•½í–ˆê³ ' (ì´ ê²½ìš°ëŠ” strong_csv_indices ë¡œì§ìƒ ê±°ì˜ ì—†ìŒ) í˜„ì¬ ë¼ì¸ë„ 'ì•½í•˜ë©´' ì¼ë‹¨ í¬í•¨
                    cleaned_lines.append(line)
        else:
            # 'ê°•ë ¥í•œ' CSV ë¼ì¸ì´ í•˜ë‚˜ë„ ì—†ìŒ (ëª¨ë“  ë¼ì¸ì˜ ì‰¼í‘œ < MIN_COMMAS_THRESHOLD, ì˜ˆ: ëª¨ë‘ 0ê°œ)
            # ì´ ê²½ìš° ë‹¨ì¼ ì—´ CSVì´ê±°ë‚˜ ì „ì²´ê°€ ì§ˆë¬¸ì¼ ìˆ˜ ìˆìŒ. ì¼ë‹¨ ëª¨ë“  ë¼ì¸ì„ ì‚¬ìš©.
            print(f"INFO: ëª¨ë“  ë¼ì¸ì˜ ì‰¼í‘œ ê°œìˆ˜ê°€ {MIN_COMMAS_THRESHOLD}ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤. ë‹¨ì¼ ì—´ CSVë¡œ ê°„ì£¼í•˜ê±°ë‚˜ ì „ì²´ê°€ ì§ˆë¬¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë“  ë¼ì¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.", flush=True)
            cleaned_lines = raw_lines

        if not cleaned_lines:
            final_answer = "âŒ ì˜¤ë¥˜: CSV ë°ì´í„° ì •ì œ í›„ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
            current_messages = state.messages
            updated_messages = current_messages + [AIMessage(content=final_answer)] if current_messages else [AIMessage(content=final_answer)]
            return {"messages": updated_messages, "final_answer": final_answer, "error_message": final_answer}
        
        cleaned_csv_data_str = "\n".join(cleaned_lines)
        
        print(f"INFO: ìµœì¢… ì •ì œëœ CSV ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²« 200ì): {cleaned_csv_data_str[:200]}...", flush=True)
        input_df = await asyncio.to_thread(pd.read_csv, io.StringIO(cleaned_csv_data_str))
        # --- END REVISED CSV DATA CLEANING LOGIC ---

        if CUSTOMER_ID_COL not in input_df.columns:
            final_answer = f"âŒ ì˜¤ë¥˜: '{CUSTOMER_ID_COL}' ì»¬ëŸ¼ì´ CSVì— ì—†ìŠµë‹ˆë‹¤."
            current_messages = state.messages # Get current messages
            updated_messages = current_messages + [AIMessage(content=final_answer)] if current_messages else [AIMessage(content=final_answer)]
            return {"messages": updated_messages, "final_answer": final_answer, "error_message": final_answer}

        customer_ids = input_df[CUSTOMER_ID_COL]
        X_predict = input_df.drop(columns=[CUSTOMER_ID_COL], errors='ignore')

        # --- ë²”ì£¼í˜• ì»¬ëŸ¼ ì¸ì½”ë”© ---
        for col in CATEGORICAL_COLS:
            if col in X_predict.columns:
                if col in label_encoders:
                    le = label_encoders[col]
                    X_predict[col] = X_predict[col].apply(
                        lambda x: le.transform([x])[0] if x in le.classes_ else -1
                    )
                else:
                    print(f"WARNING: Label encoder for column '{col}' not found. Skipping encoding.")
            else:
                print(f"WARNING: Categorical column '{col}' not found in input CSV. Skipping.")

        # --- ëˆ„ë½ëœ ì»¬ëŸ¼ ì²˜ë¦¬ (ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ëª¨ë“  ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸) ---
        missing_cols = set(EXPECTED_FEATURE_ORDER) - set(X_predict.columns)
        for col in missing_cols:
            print(f"INFO: Adding missing column '{col}' with default value 0.")
            X_predict[col] = 0 # ë˜ëŠ” np.nan ë“± ì ì ˆí•œ ê¸°ë³¸ê°’

        # --- ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬ ---
        X_predict = X_predict[EXPECTED_FEATURE_ORDER]

        # --- ì˜ˆì¸¡ ìˆ˜í–‰ ---
        predictions_proba = await asyncio.to_thread(pipeline_final.predict_proba, X_predict)
        predictions = (predictions_proba[:, 1] >= PREDICTION_THRESHOLD).astype(int)

        # --- ê²°ê³¼ ìƒì„± ---
        results_df = pd.DataFrame({
            CUSTOMER_ID_COL: customer_ids,
            'Churn Probability': predictions_proba[:, 1],
            'Churn Prediction (Threshold 0.312)': predictions
        })
        results_df['Churn Prediction (Threshold 0.312)'] = results_df['Churn Prediction (Threshold 0.312)'].map({1: 'Yes', 0: 'No'})

        final_answer = "ğŸ“Š ê³ ê° ì´íƒˆ ì˜ˆì¸¡ ê²°ê³¼:\n" + results_df.to_string(index=False)
        print(f"Prediction successful. Result preview: {final_answer[:200]}...")
        current_messages = state.messages # Get current messages
        updated_messages = current_messages + [AIMessage(content=final_answer)] if current_messages else [AIMessage(content=final_answer)]
        return {"messages": updated_messages, "final_answer": final_answer, "error_message": None}

    except pd.errors.EmptyDataError:
        error_msg = "âŒ ì˜¤ë¥˜: ì…ë ¥ëœ CSV ë°ì´í„°ê°€ ë¹„ì–´ ìˆê±°ë‚˜ ì˜ëª»ëœ í˜•ì‹ì…ë‹ˆë‹¤. CSV ë‚´ìš©ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”."
        print(f"ERROR: {error_msg}")
        current_messages = state.messages # Get current messages
        updated_messages = current_messages + [AIMessage(content=error_msg)] if current_messages else [AIMessage(content=error_msg)]
        return {"messages": updated_messages, "final_answer": error_msg, "error_message": error_msg}
    except FileNotFoundError as e:
        error_msg = f"âŒ ì˜¤ë¥˜: ëª¨ë¸ ë˜ëŠ” ì „ì²˜ë¦¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. ({e})"
        print(f"ERROR: {error_msg}")
        current_messages = state.messages # Get current messages
        updated_messages = current_messages + [AIMessage(content=error_msg)] if current_messages else [AIMessage(content=error_msg)]
        return {"messages": updated_messages, "final_answer": error_msg, "error_message": error_msg}
    except KeyError as e:
        error_msg = f"âŒ ì˜¤ë¥˜: CSV ë°ì´í„°ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆê±°ë‚˜, ëª¨ë¸ í•™ìŠµ ì‹œ ì‚¬ìš©ëœ ì»¬ëŸ¼ê³¼ ë‹¤ë¦…ë‹ˆë‹¤. (ì˜¤ë¥˜ ì»¬ëŸ¼: {e}) CSV íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
        print(f"ERROR: {error_msg}")
        current_messages = state.messages # Get current messages
        updated_messages = current_messages + [AIMessage(content=error_msg)] if current_messages else [AIMessage(content=error_msg)]
        return {"messages": updated_messages, "final_answer": error_msg, "error_message": error_msg}
    except ValueError as e: # Often from pandas if query is malformed for read_sql_query
        error_msg = f"âŒ ì˜¤ë¥˜: ë°ì´í„° ë³€í™˜ ì¤‘ ê°’ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. CSV ë°ì´í„° íƒ€ì…ì„ í™•ì¸í•´ì£¼ì„¸ìš”. (ì˜¤ë¥˜: {e})"
        print(f"ERROR: {error_msg}")
        current_messages = state.messages # Get current messages
        updated_messages = current_messages + [AIMessage(content=error_msg)] if current_messages else [AIMessage(content=error_msg)]
        return {"messages": updated_messages, "final_answer": error_msg, "error_message": error_msg}
    except Exception as e:
        error_msg = f"âŒ ì˜ˆì¸¡ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}"
        print(f"ERROR: {error_msg}")
        current_messages = state.messages # Get current messages
        updated_messages = current_messages + [AIMessage(content=error_msg)] if current_messages else [AIMessage(content=error_msg)]
        return {"messages": updated_messages, "final_answer": error_msg, "error_message": error_msg}

async def create_visualization_node(state: AgentState, config: Optional[RunnableConfig] = None):
    logger.info("--- Entered create_visualization_node (placeholder) ---")
    # In a real implementation, this node would generate a visualization
    # based on state.sql_result or other relevant data.
    # For now, it just passes through or sets a placeholder message.
    if state.sql_result is not None:
        state.visualization_output = f"Placeholder: Visualization for query result: {str(state.sql_result)[:200]}..."
        state.final_answer = state.visualization_output # Or a message indicating visualization is ready
    else:
        state.error_message = "No SQL result available to visualize."
        state.final_answer = "ì‹œê°í™”í•  SQL ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    logger.info(f"create_visualization_node state after processing: {state.visualization_output=}, {state.final_answer=}")
    return state

def route_sql_output(state: AgentState) -> Literal["create_visualization_node", "summarize_sql_result_node"]:
    choice = state.sql_output_choice
    # If execute_sql_node itself resulted in an error (indicated by error_message and no sql_result)
    # both visualization and summarization nodes have internal logic to handle this.
    # The choice made by the supervisor should still be respected if possible.
    if state.error_message and not state.sql_result:
        print(f"Error detected before routing SQL output: {state.error_message}. Proceeding with choice: {choice}")

    if choice == "visualize":
        print("Routing to create_visualization_node based on sql_output_choice.")
        return "create_visualization_node"
    elif choice == "summarize":
        print("Routing to summarize_sql_result_node based on sql_output_choice.")
        return "summarize_sql_result_node"
    else:
        # Fallback if sql_output_choice is somehow not set for a db_query path
        print(f"Warning: sql_output_choice is '{choice}'. Defaulting to summarize_sql_result_node.")
        return "summarize_sql_result_node"

def route_query(state: AgentState) -> Literal["generate_sql_node", "category_predict_node", "general_question_node"]:
    query_type = state.query_type
    print(f"Routing based on query_type: {query_type}")
    if query_type == "db_query":
        return "generate_sql_node"
    elif query_type == "category_predict_query":
        return "category_predict_node"
    elif query_type == "general_query":
        return "general_question_node"
    else:
        # This case should ideally not be reached if supervisor is strict
        print(f"Warning: Unknown query_type '{query_type}', defaulting to general_question_node.")
        return "general_question_node"

# --- Graph Definition ---
workflow = StateGraph(AgentState)

# Add nodes
workflow.add_node("supervisor", supervisor_node)
workflow.add_node("generate_sql_node", generate_sql_node)
workflow.add_node("execute_sql_node", execute_sql_node)
workflow.add_node("create_visualization_node", create_visualization_node)
workflow.add_node("summarize_sql_result_node", summarize_sql_result_node)
workflow.add_node("general_question_node", general_question_node)
workflow.add_node("category_predict_node", category_predict_node)

# Set entry point
workflow.set_entry_point("supervisor")

# Conditional edges from supervisor
workflow.add_conditional_edges(
    "supervisor",
    route_query,
    {
        "generate_sql_node": "generate_sql_node",
        "category_predict_node": "category_predict_node",
        "general_question_node": "general_question_node"
    }
)

# Edges for DB query flow (now common for 3 branches)
workflow.add_edge("generate_sql_node", "execute_sql_node")
# After execute_sql_node, route to either visualization or summarization
workflow.add_conditional_edges(
    "execute_sql_node",
    route_sql_output,
    {
        "create_visualization_node": "create_visualization_node",
        "summarize_sql_result_node": "summarize_sql_result_node"
    }
)
workflow.add_edge("create_visualization_node", END)
workflow.add_edge("summarize_sql_result_node", END)

# Edges from placeholder nodes to the SQL generation flow
workflow.add_edge("category_predict_node", END)

# Edge for general question
workflow.add_edge("general_question_node", END)

# Compile the graph
app = workflow.compile()
graph = app # For langgraph dev compatibility

# To make it runnable with langgraph dev, ensure it's assigned to 'graph'
# Example of how to run (for testing locally):
# async def main():
#     inputs = {"user_query": "ì§€ë‚œ ë‹¬ ì‚¬ìš©ì ìˆ˜ëŠ” ëª‡ ëª…ì¸ê°€ìš”?"}
#     # For testing, you can invoke the graph like this:
#     # inputs = {"messages": [HumanMessage(content="ìš°ë¦¬ íšŒì‚¬ ì§ì›ë“¤ ì¤‘ ê°€ì¥ ì—°ë´‰ì´ ë†’ì€ ìƒìœ„ 3ëª…ì€ ëˆ„êµ¬ì¸ê°€ìš”?")]}
#     # async for event in app.astream_events(inputs, version="v1"):
#     #     kind = event["event"]
#     #     if kind == "on_chat_model_stream":
#     #         content = event["data"]["chunk"].content
#     #         if content:
#     #             print(content, end="")
#     #     elif kind == "on_tool_end":
#     #         print(f"\nTool Output: {event['data']['output']}")
#     #     # print(f"\n--- Event: {kind} ---")
#     #     # print(event["data"])
# if __name__ == "__main__":
#     import asyncio
#     async def main_test():
#         app_test = await main()
#         inputs = {"input": "ìš°ë¦¬ íšŒì‚¬ í…Œì´ë¸” ëª©ë¡ ì¢€ ë³´ì—¬ì¤˜"}
#     # inputs = {"input": "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?"}
#     async for event in app_test.astream_events(inputs, version="v1"):
#             kind = event["event"]
#             if kind == "on_chat_model_stream":
#                 content = event["data"]["chunk"].content
#                 if content:
#                     print(content, end="")
#                 print(f"\nTool Output: {event['data']['output']}")
#             elif kind == "on_chain_end" and event["name"] == "AgentGraph": # Check for the graph's end
#                 print("\n--- Final State ---")
#                 print(event["data"].get("output")) # Access final output from the event

#     asyncio.run(main_test())

def _execute_sql_sync(sql_query: str, base_dir_analysis_env: str, base_dir_my_state_env: str) -> Dict[str, Any]:
    # Determine .env path and load environment variables
    dotenv_path_analysis = os.path.join(base_dir_analysis_env, '.env')
    dotenv_path_my_state = os.path.join(base_dir_my_state_env, '.env')
    specific_env_path = None

    if os.path.exists(dotenv_path_analysis):
        specific_env_path = dotenv_path_analysis
    elif os.path.exists(dotenv_path_my_state):
        specific_env_path = dotenv_path_my_state

    if specific_env_path:
        print(f"_execute_sql_sync: Loading .env from: {specific_env_path}")
        load_dotenv(dotenv_path=specific_env_path, override=True)
    else:
        print("_execute_sql_sync: No specific .env file found. Relying on system environment variables or a global .env.")
        load_dotenv(override=True) # Load global .env or system vars

    db_host = os.getenv("DB_HOST")
    db_port = os.getenv("DB_PORT")
    db_name = os.getenv("DB_NAME")
    db_user = os.getenv("DB_USER")
    db_password = os.getenv("DB_PASSWORD")

    if not all([db_host, db_port, db_name, db_user, db_password]):
        error_msg = "_execute_sql_sync: Database connection details missing in environment variables."
        print(error_msg)
        return {"error_message": error_msg, "sql_result": ""}

    conn_string = f"host='{db_host}' port='{db_port}' dbname='{db_name}' user='{db_user}' password='{db_password}'"
    conn = None
    try:
        print(f"_execute_sql_sync: Connecting to DB with: {conn_string.replace(db_password, '****') if db_password else conn_string}")
        conn = psycopg2.connect(conn_string)
        print(f"_execute_sql_sync: Executing SQL: {sql_query}")
        df = pd.read_sql_query(sql_query, conn)
        sql_result_str = df.to_string()
        print(f"_execute_sql_sync: SQL Result (first 200 chars): {sql_result_str[:200]}")
        return {"sql_result": sql_result_str, "error_message": None}
    except (psycopg2.Error, pd.io.sql.DatabaseError) as e:
        error_msg = f"_execute_sql_sync: Database error: {e}"
        print(error_msg)
        return {"error_message": error_msg, "sql_result": ""}
    except ValueError as e: # Often from pandas if query is malformed for read_sql_query
        error_msg = f"_execute_sql_sync: SQL query validation error for pandas: {e}"
        print(error_msg)
        return {"error_message": error_msg, "sql_result": ""}
    except Exception as e:
        error_msg = f"_execute_sql_sync: An unexpected error occurred during SQL execution: {e}"
        print(error_msg)
        return {"error_message": error_msg, "sql_result": ""}
    finally:
        if conn:
            conn.close()
            print("_execute_sql_sync: DB ì—°ê²° ì¢…ë£Œ.")
