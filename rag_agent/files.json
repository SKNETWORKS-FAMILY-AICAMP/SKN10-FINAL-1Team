{
  "rag_agent/src/agent/graph.py": "
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, TypedDict

from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

# â”€â”€ 1. Configurable ê°’ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class Configuration(TypedDict):
    pinecone_index_name: str
    top_k: Optional[int]
    openai_model: Optional[str]

# â”€â”€ 2. State ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@dataclass
class State:
    question: str
    query_vector: List[float] | None = None
    matches: list | None = None
    context: str = \"\"
    answer: str = \"\"

# â”€â”€ 3. ì‹¤ì œ ì‘ì—… ë¡œì§ ê°€ì ¸ì˜¤ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from src.agent.tasks import (
    embed_query,
    retrieve_best_namespace,
    build_context,
    generate_answer,
)

# â”€â”€ 4. ë…¸ë“œ í•¨ìˆ˜(ë™ê¸°) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def node_embed_query(state: State, cfg: RunnableConfig) -> Dict[str, Any]:
    return {\"query_vector\": embed_query(state.question)}

def node_retrieve_best_ns(state: State, cfg: RunnableConfig) -> Dict[str, Any]:
    qvec  = state.query_vector
    top_k = cfg[\"configurable\"].get(\"top_k\", 5)
    _, ms = retrieve_best_namespace(qvec, top_k)
    return {\"matches\": ms}

def node_build_context(state: State, cfg: RunnableConfig) -> Dict[str, Any]:
    return {\"context\": build_context(state.matches)}

def node_generate_answer(state: State, cfg: RunnableConfig) -> Dict[str, Any]:
    return {\"answer\": generate_answer(state.question, state.context)}

# â”€â”€ 5. ê·¸ë˜í”„ êµ¬ì„±(0.4.x ê·œê²©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
g = StateGraph(State, config_schema=Configuration)

# ë…¸ë“œ ë“±ë¡: \"ID\" â†’ í•¨ìˆ˜(ë™ê¸°)
g = g.add_node(\"EmbedQuery\",      node_embed_query)
g = g.add_node(\"RetrieveBestNS\",  node_retrieve_best_ns)
g = g.add_node(\"BuildContext\",    node_build_context)
g = g.add_node(\"GenerateAnswer\",  node_generate_answer)

# ì—£ì§€ ì—°ê²°
g = g.add_edge(\"EmbedQuery\",     \"RetrieveBestNS\")
g = g.add_edge(\"RetrieveBestNS\", \"BuildContext\")
g = g.add_edge(\"BuildContext\",   \"GenerateAnswer\")

# entry / finish ì§€ì •
g = g.set_entry_point(\"EmbedQuery\")
g = g.set_finish_point(\"GenerateAnswer\")

graph = g.compile(name=\"RAG_Workflow\")
",

  "rag_agent/src/agent/tasks.py": "
import os
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI
from pinecone import Pinecone

# â”€â”€â”€ .env ì½ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ROOT = Path(__file__).resolve().parents[3]   # SKN10-FINAL_1TEAM
load_dotenv(ROOT / \".env\")

OPENAI_API_KEY   = os.getenv(\"OPENAI_API_KEY\")
PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")
PINECONE_ENV     = os.getenv(\"PINECONE_ENV\")

INDEX_NAME = \"dense-index\"

openai_client = OpenAI(api_key=OPENAI_API_KEY)
pc            = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)
index         = pc.Index(INDEX_NAME)

# â”€â”€â”€ RAG helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def embed_query(text: str) -> list:
    emb = openai_client.embeddings.create(
        model=\"text-embedding-3-large\", input=text
    )
    return emb.data[0].embedding

def retrieve_best_namespace(vec: list, top_k: int = 5) -> tuple:
    stats = index.describe_index_stats()
    best_ns, best_score, best_matches = None, -1.0, None
    for ns, info in stats.namespaces.items():
        if info[\"vector_count\"] == 0:
            continue
        res = index.query(vector=vec, namespace=ns, top_k=top_k, include_metadata=True)
        if not res.matches:
            continue
        score = res.matches[0].score
        if score > best_score:
            best_ns, best_score, best_matches = ns, score, res.matches
    if best_ns is None:
        raise ValueError(\"ë§¤ì¹­ë˜ëŠ” ë„¤ì„ìŠ¤í˜ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\")
    return best_ns, best_matches

def build_context(matches: list) -> str:
    return \"\\n---\\n\".join(m.metadata.get(\"text\", \"\") for m in matches if m.metadata.get(\"text\"))

def generate_answer(question: str, context: str) -> str:
    prompt = f\"\"\"ì•„ë˜ Contextë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\\n\\nContext:\\n{context}\\n\\nì§ˆë¬¸:\\n{question}\\n\"\"\"
    resp = openai_client.chat.completions.create(
        model=\"gpt-3.5-turbo\",
        messages=[
            {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\"},
            {\"role\": \"user\",   \"content\": prompt}
        ],
        temperature=0.0,
        max_tokens=512
    )
    return resp.choices[0].message.content.strip()
",

  "rag_agent/app.py": "
from pathlib import Path
from dotenv import load_dotenv

ROOT = Path(__file__).resolve().parents[2]   # SKN10-FINAL_1TEAM
load_dotenv(ROOT / \".env\")

from src.agent.graph import graph  # ì»´íŒŒì¼ëœ StateGraph

if __name__ == \"__main__\":
    while True:
        q = input(\"ğŸ” ì§ˆë¬¸ (â€˜exitâ€™ ì…ë ¥ ì‹œ ì¢…ë£Œ): \").strip()
        if q.lower() in (\"exit\", \"quit\"): break
        if not q: continue

        try:
            result = graph.invoke(
                {\"question\": q},
                config={\"configurable\": {\"top_k\": 5}}
            )
            print(\"ğŸ† ë‹µë³€:\\n\" + result[\"answer\"] + \"\\n\")
        except Exception as e:
            print(\"âš ï¸  ì˜¤ë¥˜:\", e, \"\\n\")
"
}
