------------- app.py -------------\nimport os\nfrom pathlib import Path\nfrom dotenv import load_dotenv\n\nHERE = Path(__file__).resolve().parent           # SKN10-FINAL_1TEAM/rag_agent\nPROJECT_ROOT = HERE.parent                       # SKN10-FINAL_1TEAM\nload_dotenv(PROJECT_ROOT / \".env\")\n\nfrom src.agent.graph import graph                # ì»´íŒŒì¼ëœ StateGraph\n\nEXECUTOR = graph                                 # ê°€ë…ì„±ì„ ìœ„í•´ ë³„ì¹­\n\nif __name__ == \"__main__\":\n    while True:\n        q = input(\"ğŸ” ì§ˆë¬¸ (â€˜exitâ€™ ì…ë ¥ ì‹œ ì¢…ë£Œ): \").strip()\n        if q.lower() in (\"exit\", \"quit\"): break\n        if not q: continue\n\n        try:\n            result = EXECUTOR.invoke(             # 0.4.x â†’ invoke()\n                {\"question\": q},\n                config={\n                    \"configurable\": {\n                        \"top_k\": 5,\n                        # \"pinecone_index_name\": \"dense-index\",\n                        # \"openai_model\": \"gpt-3.5-turbo\",\n                    }\n                },\n            )\n            print(\"ğŸ† ë‹µë³€:\\n\" + result[\"answer\"] + \"\\n\")\n        except Exception as e:\n            print(\"âš ï¸  ì˜¤ë¥˜:\", e, \"\\n\")\n
# rag_agent/src/agent/app.py

import os
from pathlib import Path
from dotenv import load_dotenv

# â”€â”€â”€ ìµœìƒìœ„ .env íŒŒì¼ì„ í•œë²ˆì— ì½ì–´ë“¤ì…ë‹ˆë‹¤ â”€â”€â”€
# í˜„ì¬ íŒŒì¼(app.py)ì˜ ìœ„ì¹˜: SKN10â€FINAL_1TEAM/rag_agent/app.py
HERE         = Path(__file__).resolve().parent       # â†’ SKN10â€FINAL_1TEAM/rag_agent
PROJECT_ROOT = HERE.parent                            # â†’ SKN10â€FINAL_1TEAM
DOTENV_PATH  = PROJECT_ROOT / ".env"                  # â†’ SKN10â€FINAL_1TEAM/.env
load_dotenv(dotenv_path=DOTENV_PATH)

# â”€â”€â”€ ì´ì œ graph (â†’ tasks) ëª¨ë“ˆì„ import í•©ë‹ˆë‹¤ â”€â”€â”€
from src.agent.graph import graph

def main():
    # (load_dotenvë¥¼ ë‹¤ì‹œ í˜¸ì¶œí•´ë„ ì´ë¯¸ í•œ ë²ˆ ì½ì—ˆìœ¼ë¯€ë¡œ ê²°ê³¼ëŠ” ë™ì¼í•©ë‹ˆë‹¤.)
    load_dotenv(dotenv_path=DOTENV_PATH)

    executor = graph

    while True:
        question = input("ğŸ” ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (â€˜exitâ€™ ì…ë ¥ ì‹œ ì¢…ë£Œ): ").strip()
        if question.lower() in ("exit", "quit"):
            print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        if not question:
            continue

        try:
            # StateGraph.invoke()ë¥¼ ì“¸ ìˆ˜ë„ ìˆì§€ë§Œ, v0.4.7ì—ì„œëŠ” run()ìœ¼ë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤.
            result = executor.run(
                inputs={"question": question},
                config={"configurable": {
                    # í•„ìš”ì— ë”°ë¼ Configurable íŒŒë¼ë¯¸í„°ë¥¼ ì—¬ê¸°ì— ì¶”ê°€í•˜ì„¸ìš”.
                    # ì˜ˆì‹œ:
                    # "pinecone_index_name": "dense-index",
                    # "top_k": 5,
                    # "openai_model": "gpt-3.5-turbo",
                }}
            )
            # ë…¸ë“œ IDê°€ "GenerateAnswer"ì´ë¯€ë¡œ, ê·¸ ë…¸ë“œì˜ ë°˜í™˜ê°’ì—ì„œ "answer"ë¥¼ êº¼ëƒ…ë‹ˆë‹¤.
            final_answer = result["GenerateAnswer"]["answer"]
            print("\nğŸ† ìµœì¢… ë‹µë³€:\n" + final_answer + "\n")

        except Exception as e:
            print(f"âš ï¸ ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            continue

if __name__ == "__main__":
    main()
