from __future__ import annotations

import os
from dataclasses import dataclass
from typing import TypedDict, Dict # Dict ì¶”ê°€

from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langgraph.graph import END, START, StateGraph
from dotenv import load_dotenv
from openai import OpenAI
from pinecone import Pinecone, ServerlessSpec
from langchain.chat_models import ChatOpenAI
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
load_dotenv()

def init_clients():
    # 1-1) OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        raise ValueError("âš ï¸ í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    # OpenAI ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤.
    openai_client = OpenAI(api_key=openai_api_key)
    print("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì™„ë£Œ")

    # 1-2) Pinecone ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    pinecone_api_key = os.getenv("PINECONE_API_KEY")
    pinecone_env     = os.getenv("PINECONE_ENVIRONMENT")   # ì˜ˆ: "us-east1-gcp" ë˜ëŠ” "us-west1-gcp" ë“±
    if not pinecone_api_key or not pinecone_env:
        raise ValueError("âš ï¸ í™˜ê²½ë³€ìˆ˜ PINECONE_API_KEY ë˜ëŠ” PINECONE_ENVê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")

    pc = Pinecone(api_key=pinecone_api_key, environment=pinecone_env)
    print("âœ… Pinecone í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì™„ë£Œ")

    # 1-3) ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    index_name = "dense-index"  # ì‹¤ì œ ì‚¬ìš© ì¤‘ì¸ ì¸ë±ìŠ¤ ì´ë¦„ìœ¼ë¡œ êµì²´í•˜ì„¸ìš”
    existing_indexes = pc.list_indexes().names()
    if index_name not in existing_indexes:
        raise ValueError(f"âš ï¸ ì¸ë±ìŠ¤ '{index_name}'ê°€ Pineconeì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í˜„ì¬ ì¸ë±ìŠ¤ ëª©ë¡: {existing_indexes}")

    # 1-4) í•´ë‹¹ ì¸ë±ìŠ¤ ê°ì²´ ê°€ì ¸ì˜¤ê¸°
    index = pc.Index(index_name)
    print(f"âœ… Pinecone ì¸ë±ìŠ¤ '{index_name}' ì—°ê²° ì™„ë£Œ (Namespaces: {len(index.describe_index_stats().namespaces)})")

    return openai_client, index


# --------------------------------------------------
# 2) ì§ˆë¬¸ ë¬¸ì¥ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
# --------------------------------------------------
def embed_query(openai_client: OpenAI, text: str) -> list:
    """
    ìµœì‹  OpenAI í´ë¼ì´ì–¸íŠ¸ì—ì„œëŠ” resp.data[0].embedding ìœ¼ë¡œ ë²¡í„°ì— ì ‘ê·¼í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    resp = openai_client.embeddings.create(
        model="text-embedding-3-large",
        input=text
    )
    return resp.data[0].embedding


# --------------------------------------------------
# 3) ì—¬ëŸ¬ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì¤‘ â€œê°€ì¥ ë†’ì€ ìœ ì‚¬ë„â€ë¥¼ ì¤€ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì™€ ë§¤ì¹­ ê²°ê³¼ ë°˜í™˜
# --------------------------------------------------
def retrieve_best_namespace(index, query_vector: list, top_k: int = 5):
    """
    1) index.describe_index_stats()ë¥¼ í†µí•´ ëª¨ë“  ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ëª©ë¡ì„ ì–»ëŠ”ë‹¤.
    2) ê° ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ë¡œ query_vectorë¥¼ index.query()ë¡œ ê²€ìƒ‰í•˜ê³ ,
       matches[0].score ë¥¼ ë¹„êµí•´ì„œ â€œìµœê³  ìœ ì‚¬ë„â€ë¥¼ ì°¾ëŠ”ë‹¤.
    3) ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ë¥¼ ì¤€ ë„¤ì„ìŠ¤í˜ì´ìŠ¤(best_ns)ì™€ í•´ë‹¹ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ ì „ì²´ ë§¤ì¹­ ê²°ê³¼(best_matches)ë¥¼ ë°˜í™˜.
    """
    stats = index.describe_index_stats()
    available_namespaces = list(stats.namespaces.keys())
    if not available_namespaces:
        raise ValueError("âš ï¸ ì¸ë±ìŠ¤ì— ë„¤ì„ìŠ¤í˜ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

    best_ns = None
    best_score = -1.0
    best_matches = None

    for ns in available_namespaces:
        count = stats.namespaces[ns]["vector_count"]
        if count == 0:
            # ë¹„ì–´ ìˆëŠ” ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê±´ë„ˆë›°ê¸°
            continue

        res = index.query(
            vector=query_vector,
            namespace=ns,
            top_k=top_k,
            include_metadata=True
        )
        if not res.matches:
            continue

        top_score = res.matches[0].score
        if top_score > best_score:
            best_score = top_score
            best_ns = ns
            best_matches = res.matches

    if best_ns is None:
        raise ValueError("âš ï¸ ì–´ë–¤ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì—ì„œë„ ë§¤ì¹­ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    print(f"ğŸ” ì„ íƒëœ ë„¤ì„ìŠ¤í˜ì´ìŠ¤: '{best_ns}' (ìµœê³  ìœ ì‚¬ë„: {best_score:.4f})")
    return best_ns, best_matches


# --------------------------------------------------
# 4) ê²€ìƒ‰ëœ ë§¤ì¹­ ê²°ê³¼ì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸(ë©”íƒ€ë°ì´í„°)ë¥¼ êº¼ë‚´ Context ë¡œ ê²°í•©
# --------------------------------------------------
def build_context_from_matches(matches):
    """
    res.matches ë¦¬ìŠ¤íŠ¸ ì•ˆì˜ ê° item.metadata ì— ë“¤ì–´ ìˆëŠ” í…ìŠ¤íŠ¸ í•„ë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ì—…ë¡œë“œ ì‹œ metadata í‚¤ê°€ "text"ì˜€ë‹¤ê³  ê°€ì •í–ˆìŠµë‹ˆë‹¤.
    """
    contexts = []
    for m in matches:
        chunk_text = m.metadata.get("text", "")
        if chunk_text:
            contexts.append(chunk_text)

    return "\n---\n".join(contexts)


# --------------------------------------------------
# 5) LLM ChatCompletion í˜¸ì¶œí•˜ì—¬ ë‹µë³€ ìƒì„±
# --------------------------------------------------
def generate_answer_with_context(openai_client: OpenAI, question: str, context: str) -> str:
    """
    ìµœì‹  OpenAI í´ë¼ì´ì–¸íŠ¸ì—ì„œëŠ” client.chat.completions.create(...) í˜•íƒœë¥¼ ì”ë‹ˆë‹¤.
    """
    prompt = f"""ì•„ë˜ Contextë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

    Context:
    {context}

    ì§ˆë¬¸:
    {question}
    """
    resp = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.0,
        max_tokens=1024
    )
    # resp.choices[0].message.content ìœ¼ë¡œ ë‹µë³€ ì¶”ì¶œ
    return resp.choices[0].message.content.strip()



@dataclass
class State:
    # ì´ˆê¸°ê°’ì„ ì„¤ì •í•´ ì£¼ì§€ ì•Šìœ¼ë©´ ì¸ìê°€ requiredê°€ ëœë‹¤.
    user_input : str # ì‚¬ìš©ì ì…ë ¥
    document_type: str = ""   # ë¬¸ì„œ í˜•íƒœ
    result: str = ""  # ì±—ë´‡ ê²°ê³¼


def choose_document_type(message):
    prompt = PromptTemplate(
        input_variables=["input"],
        template="The user's question is: '{input}'. Analyze the question and categorize it into one of these document types: product, hr_policy, technical_document, proceedings. " \
        "Respond with EXACTLY ONE of these four values: 'product_document' for Product-related questions, 'proceedings' for Meeting Proceedings, 'internal_policy' for HR Policy documents, or 'technical_document' for Technical Documentation. " \
        "You must ONLY respond with one of these four exact values, without any additional text or explanation."
    )

    # LLM ì„¤ì • 
    llm = ChatOpenAI(model="gpt-3.5-turbo")

    # prompt + llm
    chain = prompt | llm

    # 4. ì‹¤í–‰
    response = chain.invoke({"input": message})
    print(response.content)
    return response

def choose_node(state: State) -> str:
    user_input = state.user_input
    document_type = choose_document_type(user_input).content.strip().lower()
    keywords = ["internal_policy", "product_document", "technical_document", "proceedings"]

    if document_type not in keywords : 
        state.document_type = "unknown"
    else : 
        state.document_type = document_type

    return {
        "document_type": state.document_type,
        "result": None
    }

def choose_one(state: State) -> str:
    choice = state.document_type
    if choice in ["internal_policy", "product_document", "technical_document", "proceedings"]:
        return choice
    else:
        return "unknown"

def execute_rag(state: State) -> str :
    print(f"\nğŸ“„ RAG ë…¸ë“œ ì‹¤í–‰: ë¬¸ì„œ íƒ€ì… = '{state.document_type}', ì§ˆë¬¸ = '{state.user_input}'")
    openai_client, pinecone_index = init_clients()
    print("   - í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

    query_vector = embed_query(openai_client, state.user_input)
    print(f"   - ì§ˆë¬¸ ì„ë² ë”© ì™„ë£Œ (ë²¡í„° í¬ê¸°: {len(query_vector)})")

    namespace_to_search = state.document_type
    if not namespace_to_search or namespace_to_search == "unknown":
        message = f"ë¬¸ì„œ íƒ€ì…ì´ '{namespace_to_search}'(ìœ¼)ë¡œ ë¶„ë¥˜ë˜ì–´ Pinecone ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
        print(f"   - ì •ë³´: {message}")
        # 'unknown'ì¼ ê²½ìš°, unknown_handler_nodeì—ì„œ ì´ë¯¸ ë©”ì‹œì§€ë¥¼ ì„¤ì •í–ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ë®ì–´ì“°ì§€ ì•Šê±°ë‚˜
        # í˜¹ì€ ì—¬ê¸°ì„œ ë‹¤ë¥¸ ë©”ì‹œì§€ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ê²€ìƒ‰ ë¶ˆê°€ ë©”ì‹œì§€ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
        # ì‹¤ì œë¡œëŠ” 'unknown' íƒ€ì…ì€ ì´ ë…¸ë“œë¡œ ì˜¤ì§€ ì•Šê³  unknown_handler_nodeë¡œ ê°€ì•¼ í•©ë‹ˆë‹¤.
        # ì´ ì½”ë“œëŠ” execute_rag_nodeê°€ 'unknown' íƒ€ì…ìœ¼ë¡œ í˜¸ì¶œë  ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ë°©ì–´ ì½”ë“œì…ë‹ˆë‹¤.
        return {"result": "ì ì ˆí•œ ë¬¸ì„œ ì €ì¥ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}


    print(f"   - Pinecone ë„¤ì„ìŠ¤í˜ì´ìŠ¤ '{namespace_to_search}'ì—ì„œ ê²€ìƒ‰ ì‹œì‘...")
    index_stats = pinecone_index.describe_index_stats()
    if namespace_to_search not in index_stats.namespaces or \
        index_stats.namespaces[namespace_to_search].vector_count == 0:
        message = f"'{namespace_to_search}' ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ Pineconeì—ì„œ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜, í•´ë‹¹ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. Pinecone ëŒ€ì‹œë³´ë“œì—ì„œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì´ë¦„ê³¼ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        print(f"   - ê²½ê³ : {message}")
        return {"result": message}

    res = pinecone_index.query(
        vector=query_vector,
        namespace=namespace_to_search,
        top_k=5, # ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
        include_metadata=True
    )
    matches = res.matches
    print(f"   - Pinecone ê²€ìƒ‰ ì™„ë£Œ: {len(matches)}ê°œ ê²°ê³¼ ìˆ˜ì‹ ")

    if not matches:
        message = f"'{namespace_to_search}' ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì—ì„œ '{state.user_input}' ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        print(f"   - ì •ë³´ ì—†ìŒ: {message}")
        return {"result": message}
    
    context = build_context_from_matches(matches)
    if not context:
        message = "ê²€ìƒ‰ëœ ì •ë³´ì—ì„œ ë‹µë³€ì„ ìƒì„±í•  ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        print(f"   - ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶• ì‹¤íŒ¨: {message}")
        return {"result": message}
    print(f"   - ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶• ì™„ë£Œ (ê¸¸ì´: {len(context)})")

    return {"result": context}
    

def summarize_node(state: State) -> str:
    text = state.result
    document_type = state.document_type
    user_input = state.user_input
    order = ""

    if document_type == "proceedings" : 
        order = (
            "ë„ˆëŠ” íšŒì˜ë¡ ì–´ì‹œìŠ¤í„´íŠ¸ì•¼. ë‚´ê°€ íšŒì˜ë¡ í…ìŠ¤íŠ¸ë¥¼ ì£¼ë©´, "
            "ì²«ë²ˆì§¸ë¡œ íšŒì˜ ì£¼ì œ / ëª©ì , "
            "ë‘ë²ˆì§¸ë¡œ í•µì‹¬ ë…¼ì˜ ì‚¬í•­, "
            "ì„¸ë²ˆì§¸ë¡œ ê²°ì •ëœ ì‚¬í•­, "
            "ë„¤ë²ˆì§¸ë¡œ ë³´ë¥˜ë˜ê±°ë‚˜ ì¶”ê°€ ë…¼ì˜ í•­ëª©ì„ "
            "ìš”ì•½í•´ì„œ í•œêµ­ì–´ë¡œ ì•Œë ¤ì¤˜."
        )
    elif document_type == "internal_policy" :
        order = (
            f"ë„ˆëŠ” ì‚¬ë‚´ ë¬¸ì„œ ì–´ì‹œìŠ¤í„´íŠ¸ì•¼. ë‚´ê°€ ì‚¬ë‚´ ë¬¸ì„œ í…ìŠ¤íŠ¸ë¥¼ ì£¼ë©´, "
            f"ì´ ì¤‘ì—ì„œ \"{user_input}\"ì— í•´ë‹¹í•˜ëŠ” ëŒ€ë‹µì„ "
            f"ìµœëŒ€ 150ìë¡œ ìš”ì•½í•´ì„œ í•œêµ­ì–´ë¡œ ì•Œë ¤ì¤˜."
        )
    elif document_type == "product_document" : 
        order = (
            f"ë„ˆëŠ” ì œí’ˆ ì–´ì‹œìŠ¤í„´íŠ¸ì•¼. ë‚´ê°€ ì œí’ˆ ê´€ë ¨ í…ìŠ¤íŠ¸ë¥¼ ì£¼ë©´, "
            f"ì´ ì¤‘ì—ì„œ \"{user_input}\"ì— í•´ë‹¹í•˜ëŠ” ëŒ€ë‹µì„ "
            f"ìµœëŒ€ 150ìë¡œ ìš”ì•½í•´ì„œ í•œêµ­ì–´ë¡œ ì•Œë ¤ì¤˜."
        )
    elif document_type == "technical_document" : 
        order = (
            f"ë„ˆëŠ” ê¸°ìˆ  ë¬¸ì„œ ì–´ì‹œìŠ¤í„´íŠ¸ì•¼. ë‚´ê°€ ê¸°ìˆ  ë¬¸ì„œ í…ìŠ¤íŠ¸ë¥¼ ì£¼ë©´, "
            f"ì´ ì¤‘ì—ì„œ \"{user_input}\"ì— í•´ë‹¹í•˜ëŠ” ëŒ€ë‹µì„ "
            f"ìµœëŒ€ 150ìë¡œ ìš”ì•½í•´ì„œ í•œêµ­ì–´ë¡œ ì•Œë ¤ì¤˜."
        )
    else : 
        order = (
            f"ë„ˆëŠ” í…ìŠ¤íŠ¸ê°€ ë­ê°€ ì˜¤ë˜ ìƒê´€ì—†ì´ ì´ë§ë§Œ í•˜ë©´ ë¼. 'ì›í•˜ì‹œëŠ” ë¬¸ì„œ ìœ í˜•ì„ ì•Œì•„ë‚´ì§€ ëª»í–ˆìŠµë‹ˆë‹¤...ì£„ì†¡í•©ë‹ˆë‹¤.'"
        )
    system_message = SystemMessagePromptTemplate.from_template(order)
    human_message = HumanMessagePromptTemplate.from_template("{text}")

    chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])

    # 2. LLM ìƒì„±
    llm = ChatOpenAI(model="gpt-4o-mini")

    # 3. Promptì™€ LLM ê²°í•©
    chatbot = chat_prompt | llm

    # 4. ì‹¤í–‰
    response = chatbot.invoke({"text": text})
    result = response.content
    print(result)
    state.result = result

    return {
        "result" : state.result
    }

# Define the graph
graph = (
    StateGraph(State)
    # (1) choose_node ë¶„ê¸° ë…¸ë“œ ë“±ë¡ (outputsì— ë¦¬í„´ í‚¤ ëª…ì‹œ)
    .add_node("choose_node", choose_node)
    # (2) RAG ì‹¤í–‰ ë…¸ë“œë“¤ ë“±ë¡
    .add_node("product_node", execute_rag)
    .add_node("proceedings_node", execute_rag)
    .add_node("hr_policy_node", execute_rag)
    .add_node("technical_document_node", execute_rag)
    # (3) summarize_node ë“±ë¡ (ìµœì¢… ë…¸ë“œ)
    .add_node("summarize_node", summarize_node)
    # (4) START â†’ ë¶„ê¸° ë…¸ë“œ(choose_node) â†’ (next_node ê°’ì— ë”°ë¼) ë¶„ê¸°
    .add_edge(START,"choose_node")
    .add_conditional_edges(
        "choose_node",
        choose_one,
        {
            "product_document": "product_node",
            "proceedings": "proceedings_node",
            "internal_policy": "hr_policy_node",
            "technical_document": "technical_document_node"
        }
    )
    # (5) ê° RAG ë…¸ë“œ â†’ summarize_node ì—°ê²°
    .add_edge("product_node", "summarize_node")
    .add_edge("proceedings_node", "summarize_node")
    .add_edge("hr_policy_node", "summarize_node")
    .add_edge("technical_document_node", "summarize_node")
    .add_edge("summarize_node", END)
    # (6) ìµœì¢… ì»´íŒŒì¼
    .compile(name="New Graph")
)