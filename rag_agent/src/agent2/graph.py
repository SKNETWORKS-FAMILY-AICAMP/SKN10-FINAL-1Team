# graph.py

from __future__ import annotations

import os
from dataclasses import dataclass
from typing import TypedDict, Dict # Dict ì¶”ê°€

from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langgraph.graph import END, START, StateGraph
from dotenv import load_dotenv

# rag.pyì—ì„œ RAG íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ë“¤ ê°€ì ¸ì˜¤ê¸°
# rag.py íŒŒì¼ì´ graph.pyì™€ ë™ì¼í•œ ë””ë ‰í† ë¦¬ ë˜ëŠ” Python ê²½ë¡œì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
from rag import (
    init_clients,
    embed_query,
    build_context_from_matches,
    generate_answer_with_context
)

# .env íŒŒì¼ ê²½ë¡œ ì§€ì • ë° ë¡œë“œ
# graph.py íŒŒì¼ ìœ„ì¹˜: SKN10-FINAL-1Team/rag_agent/src/agent/
# .env íŒŒì¼ ìœ„ì¹˜: SKN10-FINAL-1Team/
dotenv_path = os.path.join(os.path.dirname(__file__), '..', '..', '..', '.env')
if os.path.exists(dotenv_path):
    load_dotenv(dotenv_path=dotenv_path)
    print(f".env íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {dotenv_path}")
else:
    print(f"ê²½ê³ : .env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œ: {dotenv_path}")
    # í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ê°€ ì—†ë‹¤ë©´ ì—¬ê¸°ì„œ ì¤‘ë‹¨í•˜ê±°ë‚˜, ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•˜ë„ë¡ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ì—¬ê¸°ì„œëŠ” ì•„ë˜ main ë¸”ë¡ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ì¡´ì¬ ì—¬ë¶€ë¥¼ ë‹¤ì‹œ í•œë²ˆ í™•ì¸í•©ë‹ˆë‹¤.

class Configuration(TypedDict):
    """Configurable parameters for the agent."""
    my_configurable_param: str # ì˜ˆì‹œ íŒŒë¼ë¯¸í„°, ì‹¤ì œ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ ì œê±° ê°€ëŠ¥

@dataclass
class State:
    user_input: str
    document_type: str = ""  # LLMì´ ë¶„ë¥˜í•œ ë¬¸ì„œ íƒ€ì… (Pinecone ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì™€ ì¼ì¹˜í•´ì•¼ í•¨)
    result: str = ""         # ìµœì¢… RAG ê²°ê³¼ ë˜ëŠ” ì—ëŸ¬ ë©”ì‹œì§€


def choose_document_type_llm(message: str):
    """LLMì„ í˜¸ì¶œí•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ë¬¸ì„œ ìœ í˜•ì„ ê²°ì •í•©ë‹ˆë‹¤."""
    # Pinecone ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì´ë¦„ê³¼ ì¼ì¹˜í•˜ë„ë¡ í‚¤ì›Œë“œ ë° ì„¤ëª… ìˆ˜ì •
    # 'minutes_document'ëŠ” íšŒì˜ë¡ì„ ìœ„í•œ ì˜ˆì‹œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì…ë‹ˆë‹¤.
    # ì‹¤ì œ Pinecone ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒí™©ì— ë§ê²Œ í‚¤ì›Œë“œì™€ ì„¤ëª…ì„ ì¡°ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    prompt_template_str = (
        "'{input}'ì€(ëŠ”) ì‚¬ìš©ì ì§ˆë¬¸ì…ë‹ˆë‹¤. "
        "ì´ ì§ˆë¬¸ì´ ë‹¤ìŒ ë¬¸ì„œ ìœ í˜• ì¤‘ ì–´ë–¤ ê²ƒì— ê°€ì¥ ì í•©í•œì§€ íŒë‹¨í•´ì£¼ì„¸ìš”: "
        "internal_policy (ì‚¬ë‚´ ê·œì • ë° ì •ì±… ê´€ë ¨), "
        "product_document (ì œí’ˆ ì„¤ëª… ë° ì‚¬ì–‘ ê´€ë ¨), "
        "technical_document (ê¸°ìˆ  ë¬¸ì„œ, ë¬¸ì œ í•´ê²°, ë§¤ë‰´ì–¼ ê´€ë ¨), "
        "minutes_document (íšŒì˜ë¡ ìš”ì•½ ë° ê²€ìƒ‰ ê´€ë ¨), "
        "unknown (ìœ„ ì–´ë–¤ ìœ í˜•ì—ë„ ëª…í™•íˆ ì†í•˜ì§€ ì•ŠëŠ” ì¼ë°˜ ì§ˆë¬¸ ë˜ëŠ” ë¶„ë¥˜ ë¶ˆê°€ëŠ¥í•œ ì§ˆë¬¸). "
        "ë‹¹ì‹ ì˜ ë‹µë³€ì€ ë°˜ë“œì‹œ ë‹¤ìŒ í‚¤ì›Œë“œ ì¤‘ ì •í™•íˆ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤: "
        "'internal_policy', 'product_document', 'technical_document', 'minutes_document', 'unknown'. "
        "ë‹¤ë¥¸ ì–´ë–¤ ì¶”ê°€ í…ìŠ¤íŠ¸ë„ í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤."
    )
    prompt = PromptTemplate(
        input_variables=["input"],
        template=prompt_template_str
    )
    # OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
    llm = ChatOpenAI(model="gpt-3.5-turbo") # ë˜ëŠ” ì„ í˜¸í•˜ëŠ” ë‹¤ë¥¸ ëª¨ë¸
    chain = prompt | llm
    response = chain.invoke({"input": message})
    print(f"LLM ì›ë³¸ ë¬¸ì„œ íƒ€ì… ë¶„ë¥˜ ì‘ë‹µ: {response.content}")
    return response

def classify_document_node(state: State) -> Dict[str, str]:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ìœ í˜•ì„ ë¶„ë¥˜í•˜ê³ , íŒŒì‹±í•˜ì—¬ state.document_typeì„ ì—…ë°ì´íŠ¸í•  ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    user_input = state.user_input
    raw_llm_response = choose_document_type_llm(user_input).content.strip().lower()

    # ì‹¤ì œ Pinecone ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë° 'unknown' í¬í•¨ (minutes_documentëŠ” ì˜ˆì‹œ)
    # ** ì¤‘ìš”: ì´ keywords ë¦¬ìŠ¤íŠ¸ëŠ” ì‹¤ì œ Pinecone ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì™€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤. **
    # ** 'minutes_document'ëŠ” ì˜ˆì‹œì´ë©°, ì‹¤ì œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ê°€ ì—†ë‹¤ë©´ ì œê±°í•˜ê±°ë‚˜ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤. **
    keywords = ["internal_policy", "product_document", "technical_document", "minutes_document", "unknown"]
    parsed_document_type = ""

    for keyword in keywords:
        if f"'{keyword}'" in raw_llm_response or keyword == raw_llm_response:
            parsed_document_type = keyword
            break
    
    if not parsed_document_type: # í‚¤ì›Œë“œê°€ ë‹¨ìˆœíˆ í¬í•¨ëœ ê²½ìš°
        for keyword in keywords:
            if keyword in raw_llm_response:
                parsed_document_type = keyword
                print(f"ë‹¨ìˆœ í¬í•¨ìœ¼ë¡œ '{keyword}' í‚¤ì›Œë“œ íŒŒì‹±ë¨.")
                break
    
    if not parsed_document_type:
        print(f"ê²½ê³ : LLM ì‘ë‹µì—ì„œ ìœ íš¨í•œ ë¬¸ì„œ íƒ€ì… í‚¤ì›Œë“œë¥¼ íŒŒì‹±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì›ë³¸: '{raw_llm_response}'. 'unknown'ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        parsed_document_type = "unknown"

    print(f"ìµœì¢… íŒŒì‹±ëœ ë¬¸ì„œ íƒ€ì…: '{parsed_document_type}'")
    return {"document_type": parsed_document_type}

def route_after_classification(state: State) -> str:
    """state.document_typeì— ë”°ë¼ ë‹¤ìŒ ë…¸ë“œë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤."""
    doc_type = state.document_type
    print(f"ë¼ìš°íŒ… í•¨ìˆ˜ ì§„ì… - state.document_type: '{doc_type}'")

    # ì‹¤ì œ Pinecone ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì™€ 'unknown'ì— ëŒ€í•œ ë¼ìš°íŒ…
    # ** ì¤‘ìš”: ì´ ì¡°ê±´ë¬¸ë“¤ì€ ìœ„ keywords ë¦¬ìŠ¤íŠ¸ ë° ì‹¤ì œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì™€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤. **
    if doc_type == "internal_policy":
        return "internal_policy"
    elif doc_type == "product_document":
        return "product_document"
    elif doc_type == "technical_document":
        return "technical_document"
    elif doc_type == "minutes_document": # íšŒì˜ë¡ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì˜ˆì‹œ
        return "minutes_document"
    elif doc_type == "unknown":
        return "unknown"
    else:
        error_message = f"ë¶„ë¥˜ ë…¸ë“œë¡œë¶€í„° ì˜ˆê¸°ì¹˜ ì•Šì€ ë¬¸ì„œ íƒ€ì…('{doc_type}')ì„ ë°›ì•˜ìŠµë‹ˆë‹¤. 'unknown'ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤."
        print(error_message)
        # ì´ ê²½ìš°, classify_document_nodeì—ì„œ ì´ë¯¸ 'unknown'ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
        return "unknown"


def execute_rag_node(state: State) -> Dict[str, str]:
    """ê³µí†µ RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    print(f"\nğŸ“„ RAG ë…¸ë“œ ì‹¤í–‰: ë¬¸ì„œ íƒ€ì… = '{state.document_type}', ì§ˆë¬¸ = '{state.user_input}'")
    try:
        openai_client, pinecone_index = init_clients()
        print("   - í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

        query_vector = embed_query(openai_client, state.user_input)
        print(f"   - ì§ˆë¬¸ ì„ë² ë”© ì™„ë£Œ (ë²¡í„° í¬ê¸°: {len(query_vector)})")

        namespace_to_search = state.document_type
        if not namespace_to_search or namespace_to_search == "unknown":
            message = f"ë¬¸ì„œ íƒ€ì…ì´ '{namespace_to_search}'(ìœ¼)ë¡œ ë¶„ë¥˜ë˜ì–´ Pinecone ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            print(f"   - ì •ë³´: {message}")
            # 'unknown'ì¼ ê²½ìš°, unknown_handler_nodeì—ì„œ ì´ë¯¸ ë©”ì‹œì§€ë¥¼ ì„¤ì •í–ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ë®ì–´ì“°ì§€ ì•Šê±°ë‚˜
            # í˜¹ì€ ì—¬ê¸°ì„œ ë‹¤ë¥¸ ë©”ì‹œì§€ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ê²€ìƒ‰ ë¶ˆê°€ ë©”ì‹œì§€ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
            # ì‹¤ì œë¡œëŠ” 'unknown' íƒ€ì…ì€ ì´ ë…¸ë“œë¡œ ì˜¤ì§€ ì•Šê³  unknown_handler_nodeë¡œ ê°€ì•¼ í•©ë‹ˆë‹¤.
            # ì´ ì½”ë“œëŠ” execute_rag_nodeê°€ 'unknown' íƒ€ì…ìœ¼ë¡œ í˜¸ì¶œë  ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ë°©ì–´ ì½”ë“œì…ë‹ˆë‹¤.
            return {"result": "ì ì ˆí•œ ë¬¸ì„œ ì €ì¥ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}


        print(f"   - Pinecone ë„¤ì„ìŠ¤í˜ì´ìŠ¤ '{namespace_to_search}'ì—ì„œ ê²€ìƒ‰ ì‹œì‘...")
        index_stats = pinecone_index.describe_index_stats()
        if namespace_to_search not in index_stats.namespaces or \
           index_stats.namespaces[namespace_to_search].vector_count == 0:
            message = f"'{namespace_to_search}' ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ Pineconeì—ì„œ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜, í•´ë‹¹ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. Pinecone ëŒ€ì‹œë³´ë“œì—ì„œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì´ë¦„ê³¼ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
            print(f"   - ê²½ê³ : {message}")
            return {"result": message}

        res = pinecone_index.query(
            vector=query_vector,
            namespace=namespace_to_search,
            top_k=5, # ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            include_metadata=True
        )
        matches = res.matches
        print(f"   - Pinecone ê²€ìƒ‰ ì™„ë£Œ: {len(matches)}ê°œ ê²°ê³¼ ìˆ˜ì‹ ")

        if not matches:
            message = f"'{namespace_to_search}' ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì—ì„œ '{state.user_input}' ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            print(f"   - ì •ë³´ ì—†ìŒ: {message}")
            return {"result": message}

        context = build_context_from_matches(matches)
        if not context:
            message = "ê²€ìƒ‰ëœ ì •ë³´ì—ì„œ ë‹µë³€ì„ ìƒì„±í•  ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            print(f"   - ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶• ì‹¤íŒ¨: {message}")
            return {"result": message}
        print(f"   - ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶• ì™„ë£Œ (ê¸¸ì´: {len(context)})")

        # rag.pyì˜ generate_answer_with_context í•¨ìˆ˜ì—ì„œ max_tokens ì¡°ì ˆ ê°€ëŠ¥
        answer = generate_answer_with_context(openai_client, state.user_input, context)
        print(f"   - LLM ë‹µë³€ ìƒì„± ì™„ë£Œ (ì¼ë¶€): '{answer[:100]}...'") # ì „ì²´ ë‹µë³€ ëŒ€ì‹  ì¼ë¶€ë§Œ ë¡œê¹…
        return {"result": answer}

    except ValueError as ve:
        print(f"RAG íŒŒì´í”„ë¼ì¸ ê°’ ì—ëŸ¬: {ve}")
        return {"result": f"ì •ë³´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {ve}"}
    except Exception as e:
        print(f"RAG íŒŒì´í”„ë¼ì¸ ì˜ˆì™¸ ë°œìƒ: {e} (íƒ€ì…: {type(e).__name__})")
        import traceback
        traceback.print_exc() # ë””ë²„ê¹…ì„ ìœ„í•´ ì „ì²´ traceback ì¶œë ¥
        return {"result": f"ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"}

def unknown_handler_node(state: State) -> Dict[str, str]:
    """ë¶„ë¥˜í•  ìˆ˜ ì—†ëŠ” ì§ˆë¬¸ì´ë‚˜ 'unknown' íƒ€ì…ìœ¼ë¡œ ì§€ì •ëœ ê²½ìš° í˜¸ì¶œë˜ëŠ” ë…¸ë“œì…ë‹ˆë‹¤."""
    print(f"\nâ“ 'unknown' í•¸ë“¤ëŸ¬ ë…¸ë“œ ì‹¤í–‰: ì§ˆë¬¸ = '{state.user_input}'")
    message = "ì£„ì†¡í•©ë‹ˆë‹¤, ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ê±°ë‚˜ ì§ˆë¬¸ì— í•´ë‹¹í•˜ëŠ” ì ì ˆí•œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì‹œê² ì–´ìš”?"
    return {"result": message}


# ê·¸ë˜í”„ ì •ì˜
workflow = StateGraph(State, config_schema=Configuration)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("classify_document_node", classify_document_node)
workflow.add_node("unknown_handler_node", unknown_handler_node)

# RAG ë…¸ë“œ: Pinecone ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì™€ ì¼ì¹˜í•˜ëŠ” ì´ë¦„ìœ¼ë¡œ ë…¸ë“œ ë“±ë¡ (ê°€ë…ì„± í–¥ìƒ)
workflow.add_node("internal_policy_rag", execute_rag_node)
workflow.add_node("product_document_rag", execute_rag_node)
workflow.add_node("technical_document_rag", execute_rag_node)
workflow.add_node("minutes_document_rag", execute_rag_node) # íšŒì˜ë¡ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ìš© RAG ë…¸ë“œ (ì˜ˆì‹œ)


# ê·¸ë˜í”„ íë¦„ ì •ì˜
workflow.add_edge(START, "classify_document_node")

workflow.add_conditional_edges(
    "classify_document_node",
    route_after_classification,
    {
        # route_after_classificationì—ì„œ ë°˜í™˜ëœ í‚¤ì™€ ë§¤í•‘ë  ë…¸ë“œ
        "internal_policy": "internal_policy_rag",
        "product_document": "product_document_rag",
        "technical_document": "technical_document_rag",
        "minutes_document": "minutes_document_rag", # íšŒì˜ë¡ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì˜ˆì‹œ
        "unknown": "unknown_handler_node"
    }
)

# ê° RAG ì²˜ë¦¬ ë…¸ë“œ ë° unknown í•¸ë“¤ëŸ¬ ë…¸ë“œ ì‹¤í–‰ í›„, ê·¸ë˜í”„ë¥¼ ì¢…ë£Œ
workflow.add_edge("internal_policy_rag", END)
workflow.add_edge("product_document_rag", END)
workflow.add_edge("technical_document_rag", END)
workflow.add_edge("minutes_document_rag", END) # íšŒì˜ë¡ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì˜ˆì‹œ
workflow.add_edge("unknown_handler_node", END)

graph = workflow.compile(name="Interactive RAG Agent Graph")


# --- ë¡œì»¬ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì‹¤í–‰ ì½”ë“œ ---
if __name__ == "__main__":
    if not all(os.getenv(key) for key in ["OPENAI_API_KEY", "PINECONE_API_KEY", "PINECONE_ENVIRONMENT"]):
        print("âš ï¸  í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜(OPENAI_API_KEY, PINECONE_API_KEY, PINECONE_ENVIRONMENT)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    else:
        print("âœ… í™˜ê²½ ë³€ìˆ˜ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. RAG ì—ì´ì „íŠ¸ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

        while True:
            user_question = input("\në‹¹ì‹ ì˜ ì§ˆë¬¸: ")
            if user_question.lower() in ["exit", "quit"]:
                print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            if not user_question.strip():
                print("ì§ˆë¬¸ì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                continue

            inputs = {"user_input": user_question}
            
            print(f"\n--- ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘: \"{user_question}\" ---")
            try:
                final_state = graph.invoke(inputs)
                print(f"\nğŸ¤– ì—ì´ì „íŠ¸ ë‹µë³€:")
                print(final_state.get('result', 'ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'))
                # print(f"   (ìµœì¢… ìƒíƒœ Debug: document_type='{final_state.get('document_type')}', user_input='{final_state.get('user_input')}')") # ë””ë²„ê¹…ìš©

            except Exception as e:
                print(f"   ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                # import traceback # ìì„¸í•œ ì˜¤ë¥˜ë¥¼ ë³´ë ¤ë©´ ì£¼ì„ í•´ì œ
                # traceback.print_exc()
            print("--- ì§ˆë¬¸ ì²˜ë¦¬ ì™„ë£Œ ---")