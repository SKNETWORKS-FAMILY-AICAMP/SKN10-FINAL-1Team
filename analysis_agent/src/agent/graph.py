"""LangGraph graph for analysis_agent with supervisor logic.

Handles DB queries and general questions based on node/edge routing.
"""

from __future__ import annotations

import os
from typing import Optional, Dict, Any, List, Annotated, Literal, TypedDict
import asyncio
import io
from dotenv import load_dotenv
import os
import operator # For adding to message history
import psycopg2
import pandas as pd
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage # For message types
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableConfig # Added missing import
from langchain_core.output_parsers import JsonOutputParser
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from pydantic import BaseModel, Field
import pandas as pd
import numpy as np
import joblib
from sklearn.preprocessing import LabelEncoder

# --- Configuration (Optional - can be used to pass API keys, model names, etc.) ---
class Configuration(TypedDict, total=False):
    openai_api_key: Optional[str]
    db_env_path: Optional[str] # Path to .env file for DB credentials

# --- State Definition ---
class AgentState(BaseModel):
    messages: Annotated[List[BaseMessage], operator.add] = Field(default_factory=list)
    user_query: Optional[str] = None
    csv_file_content: Optional[str] = None
    query_type: Optional[Literal["db_query", "category_predict_query", "general_query"]] = None
    sql_query: Optional[str] = None
    sql_result: Optional[Any] = None
    final_answer: Optional[str] = None
    error_message: Optional[str] = None
    visualization_output: Optional[str] = None
    sql_output_choice: Optional[Literal["summarize", "visualize"]] = None # Decision for SQL output processing

    class Config:
        arbitrary_types_allowed = True # For Annotated and operator.add with BaseMessage

# --- LLM and Prompts Setup ---
# Ensure OPENAI_API_KEY is set in your environment or passed via config
llm = ChatOpenAI(temperature=0, model="gpt-4o") # Or your preferred model

supervisor_prompt = PromptTemplate.from_template(
    "Analyze the user's question. Respond with a JSON object.\n"
    "The JSON object MUST contain a 'query_type' field set to one of 'db_query', 'category_predict_query', or 'general_query'.\n\n"
    "User Question: {user_query}\n\n"
    "JSON Response (must be a valid JSON object adhering to the Pydantic model `SupervisorDecision`):"
)

sql_generation_prompt = PromptTemplate.from_template(
    """Based on the following user question and the provided database schema information, convert the question into a SQL query in PostgreSQL syntax AND determine if the result should be summarized or visualized.
Accurately understand the user's intent and use the schema information to write a query with the correct tables and columns.
For text-based searches (e.g., using LIKE clauses on string columns), ensure the search is case-insensitive by using the ILIKE operator or by applying the LOWER() function to both the column and the search term, unless the user specifically requests a case-sensitive search.
If the information needed for the question is not in the schema or is ambiguous, please state that or use the most probable interpretation.

Database Schema Information:

Table Name: analytics_results
Columns:
  - id (uuid)
  - result_type (character varying)
  - s3_key (text)
  - meta (jsonb)
  - created_at (timestamp with time zone)
  - user_id (uuid)

Table Name: chat_messages
Columns:
  - id (uuid)
  - role (character varying)
  - content (text)
  - created_at (timestamp with time zone)
  - session_id (uuid)
  - metadata (text)

Table Name: chat_sessions
Columns:
  - id (uuid)
  - agent_type (character varying)
  - started_at (timestamp with time zone)
  - ended_at (timestamp with time zone)
  - user_id (uuid)
  - title (character varying)

Table Name: llm_calls
Columns:
  - id (uuid)
  - call_type (character varying)
  - prompt (text)
  - response (text)
  - tokens_used (integer)
  - latency_ms (integer)
  - created_at (timestamp with time zone)
  - user_id (uuid)
  - session_id (uuid)

Table Name: model_artifacts
Columns:
  - id (uuid)
  - artifact_type (character varying)
  - s3_key (text)
  - meta (jsonb)
  - created_at (timestamp with time zone)
  - user_id (uuid)


Table Name: organizations
Columns:
  - id (uuid)
  - name (character varying)
  - created_at (timestamp with time zone)

Table Name: summary_news_keywords
Columns:
  - id (uuid)
  - date (date)
  - keyword (text)
  - title (text)
  - summary (text)
  - url (text)

Table Name: telecom_customers
Columns:
  - customer_id (character varying)
  - gender (character varying)
  - senior_citizen (boolean)
  - partner (boolean)
  - dependents (boolean)
  - tenure (integer)
  - phone_service (boolean)
  - multiple_lines (character varying)
  - internet_service (character varying)
  - online_security (character varying)
  - online_backup (character varying)
  - device_protection (character varying)
  - tech_support (character varying)
  - streaming_tv (character varying)
  - streaming_movies (character varying)
  - contract (character varying)
  - paperless_billing (boolean)
  - payment_method (character varying)
  - monthly_charges (numeric)
  - total_charges (numeric)
  - churn (boolean)

Table Name: users
Columns:
  - password (character varying)
  - is_superuser (boolean)
  - id (uuid)
  - email (character varying)
  - name (character varying)
  - role (character varying)
  - created_at (timestamp with time zone)
  - last_login (timestamp with time zone)
  - is_active (boolean)
  - is_staff (boolean)
  - org_id (uuid)

Respond with a JSON object that strictly adheres to the Pydantic model `SQLGenerationOutput` shown below.
The `sql_query` field MUST contain ONLY the SQL query string, without any surrounding text, explanations, or markdown formatting like ```sql.
The `sql_output_choice` field MUST be either 'summarize' (if the user asks for a textual summary, explanation, or direct answer from data) or 'visualize' (if the user asks for a chart, graph, or visual representation). Prioritize 'visualize' if visualization is explicitly or implicitly requested. If unsure, default to 'summarize'.

```python
class SQLGenerationOutput(BaseModel):
    sql_query: str = Field(description="The generated SQL query. This field MUST contain ONLY the SQL query string, without any surrounding text, explanations, or markdown formatting like ```sql.")
    sql_output_choice: Literal["summarize", "visualize"] = Field(description="The type of output processing required for the SQL result: 'summarize' or 'visualize'.")
```

User Question: {user_query}

JSON Response (must be a valid JSON object conforming to SQLGenerationOutput):
    """
)

summarization_prompt = PromptTemplate.from_template(
    "Based on the following SQL query execution result, please summarize the answer to the user's original question naturally.\n\n"
    "say in Korean"
    "User Question: {user_query}\n"
    "SQL Query: {sql_query}\n"
    "SQL Result:\n{sql_result}\n\n"
    "Summary Answer:"
)

general_answer_prompt = PromptTemplate.from_template(
    "Please answer the following user question.\n\n"
    "User Question: {user_query}\n\n"
    "Answer:"
)

# --- Pydantic Models for Structured Output ---
class SupervisorDecision(BaseModel):
    query_type: str = Field(description="The type of the user's question (db_query, category_predict_query, or general_query)")

class SQLGenerationOutput(BaseModel):
    sql_query: str = Field(description="The generated SQL query. This field MUST contain ONLY the SQL query string, without any surrounding text, explanations, or markdown formatting like ```sql.")
    sql_output_choice: Literal["summarize", "visualize"] = Field(description="The type of output processing required for the SQL result: 'summarize' or 'visualize'.")

# --- Node Functions ---
async def supervisor_node(state: AgentState, config: Optional[RunnableConfig] = None) -> Dict[str, Any]:
    print("--- SUPERVISOR NODE (Debug v4 - Enhanced Message Parsing) ---")
    
    user_query_to_process: Optional[str] = None
    current_messages = state.messages # Keep a reference to the current messages

    print(f"Supervisor - Initial state.messages length: {len(current_messages) if current_messages else 0}")

    if current_messages:
        last_message_obj = current_messages[-1]
        print(f"Supervisor - Last message object: {last_message_obj}, type: {type(last_message_obj).__name__}")

        content_candidate: Optional[str] = None

        # Try to extract content based on common patterns
        if hasattr(last_message_obj, 'content') and isinstance(getattr(last_message_obj, 'content'), str):
            # Covers HumanMessage, AIMessage, and other BaseMessage with a .content attribute
            content_candidate = getattr(last_message_obj, 'content')
            print(f"Supervisor - Candidate from .content attribute: '{content_candidate[:100] if content_candidate else 'None'}...'")
        elif isinstance(last_message_obj, dict) and 'content' in last_message_obj and isinstance(last_message_obj['content'], str):
            # Covers cases where the message might be a dictionary (e.g., from serialization)
            content_candidate = last_message_obj['content']
            print(f"Supervisor - Candidate from dict['content']: '{content_candidate[:100] if content_candidate else 'None'}...'")
        elif isinstance(last_message_obj, str):
            # Covers cases where the last message itself is a plain string
            content_candidate = last_message_obj
            print(f"Supervisor - Candidate from last_message_obj being a string: '{content_candidate[:100] if content_candidate else 'None'}...'")
        
        # Ensure the extracted content is a non-empty string
        if content_candidate and content_candidate.strip():
            user_query_to_process = content_candidate.strip()
            print(f"Supervisor - Successfully extracted user query: '{user_query_to_process[:100]}...'")
        else:
            print(f"Supervisor - Content candidate was None, empty, or whitespace. Candidate: '{content_candidate}'")
    else:
        print("Supervisor - state.messages is empty.")

    if not user_query_to_process:
        error_msg = "Supervisor - No valid user query found in state.messages. Please provide input via the 'Messages' field with textual content."
        print(f"Supervisor - {error_msg}")
        return {
            "user_query": None,
            "query_type": "general_query", # Default to general_query if no input
            "error_message": error_msg,
            "messages": current_messages # Pass through existing messages
        }

    # If user_query_to_process is successfully set, proceed with LLM analysis
    print(f"Supervisor - Processing user query: '{user_query_to_process}' for LLM analysis.")
    
    supervisor_chain = supervisor_prompt | llm.with_structured_output(SupervisorDecision)
    
    try:
        # Ensure the user_query for the LLM is the one we processed
        parsed_output: SupervisorDecision = await supervisor_chain.ainvoke({"user_query": user_query_to_process}, config=config)
        query_type = parsed_output.query_type
        print(f"Supervisor Decision: query_type = {query_type}")

        updated_messages = current_messages + [AIMessage(content=f"Routing to {query_type} based on supervisor decision.")]
        return {
            "messages": updated_messages,
            "user_query": user_query_to_process,
            "query_type": query_type,
            "error_message": None # Clear any previous error messages
        }
    except Exception as e:
        error_msg = f"Supervisor - Error during LLM decision for query '{user_query_to_process[:50]}...': {e}"
        print(error_msg)
        return {
            "user_query": user_query_to_process, # Query was extracted, but LLM failed
            "query_type": "general_query", # Default to general if LLM fails
            "error_message": error_msg,
            "messages": current_messages # Pass through existing messages
        }

async def generate_sql_node(state: AgentState, config: Optional[RunnableConfig] = None) -> Dict[str, Any]:
    print("--- GENERATE SQL NODE ---")
    user_query = state.user_query # Get user_query from the state
    if not user_query:
        return {"error_message": "No user query found for SQL generation.", "sql_query": "", "sql_output_choice": None}

    print(f"Generating SQL and determining output choice for query: {user_query[:100]}...")
    # Use llm.with_structured_output for more robust parsing to the Pydantic model
    structured_llm_sql_gen = llm.with_structured_output(SQLGenerationOutput)
    # The runnable now takes the prompt and applies the structured LLM
    sql_generation_runnable = sql_generation_prompt | structured_llm_sql_gen

    try:
        # Pass the user_query to the runnable, which will format the prompt and call the structured LLM
        response_model = await sql_generation_runnable.ainvoke({"user_query": user_query}, config=config)
        # response_model should now be an instance of SQLGenerationOutput
        sql_query = response_model.sql_query.strip()
        sql_output_choice = response_model.sql_output_choice
        print(f"Generated SQL: {sql_query}")
        print(f"Determined SQL Output Choice: {sql_output_choice}")
        return {"sql_query": sql_query, "sql_output_choice": sql_output_choice, "error_message": None}
    except Exception as e:
        error_msg = f"Error generating SQL or determining output choice: {e}"
        print(error_msg)
        return {"error_message": error_msg, "sql_query": "", "sql_output_choice": None}

import asyncio # Required for asyncio.to_thread

# Helper function for synchronous DB operations, including dotenv loading
def _execute_sql_sync(sql_query: str, base_dir_analysis_env: str, base_dir_my_state_env: str) -> Dict[str, Any]:
    # Determine .env path and load environment variables
    dotenv_path_analysis = os.path.join(base_dir_analysis_env, '.env')
    dotenv_path_my_state = os.path.join(base_dir_my_state_env, '.env')
    specific_env_path = None

    if os.path.exists(dotenv_path_analysis):
        specific_env_path = dotenv_path_analysis
    elif os.path.exists(dotenv_path_my_state):
        specific_env_path = dotenv_path_my_state

    if specific_env_path:
        print(f"_execute_sql_sync: Loading .env from: {specific_env_path}")
        load_dotenv(dotenv_path=specific_env_path, override=True)
    else:
        print("_execute_sql_sync: No specific .env file found. Relying on system environment variables or a global .env.")
        load_dotenv(override=True) # Load global .env or system vars

    db_host = os.getenv("DB_HOST")
    db_port = os.getenv("DB_PORT")
    db_name = os.getenv("DB_NAME")
    db_user = os.getenv("DB_USER")
    db_password = os.getenv("DB_PASSWORD")

    if not all([db_host, db_port, db_name, db_user, db_password]):
        error_msg = "_execute_sql_sync: Database connection details missing in environment variables."
        print(error_msg)
        return {"error_message": error_msg, "sql_result": ""}

    conn_string = f"host='{db_host}' port='{db_port}' dbname='{db_name}' user='{db_user}' password='{db_password}'"
    conn = None
    try:
        print(f"_execute_sql_sync: Connecting to DB with: {conn_string.replace(db_password, '****') if db_password else conn_string}")
        conn = psycopg2.connect(conn_string)
        print(f"_execute_sql_sync: Executing SQL: {sql_query}")
        df = pd.read_sql_query(sql_query, conn)
        sql_result_str = df.to_string()
        print(f"_execute_sql_sync: SQL Result (first 200 chars): {sql_result_str[:200]}")
        return {"sql_result": sql_result_str, "error_message": None}
    except (psycopg2.Error, pd.io.sql.DatabaseError) as e:
        error_msg = f"_execute_sql_sync: Database error: {e}"
        print(error_msg)
        return {"error_message": error_msg, "sql_result": ""}
    except ValueError as e: # Often from pandas if query is malformed for read_sql_query
        error_msg = f"_execute_sql_sync: SQL query validation error for pandas: {e}"
        print(error_msg)
        return {"error_message": error_msg, "sql_result": ""}
    except Exception as e:
        error_msg = f"_execute_sql_sync: An unexpected error occurred during SQL execution: {e}"
        print(error_msg)
        return {"error_message": error_msg, "sql_result": ""}
    finally:
        if conn:
            conn.close()
            print("_execute_sql_sync: DB ì—°ê²° ì¢…ë£Œ.")

async def execute_sql_node(state: AgentState, config: Optional[RunnableConfig] = None) -> Dict[str, Any]:
    print("--- EXECUTE SQL NODE ---")
    sql_query = state.sql_query
    if not sql_query:
        return {"error_message": "No SQL query to execute.", "sql_result": ""}

    # Define base directories for .env file search relative to this file's location
    # analysis_agent/.env -> ../../.env
    base_dir_analysis_env = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    # my_state_agent/.env -> ../../../my_state_agent
    base_dir_my_state_env = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..', 'my_state_agent'))
    
    # Run the synchronous dotenv loading and database operations in a separate thread
    try:
        print(f"execute_sql_node: Calling asyncio.to_thread for SQL: {sql_query}")
        result_dict = await asyncio.to_thread(
            _execute_sql_sync, 
            sql_query, 
            base_dir_analysis_env, 
            base_dir_my_state_env
        )
        return result_dict
    except Exception as e: # Catch potential errors from asyncio.to_thread itself
        error_msg = f"execute_sql_node: Error running DB operations in thread: {e}"
        print(error_msg)
        return {"error_message": error_msg, "sql_result": ""}

async def create_visualization_node(state: AgentState, config: Optional[RunnableConfig] = None) -> Dict[str, Any]:
    print("--- CREATE VISUALIZATION NODE (Placeholder) ---")
    sql_result = state.sql_result
    if not sql_result:
        # If there's an error message from a previous node, pass it along.
        # Otherwise, set a specific error for this node.
        error_to_pass = state.error_message or "No SQL result to visualize."
        print(f"Create Visualization Node: Error - {error_to_pass}")
        return {"error_message": error_to_pass, "sql_result": sql_result} # Pass original sql_result for context

    # Placeholder: Simulate visualization creation
    # In a real scenario, this would generate a chart, table, or some visual representation.
    visualization_output = f"[Placeholder: Visualization for SQL result: {str(sql_result)[:100]}...]"
    print(f"Create Visualization Node: Generated - {visualization_output}")
    
    # For now, we'll just pass the original sql_result and a note about visualization.
    # If the next node needs specific visualization data, we'd add it to the state here.
    return {"sql_result": sql_result, "visualization_output": visualization_output, "error_message": None}

async def summarize_sql_result_node(state: AgentState, config: Optional[RunnableConfig] = None) -> Dict[str, Any]:
    print("--- SUMMARIZE SQL RESULT NODE ---")
    user_query = state.user_query # Get user_query from the state field set by supervisor
    sql_query = state.sql_query or ""
    sql_result = state.sql_result or ""
    current_messages = state.messages # Get current messages

    if not user_query:
         # Add error message to messages as well
        error_msg = "No user query found for summarization."
        updated_messages = current_messages + [AIMessage(content=error_msg)]
        return {"messages": updated_messages, "error_message": error_msg, "final_answer": ""}
    if not sql_result:
        error_msg = state.error_message or "No SQL result to summarize."
        final_response_msg = state.error_message or "SQL query execution failed or produced no result."
        updated_messages = current_messages + [AIMessage(content=final_response_msg)]
        return {"messages": updated_messages, "error_message": error_msg, "final_answer": final_response_msg}

    print(f"Summarizing for query: {user_query}, SQL: {sql_query[:100]}..., Result: {sql_result[:100]}...")
    summarization_chain = summarization_prompt | llm
    response = await summarization_chain.ainvoke(
        {"user_query": user_query, "sql_query": sql_query, "sql_result": sql_result},
        config=config
    )
    final_answer = response.content.strip()
    print(f"Summarized Answer: {final_answer}")
    
    # Add the final_answer to the messages list as an AIMessage
    updated_messages = current_messages + [AIMessage(content=final_answer)]
    
    return {"messages": updated_messages, "final_answer": final_answer}

async def general_question_node(state: AgentState, config: Optional[RunnableConfig] = None) -> Dict[str, Any]:
    print("--- GENERAL QUESTION NODE ---")
    current_messages = state.messages # Get current messages
    
    # Ensure there's a user query to process from the last HumanMessage
    user_query_to_process: Optional[str] = None
    if state.user_query: # Prefer user_query if set by supervisor
        user_query_to_process = state.user_query
    elif current_messages and isinstance(current_messages[-1], HumanMessage):
        user_query_to_process = current_messages[-1].content
    
    if not user_query_to_process:
        error_msg = "No user query found for general question."
        print(f"General Question Node - Error: {error_msg}")
        updated_messages = current_messages + [AIMessage(content=error_msg)] if current_messages else [AIMessage(content=error_msg)]
        return {"messages": updated_messages, "error_message": error_msg, "final_answer": ""}

    print(f"General Question Node - Processing query: '{user_query_to_process}'")
    general_answer_chain = general_answer_prompt | llm
    try:
        response = await general_answer_chain.ainvoke({"user_query": user_query_to_process}, config=config)
        final_answer = response.content.strip()
        print(f"General Answer: {final_answer}")
        
        updated_messages = current_messages + [AIMessage(content=final_answer)]
        return {"messages": updated_messages, "final_answer": final_answer, "error_message": None}
    except Exception as e:
        error_msg = f"Error in general_question_node: {e}"
        print(f"General Question Node - Error: {error_msg}")
        updated_messages = current_messages + [AIMessage(content=f"Sorry, I encountered an error trying to answer: {error_msg}")]
        return {"messages": updated_messages, "error_message": error_msg, "final_answer": ""}

async def category_predict_node(state: AgentState, config: Optional[RunnableConfig] = None) -> Dict[str, Any]:
    print("--- CATEGORY PREDICT NODE (Telecom Churn Prediction with Csv File Content) ---")

    # --- ê²½ë¡œ ì„¤ì • ---
    base_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
    MODEL_PATH = os.path.join(base_path, 'churn_predictor_pipeline.pkl')
    CATEGORICAL_COLS_PATH = os.path.join(base_path, 'categorical_cols.pkl')
    LABEL_ENCODERS_PATH = os.path.join(base_path, 'label_encoders.pkl')

    EXPECTED_FEATURE_ORDER = [
        'seniorcitizen', 'partner', 'dependents', 'tenure', 'phoneservice',
        'multiplelines', 'onlinesecurity', 'onlinebackup', 'techsupport',
        'contract', 'paperlessbilling', 'paymentmethod', 'monthlycharges', 'totalcharges',
        'new_totalservices', 'new_avg_charges', 'new_increase', 'new_avg_service_fee',
        'charge_increased', 'charge_growth_rate', 'is_auto_payment',
        'expected_contract_months', 'contract_gap'
    ]
    CUSTOMER_ID_COL = 'customerid'
    PREDICTION_THRESHOLD = 0.312

    csv_data_str: Optional[str] = None

    # 1. state.csv_file_content (LangGraph Studioì˜ 'Csv File Content' í•„ë“œ) í™•ì¸
    if state.csv_file_content:
        print("INFO: Using CSV data from state.csv_file_content.")
        csv_data_str = state.csv_file_content
    # 2. state.user_query (Chat ë˜ëŠ” Messages ì…ë ¥) í™•ì¸
    elif hasattr(state, 'user_query') and state.user_query:
        print(f"INFO: Attempting to use state.user_query for CSV data. Content (first 100 chars): '{state.user_query[:100]}...'")
        # 2a. state.user_queryë¥¼ íŒŒì¼ ê²½ë¡œë¡œ ì‹œë„
        if os.path.exists(state.user_query):
            try:
                print(f"INFO: state.user_query '{state.user_query}' is an existing path. Reading file.")
                def read_file_sync(path):
                    with open(path, 'r', encoding='utf-8') as f_sync:
                        return f_sync.read()
                csv_data_str = await asyncio.to_thread(read_file_sync, state.user_query)
                if not csv_data_str:
                    print(f"WARNING: File at '{state.user_query}' was empty.")
            except Exception as e:
                print(f"WARNING: Error reading file from state.user_query path '{state.user_query}': {e}. Will attempt to treat as raw content.")
        
        # 2b. state.user_queryë¥¼ íŒŒì¼ ê²½ë¡œë¡œ ì½ì§€ ëª»í–ˆê±°ë‚˜, ê²½ë¡œê°€ ì•„ë‹ˆì—ˆë‹¤ë©´ ì›ë³¸ CSV ë‚´ìš©ìœ¼ë¡œ ê°„ì£¼
        if csv_data_str is None: # íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ë˜ëŠ” ê²½ë¡œê°€ ì•„ë‹ˆì—ˆìŒ
            print("INFO: Treating state.user_query as raw CSV content.")
            csv_data_str = state.user_query # pd.read_csvê°€ ì´í›„ì— íŒŒì‹± ì‹œë„

    # CSV ë°ì´í„°ë¥¼ ì–´ë””ì—ì„œë„ ì°¾ì§€ ëª»í•œ ê²½ìš° ì˜¤ë¥˜ ë°˜í™˜
    if csv_data_str is None:
        error_message_parts = ["âŒ ì˜¤ë¥˜: CSV ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]
        checked_sources = ["'Csv File Content' í•„ë“œ"]
        if hasattr(state, 'user_query'):
            checked_sources.append("'User Query' / ì±„íŒ… ë©”ì‹œì§€ (íŒŒì¼ ê²½ë¡œ ë˜ëŠ” CSV ë‚´ìš© ì§ì ‘ ì…ë ¥)")
        error_message_parts.append(f"í™•ì¸í•œ ì…ë ¥ ì†ŒìŠ¤: {', '.join(checked_sources)}.")
        error_message_parts.append("Csv File Content í•„ë“œì— ì§ì ‘ CSV ë‚´ìš©ì„ ë¶™ì—¬ë„£ê±°ë‚˜, ì±„íŒ…ìœ¼ë¡œ CSV íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œ ë˜ëŠ” CSV ë‚´ìš© ìì²´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        final_answer = "\n".join(error_message_parts)
        current_messages = state.messages # Get current messages
        updated_messages = current_messages + [AIMessage(content=final_answer)] if current_messages else [AIMessage(content=final_answer)]
        return {"messages": updated_messages, "final_answer": final_answer, "error_message": final_answer}

    print(f"INFO: CSV data obtained. Length: {len(csv_data_str)}. Preview (first 200 chars): {csv_data_str[:200]}...")

    try:
        # --- ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ ê°ì²´ ë¹„ë™ê¸° ë¡œë“œ ---
        pipeline_final = await asyncio.to_thread(joblib.load, MODEL_PATH)
        CATEGORICAL_COLS = await asyncio.to_thread(joblib.load, CATEGORICAL_COLS_PATH)
        label_encoders = await asyncio.to_thread(joblib.load, LABEL_ENCODERS_PATH)

        # --- CSV ë¬¸ìì—´ â†’ DataFrame ë³€í™˜ ---
        if not csv_data_str: # ì´ì¤‘ í™•ì¸, csv_data_strì´ Noneì´ë‚˜ ë¹ˆ ë¬¸ìì—´ì´ë©´ ì—ëŸ¬ ë°œìƒ ë°©ì§€
            final_answer = "âŒ ì˜¤ë¥˜: ë‚´ë¶€ ë¡œì§ ì˜¤ë¥˜ - CSV ë°ì´í„° ë¬¸ìì—´ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
            current_messages = state.messages # Get current messages
            updated_messages = current_messages + [AIMessage(content=final_answer)] if current_messages else [AIMessage(content=final_answer)]
            return {"messages": updated_messages, "final_answer": final_answer, "error_message": final_answer}
        # --- BEGIN REVISED CSV DATA CLEANING LOGIC ---
        raw_lines = csv_data_str.strip().splitlines()
        cleaned_lines = []

        if not raw_lines:
            final_answer = "âŒ ì˜¤ë¥˜: CSV ë°ì´í„° ë¬¸ìì—´ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
            current_messages = state.messages
            updated_messages = current_messages + [AIMessage(content=final_answer)] if current_messages else [AIMessage(content=final_answer)]
            return {"messages": updated_messages, "final_answer": final_answer, "error_message": final_answer}

        MIN_COMMAS_THRESHOLD = 1  # ì‰¼í‘œê°€ ì´ ê°œìˆ˜ ì´ìƒì´ë©´ 'ê°•ë ¥í•œ' CSV ë¼ì¸ìœ¼ë¡œ ê°„ì£¼

        # 'ê°•ë ¥í•œ' CSV ë¼ì¸ë“¤ì˜ ì¸ë±ìŠ¤ë¥¼ ì°¾ìŒ
        strong_csv_indices = [i for i, line in enumerate(raw_lines) if line.count(',') >= MIN_COMMAS_THRESHOLD]

        if strong_csv_indices:
            # 'ê°•ë ¥í•œ' ë¼ì¸ë“¤ì´ ì¡´ì¬í•˜ë©´, ì´ë“¤ì˜ ë²”ìœ„ë¥¼ í•µì‹¬ CSV ë¸”ë¡ìœ¼ë¡œ ê°„ì£¼
            core_block_start_idx = strong_csv_indices[0]
            core_block_end_idx = strong_csv_indices[-1]

            if core_block_start_idx > 0:
                print(f"INFO: CSV ì‹œì‘ ì „ {core_block_start_idx}ê°œì˜ ë¼ì¸ì„ ì§ˆë¬¸ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì œê±°í•©ë‹ˆë‹¤. ì²«ë²ˆì§¸ ì œê±°ëœ ë¼ì¸: '{raw_lines[0][:100]}...'", flush=True)
            if core_block_end_idx < len(raw_lines) - 1:
                print(f"INFO: CSV ì¢…ë£Œ í›„ {len(raw_lines) - 1 - core_block_end_idx}ê°œì˜ ë¼ì¸ì„ ì§ˆë¬¸ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì œê±°í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ ì œê±°ëœ ë¼ì¸: '{raw_lines[-1][:100]}...'", flush=True)

            # í•µì‹¬ CSV ë¸”ë¡ ì¶”ì¶œ
            core_block_lines = raw_lines[core_block_start_idx : core_block_end_idx + 1]

            if not core_block_lines: # Should not happen if strong_csv_indices is not empty
                final_answer = "âŒ ì˜¤ë¥˜: CSV í•µì‹¬ ë¸”ë¡ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                current_messages = state.messages
                updated_messages = current_messages + [AIMessage(content=final_answer)] if current_messages else [AIMessage(content=final_answer)]
                return {"messages": updated_messages, "final_answer": final_answer, "error_message": final_answer}

            # í•µì‹¬ ë¸”ë¡ì˜ ì²« ì¤„ì„ í—¤ë”ë¡œ ì‚¬ìš©
            header_line = core_block_lines[0]
            cleaned_lines.append(header_line)
            commas_in_header = header_line.count(',') # í—¤ë”ëŠ” MIN_COMMAS_THRESHOLD ì´ìƒì¼ ê²ƒì„

            # í•µì‹¬ ë¸”ë¡ ë‚´ì˜ ë°ì´í„° ë¼ì¸ ì •ì œ
            for i in range(1, len(core_block_lines)):
                line = core_block_lines[i]
                if line.count(',') >= MIN_COMMAS_THRESHOLD: # 'ê°•ë ¥í•œ' ë°ì´í„° ë¼ì¸
                    cleaned_lines.append(line)
                elif commas_in_header >= MIN_COMMAS_THRESHOLD: # í—¤ë”ëŠ” 'ê°•ë ¥'í–ˆìœ¼ë‚˜, í˜„ì¬ ë¼ì¸ì€ 'ì•½í•¨' (0ê°œì˜ ì‰¼í‘œ)
                    if not line.strip(): # ì˜ë„ì ìœ¼ë¡œ ë¹„ì–´ìˆëŠ” ë¼ì¸ì´ë©´ ìœ ì§€
                        cleaned_lines.append(line)
                    else: # ë‚´ìš©ì´ ìˆëŠ” 0ì‰¼í‘œ ë¼ì¸ì€ ë¸”ë¡ ë‚´ ì§ˆë¬¸ìœ¼ë¡œ ì˜ì‹¬í•˜ì—¬ í•„í„°ë§
                        print(f"INFO: CSV ë¸”ë¡ ë‚´ì—ì„œ 0ê°œì˜ ì‰¼í‘œë¥¼ ê°€ì§„ ë¹„ì–´ìˆì§€ ì•Šì€ ë¼ì¸ì„ í•„í„°ë§í•©ë‹ˆë‹¤: '{line[:100]}...'", flush=True)
                else: # í—¤ë”ë„ 'ì•½í–ˆê³ ' (ì´ ê²½ìš°ëŠ” strong_csv_indices ë¡œì§ìƒ ê±°ì˜ ì—†ìŒ) í˜„ì¬ ë¼ì¸ë„ 'ì•½í•˜ë©´' ì¼ë‹¨ í¬í•¨
                    cleaned_lines.append(line)
        else:
            # 'ê°•ë ¥í•œ' CSV ë¼ì¸ì´ í•˜ë‚˜ë„ ì—†ìŒ (ëª¨ë“  ë¼ì¸ì˜ ì‰¼í‘œ < MIN_COMMAS_THRESHOLD, ì˜ˆ: ëª¨ë‘ 0ê°œ)
            # ì´ ê²½ìš° ë‹¨ì¼ ì—´ CSVì´ê±°ë‚˜ ì „ì²´ê°€ ì§ˆë¬¸ì¼ ìˆ˜ ìˆìŒ. ì¼ë‹¨ ëª¨ë“  ë¼ì¸ì„ ì‚¬ìš©.
            print(f"INFO: ëª¨ë“  ë¼ì¸ì˜ ì‰¼í‘œ ê°œìˆ˜ê°€ {MIN_COMMAS_THRESHOLD}ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤. ë‹¨ì¼ ì—´ CSVë¡œ ê°„ì£¼í•˜ê±°ë‚˜ ì „ì²´ê°€ ì§ˆë¬¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë“  ë¼ì¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.", flush=True)
            cleaned_lines = raw_lines

        if not cleaned_lines:
            final_answer = "âŒ ì˜¤ë¥˜: CSV ë°ì´í„° ì •ì œ í›„ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
            current_messages = state.messages
            updated_messages = current_messages + [AIMessage(content=final_answer)] if current_messages else [AIMessage(content=final_answer)]
            return {"messages": updated_messages, "final_answer": final_answer, "error_message": final_answer}
        
        cleaned_csv_data_str = "\n".join(cleaned_lines)
        
        print(f"INFO: ìµœì¢… ì •ì œëœ CSV ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²« 200ì): {cleaned_csv_data_str[:200]}...", flush=True)
        input_df = await asyncio.to_thread(pd.read_csv, io.StringIO(cleaned_csv_data_str))
        # --- END REVISED CSV DATA CLEANING LOGIC ---

        if CUSTOMER_ID_COL not in input_df.columns:
            final_answer = f"âŒ ì˜¤ë¥˜: '{CUSTOMER_ID_COL}' ì»¬ëŸ¼ì´ CSVì— ì—†ìŠµë‹ˆë‹¤."
            current_messages = state.messages # Get current messages
            updated_messages = current_messages + [AIMessage(content=final_answer)] if current_messages else [AIMessage(content=final_answer)]
            return {"messages": updated_messages, "final_answer": final_answer, "error_message": final_answer}

        customer_ids = input_df[CUSTOMER_ID_COL]
        X_predict = input_df.drop(columns=[CUSTOMER_ID_COL], errors='ignore')

        # --- ë²”ì£¼í˜• ì»¬ëŸ¼ ì¸ì½”ë”© ---
        for col in CATEGORICAL_COLS:
            if col in X_predict.columns:
                if col in label_encoders:
                    le = label_encoders[col]
                    X_predict[col] = X_predict[col].apply(
                        lambda x: le.transform([x])[0] if x in le.classes_ else -1
                    )
                else:
                    print(f"WARNING: Label encoder for column '{col}' not found. Skipping encoding.")
            else:
                print(f"WARNING: Categorical column '{col}' not found in input CSV. Skipping.")

        # --- ëˆ„ë½ëœ ì»¬ëŸ¼ ì²˜ë¦¬ (ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ëª¨ë“  ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸) ---
        missing_cols = set(EXPECTED_FEATURE_ORDER) - set(X_predict.columns)
        for col in missing_cols:
            print(f"INFO: Adding missing column '{col}' with default value 0.")
            X_predict[col] = 0 # ë˜ëŠ” np.nan ë“± ì ì ˆí•œ ê¸°ë³¸ê°’

        # --- ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬ ---
        X_predict = X_predict[EXPECTED_FEATURE_ORDER]

        # --- ì˜ˆì¸¡ ìˆ˜í–‰ ---
        predictions_proba = await asyncio.to_thread(pipeline_final.predict_proba, X_predict)
        predictions = (predictions_proba[:, 1] >= PREDICTION_THRESHOLD).astype(int)

        # --- ê²°ê³¼ ìƒì„± ---
        results_df = pd.DataFrame({
            CUSTOMER_ID_COL: customer_ids,
            'Churn Probability': predictions_proba[:, 1],
            'Churn Prediction (Threshold 0.312)': predictions
        })
        results_df['Churn Prediction (Threshold 0.312)'] = results_df['Churn Prediction (Threshold 0.312)'].map({1: 'Yes', 0: 'No'})

        final_answer = "ğŸ“Š ê³ ê° ì´íƒˆ ì˜ˆì¸¡ ê²°ê³¼:\n" + results_df.to_string(index=False)
        print(f"Prediction successful. Result preview: {final_answer[:200]}...")
        current_messages = state.messages # Get current messages
        updated_messages = current_messages + [AIMessage(content=final_answer)] if current_messages else [AIMessage(content=final_answer)]
        return {"messages": updated_messages, "final_answer": final_answer, "error_message": None}

    except pd.errors.EmptyDataError:
        error_msg = "âŒ ì˜¤ë¥˜: ì…ë ¥ëœ CSV ë°ì´í„°ê°€ ë¹„ì–´ ìˆê±°ë‚˜ ì˜ëª»ëœ í˜•ì‹ì…ë‹ˆë‹¤. CSV ë‚´ìš©ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”."
        print(f"ERROR: {error_msg}")
        current_messages = state.messages # Get current messages
        updated_messages = current_messages + [AIMessage(content=error_msg)] if current_messages else [AIMessage(content=error_msg)]
        return {"messages": updated_messages, "final_answer": error_msg, "error_message": error_msg}
    except FileNotFoundError as e:
        error_msg = f"âŒ ì˜¤ë¥˜: ëª¨ë¸ ë˜ëŠ” ì „ì²˜ë¦¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. ({e})"
        print(f"ERROR: {error_msg}")
        current_messages = state.messages # Get current messages
        updated_messages = current_messages + [AIMessage(content=error_msg)] if current_messages else [AIMessage(content=error_msg)]
        return {"messages": updated_messages, "final_answer": error_msg, "error_message": error_msg}
    except KeyError as e:
        error_msg = f"âŒ ì˜¤ë¥˜: CSV ë°ì´í„°ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆê±°ë‚˜, ëª¨ë¸ í•™ìŠµ ì‹œ ì‚¬ìš©ëœ ì»¬ëŸ¼ê³¼ ë‹¤ë¦…ë‹ˆë‹¤. (ì˜¤ë¥˜ ì»¬ëŸ¼: {e}) CSV íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
        print(f"ERROR: {error_msg}")
        current_messages = state.messages # Get current messages
        updated_messages = current_messages + [AIMessage(content=error_msg)] if current_messages else [AIMessage(content=error_msg)]
        return {"messages": updated_messages, "final_answer": error_msg, "error_message": error_msg}
    except ValueError as e:
        error_msg = f"âŒ ì˜¤ë¥˜: ë°ì´í„° ë³€í™˜ ì¤‘ ê°’ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. CSV ë°ì´í„° íƒ€ì…ì„ í™•ì¸í•´ì£¼ì„¸ìš”. (ì˜¤ë¥˜: {e})"
        print(f"ERROR: {error_msg}")
        current_messages = state.messages # Get current messages
        updated_messages = current_messages + [AIMessage(content=error_msg)] if current_messages else [AIMessage(content=error_msg)]
        return {"messages": updated_messages, "final_answer": error_msg, "error_message": error_msg}
    except Exception as e:
        error_msg = f"âŒ ì˜ˆì¸¡ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}"
        print(f"ERROR: {error_msg}")
        current_messages = state.messages # Get current messages
        updated_messages = current_messages + [AIMessage(content=error_msg)] if current_messages else [AIMessage(content=error_msg)]
        return {"messages": updated_messages, "final_answer": error_msg, "error_message": error_msg}

def route_sql_output(state: AgentState) -> Literal["create_visualization_node", "summarize_sql_result_node"]:
    choice = state.sql_output_choice
    # If execute_sql_node itself resulted in an error (indicated by error_message and no sql_result)
    # both visualization and summarization nodes have internal logic to handle this.
    # The choice made by the supervisor should still be respected if possible.
    if state.error_message and not state.sql_result:
        print(f"Error detected before routing SQL output: {state.error_message}. Proceeding with choice: {choice}")

    if choice == "visualize":
        print("Routing to create_visualization_node based on sql_output_choice.")
        return "create_visualization_node"
    elif choice == "summarize":
        print("Routing to summarize_sql_result_node based on sql_output_choice.")
        return "summarize_sql_result_node"
    else:
        # Fallback if sql_output_choice is somehow not set for a db_query path
        print(f"Warning: sql_output_choice is '{choice}'. Defaulting to summarize_sql_result_node.")
        return "summarize_sql_result_node"

def route_query(state: AgentState) -> Literal["generate_sql_node", "category_predict_node", "general_question_node"]:
    query_type = state.query_type
    print(f"Routing based on query_type: {query_type}")
    if query_type == "db_query":
        return "generate_sql_node"
    elif query_type == "category_predict_query":
        return "category_predict_node"
    elif query_type == "general_query":
        return "general_question_node"
    else:
        # This case should ideally not be reached if supervisor is strict
        print(f"Warning: Unknown query_type '{query_type}', defaulting to general_question_node.")
        return "general_question_node"

# --- Graph Definition ---
workflow = StateGraph(AgentState)

# Add nodes
workflow.add_node("supervisor", supervisor_node)
workflow.add_node("generate_sql_node", generate_sql_node)
workflow.add_node("execute_sql_node", execute_sql_node)
workflow.add_node("create_visualization_node", create_visualization_node)
workflow.add_node("summarize_sql_result_node", summarize_sql_result_node)
workflow.add_node("general_question_node", general_question_node)
workflow.add_node("category_predict_node", category_predict_node)

# Set entry point
workflow.set_entry_point("supervisor")

# Conditional edges from supervisor
workflow.add_conditional_edges(
    "supervisor",
    route_query,
    {
        "generate_sql_node": "generate_sql_node",
        "category_predict_node": "category_predict_node",
        "general_question_node": "general_question_node"
    }
)

# Edges for DB query flow (now common for 3 branches)
workflow.add_edge("generate_sql_node", "execute_sql_node")
# After execute_sql_node, route to either visualization or summarization
workflow.add_conditional_edges(
    "execute_sql_node",
    route_sql_output,
    {
        "create_visualization_node": "create_visualization_node",
        "summarize_sql_result_node": "summarize_sql_result_node"
    }
)
workflow.add_edge("create_visualization_node", END)
workflow.add_edge("summarize_sql_result_node", END)

# Edges from placeholder nodes to the SQL generation flow
workflow.add_edge("category_predict_node", END)

# Edge for general question
workflow.add_edge("general_question_node", END)

# Compile the graph
app = workflow.compile()
graph = app # For langgraph dev compatibility

# To make it runnable with langgraph dev, ensure it's assigned to 'graph'
# Example of how to run (for testing locally):
# async def main():
#     inputs = {"user_query": "ì§€ë‚œ ë‹¬ ì‚¬ìš©ì ìˆ˜ëŠ” ëª‡ ëª…ì¸ê°€ìš”?"}
#     # For testing, you can invoke the graph like this:
#     # inputs = {"messages": [HumanMessage(content="ìš°ë¦¬ íšŒì‚¬ ì§ì›ë“¤ ì¤‘ ê°€ì¥ ì—°ë´‰ì´ ë†’ì€ ìƒìœ„ 3ëª…ì€ ëˆ„êµ¬ì¸ê°€ìš”?")]}
#     # async for event in app.astream_events(inputs, version="v1"):
#     #     kind = event["event"]
#     #     if kind == "on_chat_model_stream":
#     #         content = event["data"]["chunk"].content
#     #         if content:
#     #             print(content, end="")
#     #     elif kind == "on_tool_end":
#     #         print(f"\nTool Output: {event['data']['output']}")
#     #     # print(f"\n--- Event: {kind} ---")
#     #     # print(event["data"])
# if __name__ == "__main__":
#     import asyncio
#     async def main_test():
#         app_test = await main()
#         inputs = {"input": "ìš°ë¦¬ íšŒì‚¬ í…Œì´ë¸” ëª©ë¡ ì¢€ ë³´ì—¬ì¤˜"}
#     # inputs = {"input": "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?"}
#     async for event in app_test.astream_events(inputs, version="v1"):
#             kind = event["event"]
#             if kind == "on_chat_model_stream":
#                 content = event["data"]["chunk"].content
#                 if content:
#                     print(content, end="")
#                 print(f"\nTool Output: {event['data']['output']}")
#             elif kind == "on_chain_end" and event["name"] == "AgentGraph": # Check for the graph's end
#                 print("\n--- Final State ---")
#                 print(event["data"].get("output")) # Access final output from the event

#     asyncio.run(main_test())
